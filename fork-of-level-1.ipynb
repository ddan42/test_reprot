{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7395155,"sourceType":"datasetVersion","datasetId":4297632},{"sourceId":9984524,"sourceType":"datasetVersion","datasetId":6144291}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- в каждом блоке трансформера несколько голов, тогда будем в части голов считать атеншн по текущим хидденам (SA(QKV)), \nа в части голов KV считаем по хидденам после предыдущих блоков и эмбеддингам токенов, и Q по текущим хидденам (CA(KV, Q))\n- $Attn_i = Cat[SA(h_i), CA(h_{i-1}, h_i), CA(h_{i-2}, h_i)]$\n- в первом и втором слое все головы считаются по текущему контексту, начиная с 3 делаем reflex attention\n- в этой секции зафиксируем, что на SA и на каждый из CA по 2 головы (всего 6)","metadata":{}},{"cell_type":"markdown","source":"# The Architecture","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:00.326061Z","iopub.execute_input":"2024-11-28T07:24:00.326408Z","iopub.status.idle":"2024-11-28T07:24:00.356864Z","shell.execute_reply.started":"2024-11-28T07:24:00.326370Z","shell.execute_reply":"2024-11-28T07:24:00.355684Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/openwebtext-data-prepared-for-nanogpt/train.bin\n/kaggle/input/openwebtext-data-prepared-for-nanogpt/val.bin\n/kaggle/input/mingpt/mingpt/trainer.py\n/kaggle/input/mingpt/mingpt/bpe.py\n/kaggle/input/mingpt/mingpt/model.py\n/kaggle/input/mingpt/mingpt/utils.py\n/kaggle/input/mingpt/mingpt/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/mingpt/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:00.357844Z","iopub.execute_input":"2024-11-28T07:24:00.358206Z","iopub.status.idle":"2024-11-28T07:24:00.362213Z","shell.execute_reply.started":"2024-11-28T07:24:00.358132Z","shell.execute_reply":"2024-11-28T07:24:00.361328Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import mingpt.bpe\nimport mingpt.utils\nimport mingpt.model\nimport mingpt.trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:00.364260Z","iopub.execute_input":"2024-11-28T07:24:00.364536Z","iopub.status.idle":"2024-11-28T07:24:03.730297Z","shell.execute_reply.started":"2024-11-28T07:24:00.364501Z","shell.execute_reply":"2024-11-28T07:24:03.729614Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.731308Z","iopub.execute_input":"2024-11-28T07:24:03.731667Z","iopub.status.idle":"2024-11-28T07:24:03.735847Z","shell.execute_reply.started":"2024-11-28T07:24:03.731640Z","shell.execute_reply":"2024-11-28T07:24:03.734860Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.736784Z","iopub.execute_input":"2024-11-28T07:24:03.737012Z","iopub.status.idle":"2024-11-28T07:24:03.748498Z","shell.execute_reply.started":"2024-11-28T07:24:03.736988Z","shell.execute_reply":"2024-11-28T07:24:03.747699Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from mingpt.utils import set_seed\nfrom mingpt.bpe import BPETokenizer\nset_seed(3407)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.749429Z","iopub.execute_input":"2024-11-28T07:24:03.749664Z","iopub.status.idle":"2024-11-28T07:24:03.765919Z","shell.execute_reply.started":"2024-11-28T07:24:03.749641Z","shell.execute_reply":"2024-11-28T07:24:03.765199Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# let's run to see if layers can be accessed by names and added to a list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.766782Z","iopub.execute_input":"2024-11-28T07:24:03.767013Z","iopub.status.idle":"2024-11-28T07:24:03.772956Z","shell.execute_reply.started":"2024-11-28T07:24:03.766990Z","shell.execute_reply":"2024-11-28T07:24:03.772235Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\"\"\" Efficient implementation equivalent to the following:\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n        attn_bias.to(query.dtype)\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias += attn_mask\n\n    if enable_gqa:\n        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight @ value\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.773875Z","iopub.execute_input":"2024-11-28T07:24:03.774080Z","iopub.status.idle":"2024-11-28T07:24:03.787483Z","shell.execute_reply.started":"2024-11-28T07:24:03.774058Z","shell.execute_reply":"2024-11-28T07:24:03.786544Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"' Efficient implementation equivalent to the following:\\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\\n        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\\n    L, S = query.size(-2), key.size(-2)\\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\\n    attn_bias = torch.zeros(L, S, dtype=query.dtype)\\n    if is_causal:\\n        assert attn_mask is None\\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\\n        attn_bias.to(query.dtype)\\n\\n    if attn_mask is not None:\\n        if attn_mask.dtype == torch.bool:\\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\\n        else:\\n            attn_bias += attn_mask\\n\\n    if enable_gqa:\\n        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\\n        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\\n\\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\\n    attn_weight += attn_bias\\n    attn_weight = torch.softmax(attn_weight, dim=-1)\\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\\n    return attn_weight @ value'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"class NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.790786Z","iopub.execute_input":"2024-11-28T07:24:03.791027Z","iopub.status.idle":"2024-11-28T07:24:03.798143Z","shell.execute_reply.started":"2024-11-28T07:24:03.791004Z","shell.execute_reply":"2024-11-28T07:24:03.797229Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# It is easier to add 3 separate W_q, W_k, W_v for now\n# models in the experiments will be relatively small anyway\n# [ ] do we need tril? I guess yes, because otherwise query would look into the future\n# [ ] tried to make make as close to scaled_dot_product_attention\nclass ReflexAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        \n        self.key = nn.Linear(config.n_embd, config.head_size, bias=False)\n        self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n        self.value = nn.Linear(config.n_embd, config.head_size, bias=False)\n\n        #self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n\n        self.dropout = nn.Dropout(config.attn_pdrop)\n        #self.n_head = config.n_head [ ] do i need this\n        #self.n_embd = config.n_embd\n\n    def forward(self, x, x_prev = None):\n        B,T,C = x.shape\n        q = self.query(x) # (B,T,hs)\n        if x_prev is not None:\n            B_prev, T_prev, C_prev = x_prev.size()\n            # DEBUG print(B == B_prev, T == T_prev, C, C_prev)\n            k = self.key(x_prev)   # (B,T,hs)\n            v = self.value(x_prev) # (B,T,hs)\n        else:\n            k = self.key(x)   # (B,T,hs)\n            v = self.value(x) # (B,T,hs)\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.799192Z","iopub.execute_input":"2024-11-28T07:24:03.799419Z","iopub.status.idle":"2024-11-28T07:24:03.816778Z","shell.execute_reply.started":"2024-11-28T07:24:03.799396Z","shell.execute_reply":"2024-11-28T07:24:03.815966Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# [ ] there is a mess with drop out\nclass MultiHeadReflexAttention(nn.Module):\n    # heads in parallel\n    # [ ] we just split and than concat, they are independent\n    def __init__(self, config):\n        super().__init__()\n        self.heads = nn.ModuleList([ReflexAttention(config) for _ in range(config.n_head)])\n        self.proj = nn.Linear(config.head_size * config.n_head, config.n_embd)\n        self.dropout = nn.Dropout(config.resid_pdrop)\n\n    \"\"\" self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\"\"\"\n    def forward(self, x, b_i):\n        out = []\n        for h_i, h in enumerate(self.heads):\n            # DEBUG print('forward', x.shape, b_i, h_i)\n            if b_i == 0 or b_i == 1:\n                t = h(x)\n                out.append(t)\n                continue\n            if h_i == 0 or h_i == 1:\n                out.append(h(x))\n            else:\n                # DEBUG print(hdn.hiddens[b_i-1].shape, embed.x_embed.shape)\n                if h_i == 2 or h_i == 3:\n                    out.append(h(x=x, x_prev=hdn.hiddens[b_i-1]+embed.x_embed))\n                elif h_i == 4 or h_i ==5:\n                    out.append(h(x=x, x_prev=hdn.hiddens[b_i-2]+embed.x_embed))\n                elif h_i == 6 or h_i == 7:\n                    out.append(h(x=x, x_prev=hdn.hiddens[b_i-3]+embed.x_embed))\n        # DEBUG print('cat', [r.shape for r in out])       \n        # DEBUG print('proj', config.head_size * config.n_head, config.n_embd)\n        out = torch.cat(out, dim=-1) # [ ] check dim\n        out = self.dropout(self.proj(out))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.817793Z","iopub.execute_input":"2024-11-28T07:24:03.818034Z","iopub.status.idle":"2024-11-28T07:24:03.831300Z","shell.execute_reply.started":"2024-11-28T07:24:03.818009Z","shell.execute_reply":"2024-11-28T07:24:03.830567Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# check head size\n# add config\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        # head_size = n_embd // n_head\n        self.ra = MultiHeadReflexAttention(config)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n            act     = NewGELU(),\n            dropout = nn.Dropout(config.resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n        \n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n\n    def forward(self, x, b_i):\n        x = x + self.ra(self.ln1(x), b_i)\n        x = x + self.mlpf(self.ln2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.832118Z","iopub.execute_input":"2024-11-28T07:24:03.832386Z","iopub.status.idle":"2024-11-28T07:24:03.846218Z","shell.execute_reply.started":"2024-11-28T07:24:03.832353Z","shell.execute_reply":"2024-11-28T07:24:03.845504Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# [ ] I try to get all possible params considered, but it is not that easy\n# [ ] nano has blocksize 1024\n# nano uses 0 dropout\nclass Config(): \n        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n        # C.model_type = 'gpt'\n        n_layer = 8\n        n_head = 8\n        n_embd = 384\n        # these options must be filled in externally\n        vocab_size = 50257\n        block_size = 1024\n        head_size = n_embd // n_head\n        # dropout hyperparameters\n        embd_pdrop = 0\n        resid_pdrop = 0\n        attn_pdrop = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.847095Z","iopub.execute_input":"2024-11-28T07:24:03.847336Z","iopub.status.idle":"2024-11-28T07:24:03.863706Z","shell.execute_reply.started":"2024-11-28T07:24:03.847313Z","shell.execute_reply":"2024-11-28T07:24:03.863011Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"config = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.864717Z","iopub.execute_input":"2024-11-28T07:24:03.864962Z","iopub.status.idle":"2024-11-28T07:24:03.876023Z","shell.execute_reply.started":"2024-11-28T07:24:03.864938Z","shell.execute_reply":"2024-11-28T07:24:03.875383Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Hiddens():\n    hiddens = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.876986Z","iopub.execute_input":"2024-11-28T07:24:03.877323Z","iopub.status.idle":"2024-11-28T07:24:03.888968Z","shell.execute_reply.started":"2024-11-28T07:24:03.877287Z","shell.execute_reply":"2024-11-28T07:24:03.888146Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"hdn = Hiddens()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.889745Z","iopub.execute_input":"2024-11-28T07:24:03.889991Z","iopub.status.idle":"2024-11-28T07:24:03.899254Z","shell.execute_reply.started":"2024-11-28T07:24:03.889968Z","shell.execute_reply":"2024-11-28T07:24:03.898338Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Embed():\n    x_embed = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.900327Z","iopub.execute_input":"2024-11-28T07:24:03.900578Z","iopub.status.idle":"2024-11-28T07:24:03.910880Z","shell.execute_reply.started":"2024-11-28T07:24:03.900541Z","shell.execute_reply":"2024-11-28T07:24:03.910198Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"embed = Embed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.911785Z","iopub.execute_input":"2024-11-28T07:24:03.912017Z","iopub.status.idle":"2024-11-28T07:24:03.921601Z","shell.execute_reply.started":"2024-11-28T07:24:03.911994Z","shell.execute_reply":"2024-11-28T07:24:03.920932Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class ReflexTransformer(nn.Module):\n    \"\"\" Transformer with reflex attention \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        \n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.embd_pdrop),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters (note we don't count the decoder parameters in lm_head)\n        n_params = sum(p.numel() for p in self.transformer.parameters())\n        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.zeros_(module.bias)\n            torch.nn.init.ones_(module.weight)\n\n    def configure_optimizers(self, train_config):\n        \"\"\"\n        This long function is unfortunately doing something very simple and is being very defensive:\n        We are separating out all parameters of the model into two buckets: those that will experience\n        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n        We are then returning the PyTorch optimizer object.\n        \"\"\"\n\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (torch.nn.Linear, )\n        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n        for mn, m in self.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                # random note: because named_modules and named_parameters are recursive\n                # we will see the same tensors p many many times. but doing it this way\n                # allows us to know which parent module any tensor p belongs to...\n                if pn.endswith('bias'):\n                    # all biases will not be decayed\n                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                    # weights of whitelist modules will be weight decayed\n                    decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    # weights of blacklist modules will NOT be weight decayed\n                    no_decay.add(fpn)\n\n        # validate that we considered every parameter\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n                                                    % (str(param_dict.keys() - union_params), )\n\n        # create the pytorch optimizer object\n        optim_groups = [\n            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n        return optimizer\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        #assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        embed.x_embed = self.transformer.drop(tok_emb + pos_emb)\n       \n        hdn.hiddens = []\n        # DEBUG print(x.shape, embed.x_embed.shape)\n        for b_i, block in enumerate(self.transformer.h):\n            x = block(x, b_i)\n            hdn.hiddens.append(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        # if we are given some desired targets also calculate the loss\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -config.block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :] # becomes (B, C)\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    \n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = config # [ ] was self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.922789Z","iopub.execute_input":"2024-11-28T07:24:03.923025Z","iopub.status.idle":"2024-11-28T07:24:03.943593Z","shell.execute_reply.started":"2024-11-28T07:24:03.923002Z","shell.execute_reply":"2024-11-28T07:24:03.942753Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"device = 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.944980Z","iopub.execute_input":"2024-11-28T07:24:03.945316Z","iopub.status.idle":"2024-11-28T07:24:03.957077Z","shell.execute_reply.started":"2024-11-28T07:24:03.945281Z","shell.execute_reply":"2024-11-28T07:24:03.956412Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model = ReflexTransformer(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:03.958282Z","iopub.execute_input":"2024-11-28T07:24:03.958694Z","iopub.status.idle":"2024-11-28T07:24:05.266762Z","shell.execute_reply.started":"2024-11-28T07:24:03.958615Z","shell.execute_reply":"2024-11-28T07:24:05.265736Z"}},"outputs":[{"name":"stdout","text":"number of parameters: 33.88M\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"model.to(device)\nmodel.eval();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:05.267660Z","iopub.execute_input":"2024-11-28T07:24:05.267894Z","iopub.status.idle":"2024-11-28T07:24:05.596284Z","shell.execute_reply.started":"2024-11-28T07:24:05.267871Z","shell.execute_reply":"2024-11-28T07:24:05.595516Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"tokenizer = BPETokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:05.597111Z","iopub.execute_input":"2024-11-28T07:24:05.597398Z","iopub.status.idle":"2024-11-28T07:24:06.562383Z","shell.execute_reply.started":"2024-11-28T07:24:05.597365Z","shell.execute_reply":"2024-11-28T07:24:06.561239Z"}},"outputs":[{"name":"stdout","text":"downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json to /root/.cache/mingpt/encoder.json\ndownloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe to /root/.cache/mingpt/vocab.bpe\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"x = tokenizer('test test 1 2 3').to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:06.563513Z","iopub.execute_input":"2024-11-28T07:24:06.563883Z","iopub.status.idle":"2024-11-28T07:24:06.572397Z","shell.execute_reply.started":"2024-11-28T07:24:06.563841Z","shell.execute_reply":"2024-11-28T07:24:06.571171Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"y = model.generate(x, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:06.573426Z","iopub.execute_input":"2024-11-28T07:24:06.573761Z","iopub.status.idle":"2024-11-28T07:24:07.296568Z","shell.execute_reply.started":"2024-11-28T07:24:06.573723Z","shell.execute_reply":"2024-11-28T07:24:07.295719Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"for i in range(len(y)):\n    out = tokenizer.decode(y[0].cpu().squeeze())\n    print('-'*10)\n    print(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.298214Z","iopub.execute_input":"2024-11-28T07:24:07.298626Z","iopub.status.idle":"2024-11-28T07:24:07.304631Z","shell.execute_reply.started":"2024-11-28T07:24:07.298592Z","shell.execute_reply":"2024-11-28T07:24:07.303603Z"}},"outputs":[{"name":"stdout","text":"----------\ntest test 1 2 3hander Unleashed heartbeat sorcery antennas raplaughter forests Anchorage Scotland\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Training on openwebtext","metadata":{}},{"cell_type":"code","source":"max_iters=1000\nlog_interval=1\neval_interval=200\neval_iters=20\nlearning_rate=0.00008\ngradient_accumulation_steps=4\nbatch_size=8\ncompile=False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.310324Z","iopub.execute_input":"2024-11-28T07:24:07.310780Z","iopub.status.idle":"2024-11-28T07:24:07.315866Z","shell.execute_reply.started":"2024-11-28T07:24:07.310750Z","shell.execute_reply":"2024-11-28T07:24:07.315196Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"device = 'cuda' \ndtype = 'bfloat16'\ncompile = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.316720Z","iopub.execute_input":"2024-11-28T07:24:07.316933Z","iopub.status.idle":"2024-11-28T07:24:07.328370Z","shell.execute_reply.started":"2024-11-28T07:24:07.316906Z","shell.execute_reply":"2024-11-28T07:24:07.327474Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"torch.manual_seed(1337)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.329452Z","iopub.execute_input":"2024-11-28T07:24:07.329709Z","iopub.status.idle":"2024-11-28T07:24:07.338824Z","shell.execute_reply.started":"2024-11-28T07:24:07.329686Z","shell.execute_reply":"2024-11-28T07:24:07.338213Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import os\n# poor man's data loader\ndata_dir = os.path.join('/kaggle/input/openwebtext-data-prepared-for-nanogpt') # [ ] Removed ,dataset\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n    if split == 'train':\n        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.339709Z","iopub.execute_input":"2024-11-28T07:24:07.340018Z","iopub.status.idle":"2024-11-28T07:24:07.349781Z","shell.execute_reply.started":"2024-11-28T07:24:07.339982Z","shell.execute_reply":"2024-11-28T07:24:07.348875Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\niter_num = 0\nbest_val_loss = 1e9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.350738Z","iopub.execute_input":"2024-11-28T07:24:07.351045Z","iopub.status.idle":"2024-11-28T07:24:07.365031Z","shell.execute_reply.started":"2024-11-28T07:24:07.351008Z","shell.execute_reply":"2024-11-28T07:24:07.364367Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"device_type = 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.365987Z","iopub.execute_input":"2024-11-28T07:24:07.366279Z","iopub.status.idle":"2024-11-28T07:24:07.375795Z","shell.execute_reply.started":"2024-11-28T07:24:07.366255Z","shell.execute_reply":"2024-11-28T07:24:07.375029Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class Train_config():\n    weight_decay = 1e-1\n    betas = [0.9, 0.95]\n    learning_rate = learning_rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.376728Z","iopub.execute_input":"2024-11-28T07:24:07.376988Z","iopub.status.idle":"2024-11-28T07:24:07.386553Z","shell.execute_reply.started":"2024-11-28T07:24:07.376965Z","shell.execute_reply":"2024-11-28T07:24:07.385828Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"optimizer = model.configure_optimizers(Train_config()) # [ ] Remvoed device_type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:07.387590Z","iopub.execute_input":"2024-11-28T07:24:07.387903Z","iopub.status.idle":"2024-11-28T07:24:08.197930Z","shell.execute_reply.started":"2024-11-28T07:24:07.387868Z","shell.execute_reply":"2024-11-28T07:24:08.196946Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# initialize a GradScaler. If enabled=False scaler is a no-op\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.199060Z","iopub.execute_input":"2024-11-28T07:24:08.199423Z","iopub.status.idle":"2024-11-28T07:24:08.203686Z","shell.execute_reply.started":"2024-11-28T07:24:08.199397Z","shell.execute_reply":"2024-11-28T07:24:08.202837Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1972268714.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            with ctx:\n                logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.204589Z","iopub.execute_input":"2024-11-28T07:24:08.204893Z","iopub.status.idle":"2024-11-28T07:24:08.214516Z","shell.execute_reply.started":"2024-11-28T07:24:08.204869Z","shell.execute_reply":"2024-11-28T07:24:08.213735Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# learning rate decay scheduler (cosine with warmup)\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return learning_rate * it / warmup_iters\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return min_lr + coeff * (learning_rate - min_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.215430Z","iopub.execute_input":"2024-11-28T07:24:08.215690Z","iopub.status.idle":"2024-11-28T07:24:08.228667Z","shell.execute_reply.started":"2024-11-28T07:24:08.215666Z","shell.execute_reply":"2024-11-28T07:24:08.227802Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import time\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.229774Z","iopub.execute_input":"2024-11-28T07:24:08.230101Z","iopub.status.idle":"2024-11-28T07:24:08.238271Z","shell.execute_reply.started":"2024-11-28T07:24:08.230066Z","shell.execute_reply":"2024-11-28T07:24:08.237500Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"ddp = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.239166Z","iopub.execute_input":"2024-11-28T07:24:08.239546Z","iopub.status.idle":"2024-11-28T07:24:08.248077Z","shell.execute_reply.started":"2024-11-28T07:24:08.239494Z","shell.execute_reply":"2024-11-28T07:24:08.247373Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"master_procesc = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.249024Z","iopub.execute_input":"2024-11-28T07:24:08.249321Z","iopub.status.idle":"2024-11-28T07:24:08.258471Z","shell.execute_reply.started":"2024-11-28T07:24:08.249280Z","shell.execute_reply":"2024-11-28T07:24:08.257586Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"wandb_log = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.259412Z","iopub.execute_input":"2024-11-28T07:24:08.259679Z","iopub.status.idle":"2024-11-28T07:24:08.269266Z","shell.execute_reply.started":"2024-11-28T07:24:08.259656Z","shell.execute_reply":"2024-11-28T07:24:08.268395Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"eval_only = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.270023Z","iopub.execute_input":"2024-11-28T07:24:08.270277Z","iopub.status.idle":"2024-11-28T07:24:08.279397Z","shell.execute_reply.started":"2024-11-28T07:24:08.270253Z","shell.execute_reply":"2024-11-28T07:24:08.278671Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"block_size = config.block_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.280361Z","iopub.execute_input":"2024-11-28T07:24:08.280646Z","iopub.status.idle":"2024-11-28T07:24:08.290104Z","shell.execute_reply.started":"2024-11-28T07:24:08.280614Z","shell.execute_reply":"2024-11-28T07:24:08.289353Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"decay_lr = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.291078Z","iopub.execute_input":"2024-11-28T07:24:08.291357Z","iopub.status.idle":"2024-11-28T07:24:08.300482Z","shell.execute_reply.started":"2024-11-28T07:24:08.291333Z","shell.execute_reply":"2024-11-28T07:24:08.299598Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"warmup_iters = 2000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.301483Z","iopub.execute_input":"2024-11-28T07:24:08.301723Z","iopub.status.idle":"2024-11-28T07:24:08.311142Z","shell.execute_reply.started":"2024-11-28T07:24:08.301700Z","shell.execute_reply":"2024-11-28T07:24:08.310491Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"master_process = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.311985Z","iopub.execute_input":"2024-11-28T07:24:08.312235Z","iopub.status.idle":"2024-11-28T07:24:08.321490Z","shell.execute_reply.started":"2024-11-28T07:24:08.312194Z","shell.execute_reply":"2024-11-28T07:24:08.320687Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.322404Z","iopub.execute_input":"2024-11-28T07:24:08.322658Z","iopub.status.idle":"2024-11-28T07:24:08.332753Z","shell.execute_reply.started":"2024-11-28T07:24:08.322635Z","shell.execute_reply":"2024-11-28T07:24:08.331943Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.333550Z","iopub.execute_input":"2024-11-28T07:24:08.333788Z","iopub.status.idle":"2024-11-28T07:24:08.343379Z","shell.execute_reply.started":"2024-11-28T07:24:08.333762Z","shell.execute_reply":"2024-11-28T07:24:08.342568Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.344269Z","iopub.execute_input":"2024-11-28T07:24:08.344576Z","iopub.status.idle":"2024-11-28T07:24:08.354988Z","shell.execute_reply.started":"2024-11-28T07:24:08.344540Z","shell.execute_reply":"2024-11-28T07:24:08.354241Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n                  bias=False, vocab_size=None, dropout=0) # start with model_args from command line\n# [ ] dropout is set to 0\n# [ ] bias is set to False\nout_dir = '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.355762Z","iopub.execute_input":"2024-11-28T07:24:08.355989Z","iopub.status.idle":"2024-11-28T07:24:08.366139Z","shell.execute_reply.started":"2024-11-28T07:24:08.355966Z","shell.execute_reply":"2024-11-28T07:24:08.365412Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# training loop\nX, Y = get_batch('train') # fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0 # number of iterations in the lifetime of this process\n\nraw_model = model.module if ddp else model # unwrap DDP container if needed\nrunning_mfu = -1.0\nwhile True:\n    # determine and set the learning rate for this iteration\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num % eval_interval == 0 and master_process:\n        losses = estimate_loss()\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        if wandb_log:\n            wandb.log({\n                \"iter\": iter_num,\n                \"train/loss\": losses['train'],\n                \"val/loss\": losses['val'],\n                \"lr\": lr,\n                \"mfu\": running_mfu*100, # convert to percentage\n            })\n        if losses['val'] < best_val_loss or always_save_checkpoint:\n            best_val_loss = losses['val']\n            if iter_num > 0:\n                checkpoint = {\n                    'model': raw_model.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'model_args': model_args,\n                    'iter_num': iter_num,\n                    'best_val_loss': best_val_loss,\n                    'config': config,\n                }\n                print(f\"saving checkpoint to {out_dir}\")\n                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n    if iter_num == 0 and eval_only:\n        break\n\n    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(gradient_accumulation_steps):\n        if ddp:\n            # in DDP training we only need to sync gradients at the last micro step.\n            # the official way to do this is with model.no_sync() context manager, but\n            # I really dislike that this bloats the code and forces us to repeat code\n            # looking at the source of that context manager, it just toggles this variable\n            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n        with ctx:\n            logits, loss = model(X, Y)\n            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch('train')\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n    # clip the gradient\n    if grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0 and master_process:\n        # get loss as float. note: this is a CPU-GPU sync point\n        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n        lossf = loss.item() * gradient_accumulation_steps\n        if local_iter_num >= 5: # let the training loop settle a bit\n            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n    iter_num += 1\n    local_iter_num += 1\n\n    # termination conditions\n    if iter_num > max_iters:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:24:08.367339Z","iopub.execute_input":"2024-11-28T07:24:08.367830Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 10.8755, val loss 10.8765\niter 0: loss 10.8859, time 14039.24ms, mfu -100.00%\niter 1: loss 10.8844, time 3003.89ms, mfu -100.00%\niter 2: loss 10.8809, time 3182.72ms, mfu -100.00%\niter 3: loss 10.8683, time 3185.36ms, mfu -100.00%\niter 4: loss 10.8561, time 3185.62ms, mfu -100.00%\niter 5: loss 10.8697, time 3186.13ms, mfu 1.17%\niter 6: loss 10.8722, time 3187.01ms, mfu 1.17%\niter 7: loss 10.8686, time 3188.96ms, mfu 1.17%\niter 8: loss 10.8495, time 3185.75ms, mfu 1.17%\niter 9: loss 10.8839, time 3186.29ms, mfu 1.17%\niter 10: loss 10.8470, time 3185.42ms, mfu 1.17%\niter 11: loss 10.8458, time 3185.62ms, mfu 1.17%\niter 12: loss 10.8541, time 3186.86ms, mfu 1.17%\niter 13: loss 10.8564, time 3185.36ms, mfu 1.17%\niter 14: loss 10.8280, time 3187.90ms, mfu 1.17%\niter 15: loss 10.8265, time 3186.71ms, mfu 1.17%\niter 16: loss 10.8274, time 3188.81ms, mfu 1.17%\niter 17: loss 10.8211, time 3184.12ms, mfu 1.17%\niter 18: loss 10.8010, time 3188.07ms, mfu 1.17%\niter 19: loss 10.8138, time 3187.96ms, mfu 1.17%\niter 20: loss 10.8000, time 3187.68ms, mfu 1.17%\niter 21: loss 10.7918, time 3185.99ms, mfu 1.17%\niter 22: loss 10.7906, time 3186.76ms, mfu 1.17%\niter 23: loss 10.7781, time 3186.31ms, mfu 1.17%\niter 24: loss 10.7501, time 3186.77ms, mfu 1.17%\niter 25: loss 10.7414, time 3187.71ms, mfu 1.17%\niter 26: loss 10.7451, time 3188.81ms, mfu 1.17%\niter 27: loss 10.7261, time 3180.58ms, mfu 1.17%\niter 28: loss 10.7217, time 3187.39ms, mfu 1.17%\niter 29: loss 10.7143, time 3186.79ms, mfu 1.17%\niter 30: loss 10.7231, time 3182.83ms, mfu 1.17%\niter 31: loss 10.6799, time 3187.87ms, mfu 1.17%\niter 32: loss 10.7026, time 3187.25ms, mfu 1.17%\niter 33: loss 10.6842, time 3187.82ms, mfu 1.17%\niter 34: loss 10.6777, time 3186.87ms, mfu 1.17%\niter 35: loss 10.6671, time 3186.71ms, mfu 1.17%\niter 36: loss 10.6491, time 3185.82ms, mfu 1.17%\niter 37: loss 10.6349, time 3187.41ms, mfu 1.17%\niter 38: loss 10.6211, time 3185.64ms, mfu 1.17%\niter 39: loss 10.6034, time 3187.09ms, mfu 1.17%\niter 40: loss 10.6069, time 3185.45ms, mfu 1.17%\niter 41: loss 10.6203, time 3185.65ms, mfu 1.17%\niter 42: loss 10.6209, time 3188.19ms, mfu 1.17%\niter 43: loss 10.6222, time 3187.72ms, mfu 1.17%\niter 44: loss 10.5752, time 3186.17ms, mfu 1.17%\niter 45: loss 10.5783, time 3187.11ms, mfu 1.17%\niter 46: loss 10.5324, time 3186.59ms, mfu 1.17%\niter 47: loss 10.5560, time 3187.31ms, mfu 1.17%\niter 48: loss 10.5372, time 3186.53ms, mfu 1.17%\niter 49: loss 10.5065, time 3186.28ms, mfu 1.17%\niter 50: loss 10.5205, time 3187.89ms, mfu 1.17%\niter 51: loss 10.5051, time 3187.86ms, mfu 1.17%\niter 52: loss 10.4969, time 3187.57ms, mfu 1.17%\niter 53: loss 10.4396, time 3184.27ms, mfu 1.17%\niter 54: loss 10.4514, time 3187.41ms, mfu 1.17%\niter 55: loss 10.4755, time 3183.05ms, mfu 1.17%\niter 56: loss 10.4568, time 3186.47ms, mfu 1.17%\niter 57: loss 10.4172, time 3187.88ms, mfu 1.17%\niter 58: loss 10.3974, time 3186.22ms, mfu 1.17%\niter 59: loss 10.4062, time 3186.16ms, mfu 1.17%\niter 60: loss 10.4153, time 3185.60ms, mfu 1.17%\niter 61: loss 10.3881, time 3187.32ms, mfu 1.17%\niter 62: loss 10.3839, time 3187.65ms, mfu 1.17%\niter 63: loss 10.3603, time 3188.34ms, mfu 1.17%\niter 64: loss 10.3975, time 3185.95ms, mfu 1.17%\niter 65: loss 10.3501, time 3189.34ms, mfu 1.17%\niter 66: loss 10.3591, time 3185.03ms, mfu 1.17%\niter 67: loss 10.3268, time 3186.49ms, mfu 1.17%\niter 68: loss 10.3368, time 3186.36ms, mfu 1.17%\niter 69: loss 10.3530, time 3187.99ms, mfu 1.17%\niter 70: loss 10.2810, time 3187.26ms, mfu 1.17%\niter 71: loss 10.2875, time 3187.40ms, mfu 1.17%\niter 72: loss 10.3316, time 3187.73ms, mfu 1.17%\niter 73: loss 10.3088, time 3188.12ms, mfu 1.17%\niter 74: loss 10.2305, time 3187.74ms, mfu 1.17%\niter 75: loss 10.2703, time 3188.01ms, mfu 1.17%\niter 76: loss 10.3125, time 3184.95ms, mfu 1.17%\niter 77: loss 10.2139, time 3188.81ms, mfu 1.17%\niter 78: loss 10.2776, time 3186.73ms, mfu 1.17%\niter 79: loss 10.2578, time 3186.77ms, mfu 1.17%\niter 80: loss 10.3174, time 3187.15ms, mfu 1.17%\niter 81: loss 10.2474, time 3188.18ms, mfu 1.17%\niter 82: loss 10.2463, time 3186.57ms, mfu 1.17%\niter 83: loss 10.2317, time 3186.95ms, mfu 1.17%\niter 84: loss 10.2925, time 3186.50ms, mfu 1.17%\niter 85: loss 10.2478, time 3189.12ms, mfu 1.17%\niter 86: loss 10.1621, time 3187.58ms, mfu 1.17%\niter 87: loss 10.1245, time 3187.71ms, mfu 1.17%\niter 88: loss 10.2881, time 3188.35ms, mfu 1.17%\niter 89: loss 10.1960, time 3188.01ms, mfu 1.17%\niter 90: loss 10.2111, time 3188.76ms, mfu 1.17%\niter 91: loss 10.2504, time 3187.12ms, mfu 1.17%\niter 92: loss 10.2010, time 3188.28ms, mfu 1.17%\niter 93: loss 10.1712, time 3187.14ms, mfu 1.17%\niter 94: loss 10.2459, time 3186.90ms, mfu 1.17%\niter 95: loss 10.2335, time 3186.60ms, mfu 1.17%\niter 96: loss 10.1287, time 3188.32ms, mfu 1.17%\niter 97: loss 10.1361, time 3188.18ms, mfu 1.17%\niter 98: loss 10.1577, time 3184.81ms, mfu 1.17%\niter 99: loss 10.1452, time 3186.04ms, mfu 1.17%\niter 100: loss 10.1563, time 3188.58ms, mfu 1.17%\niter 101: loss 10.2175, time 3186.66ms, mfu 1.17%\niter 102: loss 10.1472, time 3188.24ms, mfu 1.17%\niter 103: loss 10.1450, time 3188.01ms, mfu 1.17%\niter 104: loss 10.1806, time 3186.41ms, mfu 1.17%\niter 105: loss 10.1287, time 3185.29ms, mfu 1.17%\niter 106: loss 10.0490, time 3187.68ms, mfu 1.17%\niter 107: loss 10.1768, time 3188.42ms, mfu 1.17%\niter 108: loss 10.1357, time 3187.69ms, mfu 1.17%\niter 109: loss 10.1169, time 3188.45ms, mfu 1.17%\niter 110: loss 10.1630, time 3186.14ms, mfu 1.17%\niter 111: loss 10.1494, time 3186.83ms, mfu 1.17%\niter 112: loss 10.1012, time 3188.13ms, mfu 1.17%\niter 113: loss 10.0823, time 3187.95ms, mfu 1.17%\niter 114: loss 10.1876, time 3188.68ms, mfu 1.17%\niter 115: loss 10.1135, time 3187.45ms, mfu 1.17%\niter 116: loss 10.0269, time 3188.67ms, mfu 1.17%\niter 117: loss 9.9879, time 3187.32ms, mfu 1.17%\niter 118: loss 10.0566, time 3188.80ms, mfu 1.17%\niter 119: loss 10.1963, time 3188.52ms, mfu 1.17%\niter 120: loss 10.2101, time 3187.04ms, mfu 1.17%\niter 121: loss 10.0045, time 3188.53ms, mfu 1.17%\niter 122: loss 10.0768, time 3186.84ms, mfu 1.17%\niter 123: loss 10.1243, time 3188.27ms, mfu 1.17%\niter 124: loss 10.1185, time 3189.34ms, mfu 1.17%\niter 125: loss 10.1070, time 3188.27ms, mfu 1.17%\niter 126: loss 10.0840, time 3186.42ms, mfu 1.17%\niter 127: loss 10.0619, time 3188.82ms, mfu 1.17%\niter 128: loss 10.2244, time 3188.26ms, mfu 1.17%\niter 129: loss 10.0381, time 3188.90ms, mfu 1.17%\niter 130: loss 10.0376, time 3187.25ms, mfu 1.17%\niter 131: loss 10.0307, time 3188.44ms, mfu 1.17%\niter 132: loss 10.0476, time 3186.85ms, mfu 1.17%\niter 133: loss 10.0842, time 3187.35ms, mfu 1.17%\niter 134: loss 10.0592, time 3189.82ms, mfu 1.17%\niter 135: loss 10.0286, time 3187.72ms, mfu 1.17%\niter 136: loss 10.0314, time 3187.89ms, mfu 1.17%\niter 137: loss 10.0800, time 3188.83ms, mfu 1.17%\niter 138: loss 10.0302, time 3188.96ms, mfu 1.17%\niter 139: loss 10.0278, time 3188.49ms, mfu 1.17%\niter 140: loss 10.0422, time 3188.03ms, mfu 1.17%\niter 141: loss 10.1011, time 3189.24ms, mfu 1.17%\niter 142: loss 10.1503, time 3189.09ms, mfu 1.17%\niter 143: loss 10.0927, time 3189.39ms, mfu 1.17%\niter 144: loss 10.0822, time 3186.97ms, mfu 1.17%\niter 145: loss 10.0172, time 3185.37ms, mfu 1.17%\niter 146: loss 10.0579, time 3188.67ms, mfu 1.17%\niter 147: loss 10.0017, time 3189.20ms, mfu 1.17%\niter 148: loss 10.0835, time 3188.64ms, mfu 1.17%\niter 149: loss 10.0366, time 3187.65ms, mfu 1.17%\niter 150: loss 10.0090, time 3188.65ms, mfu 1.17%\niter 151: loss 9.9163, time 3188.47ms, mfu 1.17%\niter 152: loss 10.1096, time 3187.69ms, mfu 1.17%\niter 153: loss 9.9779, time 3189.17ms, mfu 1.17%\niter 154: loss 9.9163, time 3189.78ms, mfu 1.17%\niter 155: loss 9.9725, time 3184.62ms, mfu 1.17%\niter 156: loss 10.0285, time 3188.03ms, mfu 1.17%\niter 157: loss 9.9965, time 3188.96ms, mfu 1.17%\niter 158: loss 10.0148, time 3188.91ms, mfu 1.17%\niter 159: loss 9.9539, time 3187.30ms, mfu 1.17%\niter 160: loss 9.9671, time 3189.49ms, mfu 1.17%\niter 161: loss 9.8864, time 3188.84ms, mfu 1.17%\niter 162: loss 9.8942, time 3189.19ms, mfu 1.17%\niter 163: loss 10.0166, time 3184.92ms, mfu 1.17%\niter 164: loss 9.8857, time 3190.77ms, mfu 1.17%\niter 165: loss 9.8498, time 3186.93ms, mfu 1.17%\niter 166: loss 9.9778, time 3188.75ms, mfu 1.17%\niter 167: loss 9.8757, time 3188.86ms, mfu 1.17%\niter 168: loss 9.8989, time 3189.08ms, mfu 1.17%\niter 169: loss 9.9299, time 3189.79ms, mfu 1.17%\niter 170: loss 9.8964, time 3186.70ms, mfu 1.17%\niter 171: loss 9.9449, time 3187.53ms, mfu 1.17%\niter 172: loss 9.8592, time 3189.42ms, mfu 1.17%\niter 173: loss 9.8728, time 3189.24ms, mfu 1.17%\niter 174: loss 9.8530, time 3188.25ms, mfu 1.17%\niter 175: loss 9.8167, time 3189.31ms, mfu 1.17%\niter 176: loss 9.8252, time 3189.36ms, mfu 1.17%\niter 177: loss 9.8660, time 3187.91ms, mfu 1.17%\niter 178: loss 9.8995, time 3189.04ms, mfu 1.17%\niter 179: loss 9.8618, time 3185.72ms, mfu 1.17%\niter 180: loss 9.8340, time 3188.25ms, mfu 1.17%\niter 181: loss 9.8144, time 3187.80ms, mfu 1.17%\niter 182: loss 9.8353, time 3189.08ms, mfu 1.17%\niter 183: loss 9.8106, time 3189.50ms, mfu 1.17%\niter 184: loss 9.6938, time 3187.05ms, mfu 1.17%\niter 185: loss 9.8240, time 3187.37ms, mfu 1.17%\niter 186: loss 9.8380, time 3189.94ms, mfu 1.17%\niter 187: loss 9.7623, time 3189.58ms, mfu 1.17%\niter 188: loss 9.8438, time 3187.73ms, mfu 1.17%\niter 189: loss 9.7054, time 3186.07ms, mfu 1.17%\niter 190: loss 9.7772, time 3188.16ms, mfu 1.17%\niter 191: loss 9.7544, time 3188.46ms, mfu 1.17%\niter 192: loss 9.7443, time 3187.43ms, mfu 1.17%\niter 193: loss 9.7163, time 3189.07ms, mfu 1.17%\niter 194: loss 9.6978, time 3187.33ms, mfu 1.17%\niter 195: loss 9.7680, time 3189.25ms, mfu 1.17%\niter 196: loss 9.7353, time 3187.70ms, mfu 1.17%\niter 197: loss 9.7382, time 3189.09ms, mfu 1.17%\niter 198: loss 9.7272, time 3188.25ms, mfu 1.17%\niter 199: loss 9.7659, time 3189.37ms, mfu 1.17%\nstep 200: train loss 9.7440, val loss 9.7583\nsaving checkpoint to /kaggle/working/\niter 200: loss 9.6790, time 14927.82ms, mfu 1.08%\niter 201: loss 9.6671, time 3188.80ms, mfu 1.08%\niter 202: loss 9.7299, time 3193.56ms, mfu 1.09%\niter 203: loss 9.6731, time 3176.83ms, mfu 1.10%\niter 204: loss 9.6981, time 3189.64ms, mfu 1.11%\niter 205: loss 9.6992, time 3188.32ms, mfu 1.11%\niter 206: loss 9.7946, time 3188.28ms, mfu 1.12%\niter 207: loss 9.6909, time 3189.40ms, mfu 1.12%\niter 208: loss 9.6810, time 3189.64ms, mfu 1.13%\niter 209: loss 9.8506, time 3189.71ms, mfu 1.13%\niter 210: loss 9.6379, time 3189.34ms, mfu 1.14%\niter 211: loss 9.6786, time 3188.99ms, mfu 1.14%\niter 212: loss 9.6458, time 3186.30ms, mfu 1.14%\niter 213: loss 9.6670, time 3189.12ms, mfu 1.14%\niter 214: loss 9.7501, time 3187.50ms, mfu 1.15%\niter 215: loss 9.6873, time 3190.47ms, mfu 1.15%\niter 216: loss 9.7407, time 3188.18ms, mfu 1.15%\niter 217: loss 9.8072, time 3189.68ms, mfu 1.15%\niter 218: loss 9.6473, time 3188.73ms, mfu 1.15%\niter 219: loss 9.6907, time 3189.66ms, mfu 1.16%\niter 220: loss 9.5653, time 3188.60ms, mfu 1.16%\niter 221: loss 9.7824, time 3187.35ms, mfu 1.16%\niter 222: loss 9.6013, time 3189.86ms, mfu 1.16%\niter 223: loss 9.7612, time 3189.64ms, mfu 1.16%\niter 224: loss 9.5408, time 3187.57ms, mfu 1.16%\niter 225: loss 9.7387, time 3189.60ms, mfu 1.16%\niter 226: loss 9.6025, time 3186.81ms, mfu 1.16%\niter 227: loss 9.6231, time 3186.15ms, mfu 1.16%\niter 228: loss 9.6348, time 3188.82ms, mfu 1.16%\niter 229: loss 9.6034, time 3189.71ms, mfu 1.16%\niter 230: loss 9.6461, time 3189.31ms, mfu 1.16%\niter 231: loss 9.5281, time 3189.59ms, mfu 1.16%\niter 232: loss 9.5602, time 3186.51ms, mfu 1.16%\niter 233: loss 9.6216, time 3187.49ms, mfu 1.16%\niter 234: loss 9.5361, time 3188.34ms, mfu 1.17%\niter 235: loss 9.5553, time 3190.25ms, mfu 1.17%\niter 236: loss 9.5594, time 3189.62ms, mfu 1.17%\niter 237: loss 9.6153, time 3190.18ms, mfu 1.17%\niter 238: loss 9.5026, time 3189.81ms, mfu 1.17%\niter 239: loss 9.5256, time 3189.64ms, mfu 1.17%\niter 240: loss 9.5996, time 3188.79ms, mfu 1.17%\niter 241: loss 9.5556, time 3190.05ms, mfu 1.17%\niter 242: loss 9.6519, time 3189.95ms, mfu 1.17%\niter 243: loss 9.5442, time 3189.61ms, mfu 1.17%\niter 244: loss 9.3574, time 3187.82ms, mfu 1.17%\niter 245: loss 9.5582, time 3188.17ms, mfu 1.17%\niter 246: loss 9.5777, time 3188.69ms, mfu 1.17%\niter 247: loss 9.5675, time 3189.83ms, mfu 1.17%\niter 248: loss 9.3759, time 3190.05ms, mfu 1.17%\niter 249: loss 9.4708, time 3187.86ms, mfu 1.17%\niter 250: loss 9.4445, time 3188.90ms, mfu 1.17%\niter 251: loss 9.3890, time 3189.49ms, mfu 1.17%\niter 252: loss 9.5559, time 3188.51ms, mfu 1.17%\niter 253: loss 9.4623, time 3188.60ms, mfu 1.17%\niter 254: loss 9.5051, time 3190.76ms, mfu 1.17%\niter 255: loss 9.4114, time 3189.66ms, mfu 1.17%\niter 256: loss 9.4653, time 3189.35ms, mfu 1.17%\niter 257: loss 9.5181, time 3190.16ms, mfu 1.17%\niter 258: loss 9.2362, time 3188.67ms, mfu 1.17%\niter 259: loss 9.4194, time 3191.26ms, mfu 1.17%\niter 260: loss 9.4385, time 3185.65ms, mfu 1.17%\niter 261: loss 9.3939, time 3190.16ms, mfu 1.17%\niter 262: loss 9.2810, time 3188.55ms, mfu 1.17%\niter 263: loss 9.3410, time 3187.19ms, mfu 1.17%\niter 264: loss 9.3275, time 3188.93ms, mfu 1.17%\niter 265: loss 9.4769, time 3190.74ms, mfu 1.17%\niter 266: loss 9.3857, time 3190.33ms, mfu 1.17%\niter 267: loss 9.3003, time 3190.42ms, mfu 1.17%\niter 268: loss 9.3547, time 3188.97ms, mfu 1.17%\niter 269: loss 9.2962, time 3190.02ms, mfu 1.17%\niter 270: loss 9.4564, time 3188.39ms, mfu 1.17%\niter 271: loss 9.3984, time 3189.91ms, mfu 1.17%\niter 272: loss 9.4149, time 3188.95ms, mfu 1.17%\niter 273: loss 9.3719, time 3190.50ms, mfu 1.17%\niter 274: loss 9.3019, time 3189.12ms, mfu 1.17%\niter 275: loss 9.3625, time 3190.24ms, mfu 1.17%\niter 276: loss 9.2987, time 3190.28ms, mfu 1.17%\niter 277: loss 9.3827, time 3190.53ms, mfu 1.17%\niter 278: loss 9.2694, time 3188.41ms, mfu 1.17%\niter 279: loss 9.4834, time 3190.07ms, mfu 1.17%\niter 280: loss 9.2658, time 3189.33ms, mfu 1.17%\niter 281: loss 9.2972, time 3186.42ms, mfu 1.17%\niter 282: loss 9.3659, time 3188.60ms, mfu 1.17%\niter 283: loss 9.1954, time 3188.61ms, mfu 1.17%\niter 284: loss 9.2491, time 3188.76ms, mfu 1.17%\niter 285: loss 9.3586, time 3188.63ms, mfu 1.17%\niter 286: loss 9.2526, time 3190.48ms, mfu 1.17%\niter 287: loss 9.2488, time 3189.76ms, mfu 1.17%\niter 288: loss 9.3572, time 3190.49ms, mfu 1.17%\niter 289: loss 9.1689, time 3189.88ms, mfu 1.17%\niter 290: loss 9.2407, time 3188.91ms, mfu 1.17%\niter 291: loss 9.2677, time 3190.53ms, mfu 1.17%\niter 292: loss 9.3162, time 3188.92ms, mfu 1.17%\niter 293: loss 9.2382, time 3190.72ms, mfu 1.17%\niter 294: loss 9.2321, time 3191.00ms, mfu 1.17%\niter 295: loss 9.3864, time 3190.37ms, mfu 1.17%\niter 296: loss 9.2539, time 3188.86ms, mfu 1.17%\niter 297: loss 9.1841, time 3189.47ms, mfu 1.17%\niter 298: loss 9.2270, time 3188.36ms, mfu 1.17%\niter 299: loss 9.2382, time 3190.37ms, mfu 1.17%\niter 300: loss 9.2715, time 3188.07ms, mfu 1.17%\niter 301: loss 9.2780, time 3188.64ms, mfu 1.17%\niter 302: loss 9.1303, time 3190.37ms, mfu 1.17%\niter 303: loss 9.1150, time 3191.40ms, mfu 1.17%\niter 304: loss 9.1760, time 3190.39ms, mfu 1.17%\niter 305: loss 9.2594, time 3188.61ms, mfu 1.17%\niter 306: loss 9.0971, time 3190.26ms, mfu 1.17%\niter 307: loss 9.0938, time 3190.04ms, mfu 1.17%\niter 308: loss 9.0382, time 3190.40ms, mfu 1.17%\niter 309: loss 9.2144, time 3190.63ms, mfu 1.17%\niter 310: loss 9.1337, time 3189.20ms, mfu 1.17%\niter 311: loss 9.0889, time 3188.97ms, mfu 1.17%\niter 312: loss 9.0971, time 3190.65ms, mfu 1.17%\niter 313: loss 9.0776, time 3191.43ms, mfu 1.17%\niter 314: loss 9.1261, time 3189.84ms, mfu 1.17%\niter 315: loss 9.1070, time 3190.87ms, mfu 1.17%\niter 316: loss 9.0713, time 3190.34ms, mfu 1.17%\niter 317: loss 9.0694, time 3187.78ms, mfu 1.17%\niter 318: loss 9.2318, time 3189.66ms, mfu 1.17%\niter 319: loss 9.1568, time 3190.66ms, mfu 1.17%\niter 320: loss 9.1042, time 3189.95ms, mfu 1.17%\niter 321: loss 9.0621, time 3190.78ms, mfu 1.17%\niter 322: loss 8.9935, time 3190.31ms, mfu 1.17%\niter 323: loss 9.0413, time 3188.96ms, mfu 1.17%\niter 324: loss 9.1328, time 3189.16ms, mfu 1.17%\niter 325: loss 9.2134, time 3188.92ms, mfu 1.17%\niter 326: loss 9.0973, time 3190.37ms, mfu 1.17%\niter 327: loss 9.1897, time 3189.26ms, mfu 1.17%\niter 328: loss 9.1886, time 3190.81ms, mfu 1.17%\niter 329: loss 9.0088, time 3189.29ms, mfu 1.17%\niter 330: loss 9.1157, time 3190.25ms, mfu 1.17%\niter 331: loss 9.1779, time 3190.68ms, mfu 1.17%\niter 332: loss 9.0370, time 3189.45ms, mfu 1.17%\niter 333: loss 8.9589, time 3190.36ms, mfu 1.17%\niter 334: loss 8.9672, time 3190.87ms, mfu 1.17%\niter 335: loss 9.0936, time 3188.90ms, mfu 1.17%\niter 336: loss 8.9695, time 3190.90ms, mfu 1.17%\niter 337: loss 9.0838, time 3190.16ms, mfu 1.17%\niter 338: loss 9.0313, time 3191.00ms, mfu 1.17%\niter 339: loss 9.0005, time 3189.49ms, mfu 1.17%\niter 340: loss 8.9811, time 3187.42ms, mfu 1.17%\niter 341: loss 9.0019, time 3190.70ms, mfu 1.17%\niter 342: loss 9.1169, time 3189.40ms, mfu 1.17%\niter 343: loss 8.9201, time 3188.80ms, mfu 1.17%\niter 344: loss 8.9858, time 3188.67ms, mfu 1.17%\niter 345: loss 8.9478, time 3190.77ms, mfu 1.17%\niter 346: loss 8.9544, time 3188.48ms, mfu 1.17%\niter 347: loss 8.9428, time 3187.86ms, mfu 1.17%\niter 348: loss 8.9871, time 3186.20ms, mfu 1.17%\niter 349: loss 9.0516, time 3190.47ms, mfu 1.17%\niter 350: loss 8.8355, time 3189.52ms, mfu 1.17%\niter 351: loss 9.0120, time 3189.57ms, mfu 1.17%\niter 352: loss 9.1258, time 3190.66ms, mfu 1.17%\niter 353: loss 9.0434, time 3192.14ms, mfu 1.17%\niter 354: loss 8.9708, time 3187.46ms, mfu 1.17%\niter 355: loss 9.0049, time 3188.96ms, mfu 1.17%\niter 356: loss 8.8781, time 3189.22ms, mfu 1.17%\niter 357: loss 8.8584, time 3191.22ms, mfu 1.17%\niter 358: loss 8.9333, time 3191.06ms, mfu 1.17%\niter 359: loss 8.9073, time 3189.98ms, mfu 1.17%\niter 360: loss 8.8591, time 3190.63ms, mfu 1.17%\niter 361: loss 8.9560, time 3188.98ms, mfu 1.17%\niter 362: loss 8.8464, time 3191.42ms, mfu 1.17%\niter 363: loss 8.8113, time 3191.28ms, mfu 1.17%\niter 364: loss 8.9827, time 3190.57ms, mfu 1.17%\niter 365: loss 8.8592, time 3189.68ms, mfu 1.17%\niter 366: loss 8.9004, time 3189.81ms, mfu 1.17%\niter 367: loss 8.9031, time 3191.36ms, mfu 1.17%\niter 368: loss 8.7492, time 3189.53ms, mfu 1.17%\niter 369: loss 8.7648, time 3190.60ms, mfu 1.17%\niter 370: loss 9.0819, time 3190.47ms, mfu 1.17%\niter 371: loss 8.8903, time 3187.68ms, mfu 1.17%\niter 372: loss 8.9201, time 3191.25ms, mfu 1.17%\niter 373: loss 8.9472, time 3190.93ms, mfu 1.17%\niter 374: loss 8.7435, time 3189.47ms, mfu 1.17%\niter 375: loss 8.7325, time 3190.88ms, mfu 1.17%\niter 376: loss 8.7876, time 3189.76ms, mfu 1.17%\niter 377: loss 8.7966, time 3189.79ms, mfu 1.17%\niter 378: loss 8.7683, time 3188.72ms, mfu 1.17%\niter 379: loss 8.7714, time 3190.87ms, mfu 1.17%\niter 380: loss 8.7561, time 3189.26ms, mfu 1.17%\niter 381: loss 8.8548, time 3188.12ms, mfu 1.17%\niter 382: loss 8.7234, time 3190.85ms, mfu 1.17%\niter 383: loss 8.8577, time 3190.33ms, mfu 1.17%\niter 384: loss 8.7152, time 3187.83ms, mfu 1.17%\niter 385: loss 8.7325, time 3189.06ms, mfu 1.17%\niter 386: loss 8.7586, time 3190.63ms, mfu 1.17%\niter 387: loss 8.5955, time 3189.89ms, mfu 1.17%\niter 388: loss 8.6565, time 3191.02ms, mfu 1.17%\niter 389: loss 8.6156, time 3187.95ms, mfu 1.17%\niter 390: loss 8.6609, time 3191.28ms, mfu 1.17%\niter 391: loss 8.7109, time 3189.35ms, mfu 1.17%\niter 392: loss 8.8339, time 3191.00ms, mfu 1.17%\niter 393: loss 8.6711, time 3191.05ms, mfu 1.17%\niter 394: loss 8.7482, time 3189.20ms, mfu 1.17%\niter 395: loss 8.5891, time 3190.83ms, mfu 1.17%\niter 396: loss 8.8047, time 3189.32ms, mfu 1.17%\niter 397: loss 8.7851, time 3191.44ms, mfu 1.17%\niter 398: loss 8.6083, time 3190.55ms, mfu 1.17%\niter 399: loss 8.5803, time 3190.66ms, mfu 1.17%\nstep 400: train loss 8.6308, val loss 8.6645\nsaving checkpoint to /kaggle/working/\niter 400: loss 8.5982, time 15722.77ms, mfu 1.07%\niter 401: loss 8.6447, time 3189.77ms, mfu 1.08%\niter 402: loss 8.6025, time 3191.00ms, mfu 1.09%\niter 403: loss 8.7220, time 3193.33ms, mfu 1.10%\niter 404: loss 8.7943, time 3189.88ms, mfu 1.11%\niter 405: loss 8.6356, time 3191.46ms, mfu 1.11%\niter 406: loss 8.6488, time 3191.90ms, mfu 1.12%\niter 407: loss 8.6957, time 3190.87ms, mfu 1.12%\niter 408: loss 8.8979, time 3189.66ms, mfu 1.13%\niter 409: loss 8.6581, time 3188.33ms, mfu 1.13%\niter 410: loss 8.5550, time 3191.45ms, mfu 1.13%\niter 411: loss 8.6819, time 3189.79ms, mfu 1.14%\niter 412: loss 8.5347, time 3190.04ms, mfu 1.14%\niter 413: loss 8.7669, time 3188.00ms, mfu 1.14%\niter 414: loss 8.6071, time 3191.13ms, mfu 1.15%\niter 415: loss 8.7260, time 3190.39ms, mfu 1.15%\niter 416: loss 8.4138, time 3191.03ms, mfu 1.15%\niter 417: loss 8.4754, time 3189.11ms, mfu 1.15%\niter 418: loss 8.4960, time 3190.12ms, mfu 1.15%\niter 419: loss 8.7102, time 3189.56ms, mfu 1.15%\niter 420: loss 8.5836, time 3189.49ms, mfu 1.16%\niter 421: loss 8.5302, time 3192.71ms, mfu 1.16%\niter 422: loss 8.5344, time 3191.33ms, mfu 1.16%\niter 423: loss 8.4436, time 3193.59ms, mfu 1.16%\niter 424: loss 8.5789, time 3187.69ms, mfu 1.16%\niter 425: loss 8.6579, time 3189.80ms, mfu 1.16%\niter 426: loss 8.6049, time 3189.67ms, mfu 1.16%\niter 427: loss 8.5369, time 3189.60ms, mfu 1.16%\niter 428: loss 8.4487, time 3191.33ms, mfu 1.16%\niter 429: loss 8.6246, time 3191.41ms, mfu 1.16%\niter 430: loss 8.5930, time 3190.74ms, mfu 1.16%\niter 431: loss 8.3955, time 3190.07ms, mfu 1.16%\niter 432: loss 8.4266, time 3192.12ms, mfu 1.16%\niter 433: loss 8.3313, time 3191.81ms, mfu 1.16%\niter 434: loss 8.5005, time 3191.10ms, mfu 1.16%\niter 435: loss 8.5573, time 3190.02ms, mfu 1.16%\niter 436: loss 8.5227, time 3190.03ms, mfu 1.16%\niter 437: loss 8.3850, time 3189.21ms, mfu 1.16%\niter 438: loss 8.3665, time 3190.04ms, mfu 1.17%\niter 439: loss 8.4947, time 3191.27ms, mfu 1.17%\niter 440: loss 8.2907, time 3190.91ms, mfu 1.17%\niter 441: loss 8.3923, time 3191.00ms, mfu 1.17%\niter 442: loss 8.4814, time 3191.13ms, mfu 1.17%\niter 443: loss 8.2183, time 3190.91ms, mfu 1.17%\niter 444: loss 8.3767, time 3190.90ms, mfu 1.17%\niter 445: loss 8.4074, time 3190.64ms, mfu 1.17%\niter 446: loss 8.3202, time 3190.71ms, mfu 1.17%\niter 447: loss 8.4857, time 3189.82ms, mfu 1.17%\niter 448: loss 8.1828, time 3190.09ms, mfu 1.17%\niter 449: loss 8.3362, time 3190.31ms, mfu 1.17%\niter 450: loss 8.3190, time 3189.86ms, mfu 1.17%\niter 451: loss 8.4106, time 3190.15ms, mfu 1.17%\niter 452: loss 8.3706, time 3189.51ms, mfu 1.17%\niter 453: loss 8.3368, time 3189.90ms, mfu 1.17%\niter 454: loss 8.3631, time 3189.75ms, mfu 1.17%\niter 455: loss 8.5149, time 3190.86ms, mfu 1.17%\niter 456: loss 8.1926, time 3189.88ms, mfu 1.17%\niter 457: loss 8.4161, time 3190.84ms, mfu 1.17%\niter 458: loss 8.3708, time 3190.93ms, mfu 1.17%\niter 459: loss 8.5554, time 3190.40ms, mfu 1.17%\niter 460: loss 8.4313, time 3192.00ms, mfu 1.17%\niter 461: loss 8.2279, time 3191.14ms, mfu 1.17%\niter 462: loss 8.2968, time 3192.08ms, mfu 1.17%\niter 463: loss 8.2366, time 3190.41ms, mfu 1.17%\niter 464: loss 8.2709, time 3190.85ms, mfu 1.17%\niter 465: loss 8.4097, time 3188.08ms, mfu 1.17%\niter 466: loss 8.2386, time 3190.30ms, mfu 1.17%\niter 467: loss 8.4156, time 3191.51ms, mfu 1.17%\niter 468: loss 8.5965, time 3191.28ms, mfu 1.17%\niter 469: loss 8.2561, time 3191.21ms, mfu 1.17%\niter 470: loss 8.3697, time 3190.21ms, mfu 1.17%\niter 471: loss 8.3694, time 3187.83ms, mfu 1.17%\niter 472: loss 8.2466, time 3194.55ms, mfu 1.17%\niter 473: loss 8.2911, time 3188.62ms, mfu 1.17%\niter 474: loss 8.1033, time 3191.24ms, mfu 1.17%\niter 475: loss 8.2924, time 3191.49ms, mfu 1.17%\niter 476: loss 8.1640, time 3191.18ms, mfu 1.17%\niter 477: loss 8.1884, time 3192.43ms, mfu 1.17%\niter 478: loss 8.3050, time 3191.14ms, mfu 1.17%\niter 479: loss 8.1765, time 3191.12ms, mfu 1.17%\niter 480: loss 8.0604, time 3189.76ms, mfu 1.17%\niter 481: loss 8.2004, time 3188.16ms, mfu 1.17%\niter 482: loss 8.4085, time 3189.43ms, mfu 1.17%\niter 483: loss 8.0727, time 3190.23ms, mfu 1.17%\niter 484: loss 8.2996, time 3191.09ms, mfu 1.17%\niter 485: loss 8.1484, time 3190.93ms, mfu 1.17%\niter 486: loss 8.1988, time 3192.64ms, mfu 1.17%\niter 487: loss 8.3641, time 3191.56ms, mfu 1.17%\niter 488: loss 8.2302, time 3191.09ms, mfu 1.17%\niter 489: loss 8.1909, time 3191.23ms, mfu 1.17%\niter 490: loss 8.2238, time 3189.76ms, mfu 1.17%\niter 491: loss 8.0695, time 3191.22ms, mfu 1.17%\niter 492: loss 8.1990, time 3190.04ms, mfu 1.17%\niter 493: loss 8.3359, time 3191.33ms, mfu 1.17%\niter 494: loss 8.1839, time 3190.91ms, mfu 1.17%\niter 495: loss 8.1671, time 3190.03ms, mfu 1.17%\niter 496: loss 8.0714, time 3191.68ms, mfu 1.17%\niter 497: loss 8.0599, time 3190.35ms, mfu 1.17%\niter 498: loss 8.0597, time 3191.70ms, mfu 1.17%\niter 499: loss 8.2275, time 3189.73ms, mfu 1.17%\niter 500: loss 8.2370, time 3190.94ms, mfu 1.17%\niter 501: loss 8.2672, time 3191.48ms, mfu 1.17%\niter 502: loss 8.1482, time 3191.15ms, mfu 1.17%\niter 503: loss 8.2062, time 3189.94ms, mfu 1.17%\niter 504: loss 7.9890, time 3189.81ms, mfu 1.17%\niter 505: loss 8.0036, time 3189.56ms, mfu 1.17%\niter 506: loss 8.1308, time 3190.82ms, mfu 1.17%\niter 507: loss 7.9446, time 3189.31ms, mfu 1.17%\niter 508: loss 7.9631, time 3189.81ms, mfu 1.17%\niter 509: loss 7.9964, time 3191.58ms, mfu 1.17%\niter 510: loss 7.9205, time 3191.10ms, mfu 1.17%\niter 511: loss 8.2449, time 3189.63ms, mfu 1.17%\niter 512: loss 7.9750, time 3191.35ms, mfu 1.17%\niter 513: loss 7.8912, time 3191.34ms, mfu 1.17%\niter 514: loss 7.8206, time 3190.14ms, mfu 1.17%\niter 515: loss 8.2001, time 3191.77ms, mfu 1.17%\niter 516: loss 8.1315, time 3189.68ms, mfu 1.17%\niter 517: loss 8.0355, time 3191.59ms, mfu 1.17%\niter 518: loss 8.0451, time 3190.31ms, mfu 1.17%\niter 519: loss 7.9948, time 3191.16ms, mfu 1.17%\niter 520: loss 8.2229, time 3191.16ms, mfu 1.17%\niter 521: loss 8.0405, time 3191.69ms, mfu 1.17%\niter 522: loss 7.9906, time 3190.97ms, mfu 1.17%\niter 523: loss 7.9346, time 3191.02ms, mfu 1.17%\niter 524: loss 8.0180, time 3191.10ms, mfu 1.17%\niter 525: loss 7.8692, time 3186.82ms, mfu 1.17%\niter 526: loss 7.9118, time 3190.36ms, mfu 1.17%\niter 527: loss 7.8415, time 3191.19ms, mfu 1.17%\niter 528: loss 8.1781, time 3188.27ms, mfu 1.17%\niter 529: loss 7.7720, time 3191.35ms, mfu 1.17%\niter 530: loss 7.9850, time 3191.78ms, mfu 1.17%\niter 531: loss 8.2583, time 3191.35ms, mfu 1.17%\niter 532: loss 7.7769, time 3189.98ms, mfu 1.17%\niter 533: loss 8.0127, time 3191.49ms, mfu 1.17%\niter 534: loss 8.3376, time 3190.11ms, mfu 1.17%\niter 535: loss 7.9006, time 3189.93ms, mfu 1.17%\niter 536: loss 7.7097, time 3189.67ms, mfu 1.17%\niter 537: loss 8.1154, time 3191.58ms, mfu 1.17%\niter 538: loss 7.8176, time 3188.34ms, mfu 1.17%\niter 539: loss 7.8709, time 3188.15ms, mfu 1.17%\niter 540: loss 8.0773, time 3191.51ms, mfu 1.17%\niter 541: loss 7.8010, time 3188.91ms, mfu 1.17%\niter 542: loss 7.8103, time 3190.52ms, mfu 1.17%\niter 543: loss 7.8217, time 3190.71ms, mfu 1.17%\niter 544: loss 7.6912, time 3190.01ms, mfu 1.17%\niter 545: loss 7.8611, time 3191.56ms, mfu 1.17%\niter 546: loss 7.9597, time 3191.44ms, mfu 1.17%\niter 547: loss 7.7245, time 3191.07ms, mfu 1.17%\niter 548: loss 7.8793, time 3191.35ms, mfu 1.17%\niter 549: loss 7.8661, time 3190.03ms, mfu 1.17%\niter 550: loss 7.6245, time 3191.41ms, mfu 1.17%\niter 551: loss 8.0106, time 3193.87ms, mfu 1.17%\niter 552: loss 7.6738, time 3187.55ms, mfu 1.17%\niter 553: loss 7.7769, time 3189.85ms, mfu 1.17%\niter 554: loss 7.6955, time 3191.57ms, mfu 1.17%\niter 555: loss 7.7967, time 3191.75ms, mfu 1.17%\niter 556: loss 7.7443, time 3189.30ms, mfu 1.17%\niter 557: loss 7.6300, time 3190.79ms, mfu 1.17%\niter 558: loss 7.6225, time 3190.06ms, mfu 1.17%\niter 559: loss 7.6572, time 3191.66ms, mfu 1.17%\niter 560: loss 7.5043, time 3189.74ms, mfu 1.17%\niter 561: loss 7.6874, time 3190.95ms, mfu 1.17%\niter 562: loss 7.9244, time 3187.01ms, mfu 1.17%\niter 563: loss 7.6136, time 3191.61ms, mfu 1.17%\niter 564: loss 7.8040, time 3191.94ms, mfu 1.17%\niter 565: loss 7.8955, time 3190.88ms, mfu 1.17%\niter 566: loss 7.8454, time 3189.84ms, mfu 1.17%\niter 567: loss 7.6848, time 3191.32ms, mfu 1.17%\niter 568: loss 7.6923, time 3190.36ms, mfu 1.17%\niter 569: loss 7.9127, time 3192.06ms, mfu 1.17%\niter 570: loss 7.9597, time 3192.14ms, mfu 1.17%\niter 571: loss 7.8583, time 3186.29ms, mfu 1.17%\niter 572: loss 7.9005, time 3192.16ms, mfu 1.17%\niter 573: loss 8.0163, time 3191.89ms, mfu 1.17%\niter 574: loss 7.7556, time 3190.15ms, mfu 1.17%\niter 575: loss 7.6367, time 3192.42ms, mfu 1.17%\niter 576: loss 7.6872, time 3191.88ms, mfu 1.17%\niter 577: loss 7.9185, time 3191.42ms, mfu 1.17%\niter 578: loss 7.7215, time 3190.16ms, mfu 1.17%\niter 579: loss 7.8423, time 3190.48ms, mfu 1.17%\niter 580: loss 7.5826, time 3191.66ms, mfu 1.17%\niter 581: loss 7.6100, time 3190.44ms, mfu 1.17%\niter 582: loss 7.9938, time 3192.04ms, mfu 1.17%\niter 583: loss 7.5695, time 3191.71ms, mfu 1.17%\niter 584: loss 7.6375, time 3190.02ms, mfu 1.17%\niter 585: loss 7.5324, time 3191.91ms, mfu 1.17%\niter 586: loss 7.7907, time 3191.79ms, mfu 1.17%\niter 587: loss 7.5527, time 3190.40ms, mfu 1.17%\niter 588: loss 7.4971, time 3190.66ms, mfu 1.17%\niter 589: loss 7.8348, time 3192.10ms, mfu 1.17%\niter 590: loss 7.6293, time 3190.58ms, mfu 1.17%\niter 591: loss 7.8049, time 3191.09ms, mfu 1.17%\niter 592: loss 7.6119, time 3191.88ms, mfu 1.17%\niter 593: loss 7.7987, time 3189.87ms, mfu 1.17%\niter 594: loss 7.6471, time 3190.72ms, mfu 1.17%\niter 595: loss 7.6705, time 3187.66ms, mfu 1.17%\niter 596: loss 7.7156, time 3188.95ms, mfu 1.17%\niter 597: loss 7.5187, time 3191.85ms, mfu 1.17%\niter 598: loss 7.5634, time 3190.10ms, mfu 1.17%\niter 599: loss 7.4506, time 3191.85ms, mfu 1.17%\nstep 600: train loss 7.6035, val loss 7.5392\nsaving checkpoint to /kaggle/working/\niter 600: loss 7.7341, time 15505.46ms, mfu 1.07%\niter 601: loss 7.8341, time 3191.11ms, mfu 1.08%\niter 602: loss 7.7294, time 3192.19ms, mfu 1.09%\niter 603: loss 7.5621, time 3190.60ms, mfu 1.10%\niter 604: loss 7.5735, time 3189.14ms, mfu 1.11%\niter 605: loss 7.6190, time 3189.38ms, mfu 1.11%\niter 606: loss 7.4358, time 3191.62ms, mfu 1.12%\niter 607: loss 7.4402, time 3190.22ms, mfu 1.12%\niter 608: loss 7.5757, time 3189.07ms, mfu 1.13%\niter 609: loss 7.6148, time 3190.27ms, mfu 1.13%\niter 610: loss 7.8253, time 3193.01ms, mfu 1.13%\niter 611: loss 7.5024, time 3191.47ms, mfu 1.14%\niter 612: loss 7.5993, time 3190.42ms, mfu 1.14%\niter 613: loss 7.6340, time 3190.46ms, mfu 1.14%\niter 614: loss 7.7899, time 3192.59ms, mfu 1.15%\niter 615: loss 7.6340, time 3191.83ms, mfu 1.15%\niter 616: loss 7.4182, time 3192.75ms, mfu 1.15%\niter 617: loss 7.6357, time 3189.37ms, mfu 1.15%\niter 618: loss 7.5252, time 3189.37ms, mfu 1.15%\niter 619: loss 7.6497, time 3191.74ms, mfu 1.15%\niter 620: loss 7.5635, time 3188.66ms, mfu 1.16%\niter 621: loss 7.5626, time 3190.28ms, mfu 1.16%\niter 622: loss 7.4813, time 3190.27ms, mfu 1.16%\niter 623: loss 7.4483, time 3190.88ms, mfu 1.16%\niter 624: loss 7.5979, time 3192.39ms, mfu 1.16%\niter 625: loss 7.4640, time 3192.43ms, mfu 1.16%\niter 626: loss 7.6153, time 3192.20ms, mfu 1.16%\niter 627: loss 7.1022, time 3189.59ms, mfu 1.16%\niter 628: loss 7.3606, time 3188.57ms, mfu 1.16%\niter 629: loss 7.6049, time 3191.97ms, mfu 1.16%\niter 630: loss 7.3659, time 3190.50ms, mfu 1.16%\niter 631: loss 7.4254, time 3191.96ms, mfu 1.16%\niter 632: loss 7.7035, time 3191.09ms, mfu 1.16%\niter 633: loss 7.3444, time 3190.69ms, mfu 1.16%\niter 634: loss 7.4447, time 3191.66ms, mfu 1.16%\niter 635: loss 7.2641, time 3190.88ms, mfu 1.16%\niter 636: loss 7.4043, time 3193.87ms, mfu 1.16%\niter 637: loss 7.4673, time 3188.08ms, mfu 1.16%\niter 638: loss 7.4339, time 3190.32ms, mfu 1.16%\niter 639: loss 7.3110, time 3190.94ms, mfu 1.17%\niter 640: loss 7.6139, time 3190.44ms, mfu 1.17%\niter 641: loss 7.2358, time 3192.51ms, mfu 1.17%\niter 642: loss 7.2847, time 3190.91ms, mfu 1.17%\niter 643: loss 7.2139, time 3193.09ms, mfu 1.17%\niter 644: loss 7.4498, time 3188.79ms, mfu 1.17%\niter 645: loss 7.4439, time 3191.90ms, mfu 1.17%\niter 646: loss 7.3005, time 3192.59ms, mfu 1.17%\niter 647: loss 7.1552, time 3188.88ms, mfu 1.17%\niter 648: loss 7.3504, time 3190.83ms, mfu 1.17%\niter 649: loss 7.2860, time 3191.00ms, mfu 1.17%\niter 650: loss 7.3836, time 3191.04ms, mfu 1.17%\niter 651: loss 7.5172, time 3192.22ms, mfu 1.17%\niter 652: loss 7.3653, time 3191.94ms, mfu 1.17%\niter 653: loss 7.2194, time 3191.34ms, mfu 1.17%\niter 654: loss 7.3435, time 3192.27ms, mfu 1.17%\niter 655: loss 7.4643, time 3192.17ms, mfu 1.17%\niter 656: loss 7.3915, time 3190.76ms, mfu 1.17%\niter 657: loss 7.5707, time 3193.09ms, mfu 1.17%\niter 658: loss 7.2582, time 3190.48ms, mfu 1.17%\niter 659: loss 7.3547, time 3190.60ms, mfu 1.17%\niter 660: loss 7.3509, time 3188.67ms, mfu 1.17%\niter 661: loss 7.0693, time 3191.00ms, mfu 1.17%\niter 662: loss 7.2097, time 3191.67ms, mfu 1.17%\niter 663: loss 7.2342, time 3191.39ms, mfu 1.17%\niter 664: loss 7.5487, time 3191.83ms, mfu 1.17%\niter 665: loss 7.2820, time 3191.52ms, mfu 1.17%\niter 666: loss 7.2557, time 3192.10ms, mfu 1.17%\niter 667: loss 7.2506, time 3192.49ms, mfu 1.17%\niter 668: loss 7.4008, time 3192.82ms, mfu 1.17%\niter 669: loss 7.1527, time 3192.27ms, mfu 1.17%\niter 670: loss 7.2231, time 3191.48ms, mfu 1.17%\niter 671: loss 7.2683, time 3189.08ms, mfu 1.17%\niter 672: loss 7.7907, time 3191.98ms, mfu 1.17%\niter 673: loss 7.4557, time 3188.91ms, mfu 1.17%\niter 674: loss 7.1152, time 3190.94ms, mfu 1.17%\niter 675: loss 7.0557, time 3190.97ms, mfu 1.17%\niter 676: loss 7.1721, time 3192.46ms, mfu 1.17%\niter 677: loss 6.9620, time 3190.37ms, mfu 1.17%\niter 678: loss 7.3515, time 3191.12ms, mfu 1.17%\niter 679: loss 7.1559, time 3190.75ms, mfu 1.17%\niter 680: loss 7.4026, time 3192.07ms, mfu 1.17%\niter 681: loss 7.2354, time 3190.18ms, mfu 1.17%\niter 682: loss 7.1935, time 3191.96ms, mfu 1.17%\niter 683: loss 7.0816, time 3192.47ms, mfu 1.17%\niter 684: loss 7.1302, time 3192.80ms, mfu 1.17%\niter 685: loss 7.1387, time 3192.24ms, mfu 1.17%\niter 686: loss 7.3055, time 3192.25ms, mfu 1.17%\niter 687: loss 7.2197, time 3192.24ms, mfu 1.17%\niter 688: loss 7.4268, time 3190.87ms, mfu 1.17%\niter 689: loss 7.1947, time 3191.10ms, mfu 1.17%\niter 690: loss 7.2868, time 3190.80ms, mfu 1.17%\niter 691: loss 7.2484, time 3190.42ms, mfu 1.17%\niter 692: loss 7.0303, time 3192.80ms, mfu 1.17%\niter 693: loss 7.4908, time 3190.80ms, mfu 1.17%\niter 694: loss 7.0914, time 3192.45ms, mfu 1.17%\niter 695: loss 7.3839, time 3191.60ms, mfu 1.17%\niter 696: loss 6.9130, time 3192.26ms, mfu 1.17%\niter 697: loss 7.1201, time 3191.68ms, mfu 1.17%\niter 698: loss 7.3376, time 3190.69ms, mfu 1.17%\niter 699: loss 7.1630, time 3192.37ms, mfu 1.17%\niter 700: loss 7.6157, time 3192.32ms, mfu 1.17%\niter 701: loss 7.3446, time 3192.43ms, mfu 1.17%\niter 702: loss 7.2668, time 3191.74ms, mfu 1.17%\niter 703: loss 7.1385, time 3192.25ms, mfu 1.17%\niter 704: loss 7.1847, time 3192.31ms, mfu 1.17%\niter 705: loss 7.1427, time 3193.34ms, mfu 1.17%\niter 706: loss 7.0288, time 3190.18ms, mfu 1.17%\niter 707: loss 6.9415, time 3190.56ms, mfu 1.17%\niter 708: loss 7.2186, time 3191.42ms, mfu 1.17%\niter 709: loss 7.1596, time 3192.75ms, mfu 1.17%\niter 710: loss 6.9513, time 3190.80ms, mfu 1.17%\niter 711: loss 7.5803, time 3190.18ms, mfu 1.17%\niter 712: loss 7.1596, time 3192.28ms, mfu 1.17%\niter 713: loss 7.0505, time 3192.23ms, mfu 1.17%\niter 714: loss 6.9928, time 3192.15ms, mfu 1.17%\niter 715: loss 6.8593, time 3191.44ms, mfu 1.17%\niter 716: loss 6.9516, time 3192.70ms, mfu 1.17%\niter 717: loss 7.0863, time 3192.14ms, mfu 1.17%\niter 718: loss 7.4742, time 3192.18ms, mfu 1.17%\niter 719: loss 7.4266, time 3189.46ms, mfu 1.17%\niter 720: loss 7.2434, time 3191.77ms, mfu 1.17%\niter 721: loss 7.0447, time 3191.68ms, mfu 1.17%\niter 722: loss 7.1373, time 3191.99ms, mfu 1.17%\niter 723: loss 7.0994, time 3190.24ms, mfu 1.17%\niter 724: loss 7.3864, time 3190.78ms, mfu 1.17%\niter 725: loss 7.1539, time 3192.24ms, mfu 1.17%\niter 726: loss 7.0741, time 3192.26ms, mfu 1.17%\niter 727: loss 7.0698, time 3190.75ms, mfu 1.17%\niter 728: loss 7.0850, time 3188.92ms, mfu 1.17%\niter 729: loss 7.0377, time 3191.16ms, mfu 1.17%\niter 730: loss 7.0406, time 3192.22ms, mfu 1.17%\niter 731: loss 7.0023, time 3192.00ms, mfu 1.17%\niter 732: loss 7.0944, time 3192.00ms, mfu 1.17%\niter 733: loss 6.9019, time 3191.46ms, mfu 1.17%\niter 734: loss 7.0080, time 3190.76ms, mfu 1.17%\niter 735: loss 7.0433, time 3190.46ms, mfu 1.17%\niter 736: loss 7.1442, time 3192.64ms, mfu 1.17%\niter 737: loss 6.8071, time 3192.47ms, mfu 1.17%\niter 738: loss 7.0472, time 3190.14ms, mfu 1.17%\niter 739: loss 7.1399, time 3189.05ms, mfu 1.17%\niter 740: loss 7.1531, time 3191.13ms, mfu 1.17%\niter 741: loss 7.3310, time 3190.32ms, mfu 1.17%\niter 742: loss 7.2476, time 3192.40ms, mfu 1.17%\niter 743: loss 7.1205, time 3192.67ms, mfu 1.17%\niter 744: loss 7.0140, time 3190.54ms, mfu 1.17%\niter 745: loss 7.1185, time 3192.99ms, mfu 1.17%\niter 746: loss 6.9644, time 3192.50ms, mfu 1.17%\niter 747: loss 7.0527, time 3192.48ms, mfu 1.17%\niter 748: loss 7.4041, time 3189.29ms, mfu 1.17%\niter 749: loss 6.9998, time 3189.44ms, mfu 1.17%\niter 750: loss 6.9930, time 3190.61ms, mfu 1.17%\niter 751: loss 7.1502, time 3192.39ms, mfu 1.17%\niter 752: loss 6.8596, time 3190.61ms, mfu 1.17%\niter 753: loss 7.1768, time 3191.22ms, mfu 1.17%\niter 754: loss 7.0828, time 3186.82ms, mfu 1.17%\niter 755: loss 6.9421, time 3190.91ms, mfu 1.17%\niter 756: loss 7.1883, time 3191.22ms, mfu 1.17%\niter 757: loss 7.1195, time 3192.43ms, mfu 1.17%\niter 758: loss 7.0346, time 3192.33ms, mfu 1.17%\niter 759: loss 7.2468, time 3190.64ms, mfu 1.17%\niter 760: loss 7.0957, time 3192.61ms, mfu 1.17%\niter 761: loss 7.0726, time 3191.80ms, mfu 1.17%\niter 762: loss 7.1054, time 3193.09ms, mfu 1.17%\niter 763: loss 7.2888, time 3192.47ms, mfu 1.17%\niter 764: loss 6.9592, time 3196.28ms, mfu 1.17%\niter 765: loss 7.3377, time 3189.18ms, mfu 1.17%\niter 766: loss 7.0877, time 3193.48ms, mfu 1.17%\niter 767: loss 7.1374, time 3192.64ms, mfu 1.17%\niter 768: loss 7.1046, time 3192.58ms, mfu 1.17%\niter 769: loss 7.1337, time 3192.14ms, mfu 1.17%\niter 770: loss 7.0250, time 3191.95ms, mfu 1.17%\niter 771: loss 7.2018, time 3191.86ms, mfu 1.17%\niter 772: loss 6.7038, time 3191.87ms, mfu 1.17%\niter 773: loss 7.1283, time 3192.31ms, mfu 1.17%\niter 774: loss 7.0576, time 3195.28ms, mfu 1.17%\niter 775: loss 7.0899, time 3189.78ms, mfu 1.17%\niter 776: loss 7.0428, time 3192.08ms, mfu 1.17%\niter 777: loss 6.9695, time 3189.15ms, mfu 1.17%\niter 778: loss 7.0102, time 3189.00ms, mfu 1.17%\niter 779: loss 7.0806, time 3191.79ms, mfu 1.17%\niter 780: loss 7.3382, time 3191.24ms, mfu 1.17%\niter 781: loss 7.1190, time 3191.99ms, mfu 1.17%\niter 782: loss 7.1100, time 3191.80ms, mfu 1.17%\niter 783: loss 6.8205, time 3190.88ms, mfu 1.17%\niter 784: loss 6.9790, time 3192.28ms, mfu 1.17%\niter 785: loss 7.1939, time 3192.05ms, mfu 1.17%\niter 786: loss 6.8145, time 3192.05ms, mfu 1.17%\niter 787: loss 6.9562, time 3192.63ms, mfu 1.17%\niter 788: loss 6.9436, time 3191.70ms, mfu 1.17%\niter 789: loss 6.8410, time 3190.37ms, mfu 1.17%\niter 790: loss 6.9554, time 3192.42ms, mfu 1.17%\niter 791: loss 7.2310, time 3191.06ms, mfu 1.17%\niter 792: loss 7.3874, time 3190.93ms, mfu 1.17%\niter 793: loss 7.1000, time 3191.01ms, mfu 1.17%\niter 794: loss 6.8771, time 3192.54ms, mfu 1.17%\niter 795: loss 6.9272, time 3191.99ms, mfu 1.17%\niter 796: loss 6.8179, time 3191.84ms, mfu 1.17%\niter 797: loss 6.7805, time 3190.49ms, mfu 1.17%\niter 798: loss 6.7154, time 3191.67ms, mfu 1.17%\niter 799: loss 6.9312, time 3189.20ms, mfu 1.17%\nstep 800: train loss 6.9815, val loss 6.9677\nsaving checkpoint to /kaggle/working/\niter 800: loss 6.9644, time 15409.82ms, mfu 1.07%\niter 801: loss 6.9600, time 3191.01ms, mfu 1.08%\niter 802: loss 7.0904, time 3192.39ms, mfu 1.09%\niter 803: loss 7.1765, time 3192.02ms, mfu 1.10%\niter 804: loss 6.9266, time 3191.41ms, mfu 1.11%\niter 805: loss 6.8096, time 3192.16ms, mfu 1.11%\niter 806: loss 6.8839, time 3191.93ms, mfu 1.12%\niter 807: loss 6.7596, time 3191.31ms, mfu 1.12%\niter 808: loss 6.8157, time 3190.75ms, mfu 1.13%\niter 809: loss 6.8551, time 3191.47ms, mfu 1.13%\niter 810: loss 7.0159, time 3192.31ms, mfu 1.13%\niter 811: loss 6.9553, time 3190.63ms, mfu 1.14%\niter 812: loss 6.9462, time 3192.41ms, mfu 1.14%\niter 813: loss 7.0775, time 3192.38ms, mfu 1.14%\niter 814: loss 7.1920, time 3191.79ms, mfu 1.15%\niter 815: loss 7.0015, time 3192.41ms, mfu 1.15%\niter 816: loss 6.9522, time 3192.14ms, mfu 1.15%\niter 817: loss 6.8729, time 3190.54ms, mfu 1.15%\niter 818: loss 7.0054, time 3192.45ms, mfu 1.15%\niter 819: loss 6.9136, time 3191.18ms, mfu 1.15%\niter 820: loss 6.8998, time 3191.87ms, mfu 1.16%\niter 821: loss 6.8554, time 3190.25ms, mfu 1.16%\niter 822: loss 7.0597, time 3192.13ms, mfu 1.16%\niter 823: loss 6.7116, time 3192.49ms, mfu 1.16%\niter 824: loss 6.8223, time 3192.17ms, mfu 1.16%\niter 825: loss 7.0135, time 3190.33ms, mfu 1.16%\niter 826: loss 6.8225, time 3191.93ms, mfu 1.16%\niter 827: loss 6.6081, time 3192.64ms, mfu 1.16%\niter 828: loss 6.9768, time 3189.73ms, mfu 1.16%\niter 829: loss 6.7819, time 3193.97ms, mfu 1.16%\niter 830: loss 7.0669, time 3190.01ms, mfu 1.16%\niter 831: loss 6.9514, time 3190.12ms, mfu 1.16%\niter 832: loss 7.0161, time 3190.56ms, mfu 1.16%\niter 833: loss 6.8185, time 3191.73ms, mfu 1.16%\niter 834: loss 6.9490, time 3190.32ms, mfu 1.16%\niter 835: loss 6.8296, time 3190.94ms, mfu 1.16%\niter 836: loss 6.7203, time 3190.21ms, mfu 1.16%\niter 837: loss 7.0915, time 3191.14ms, mfu 1.16%\niter 838: loss 6.8892, time 3192.56ms, mfu 1.16%\niter 839: loss 7.0606, time 3194.19ms, mfu 1.16%\niter 840: loss 7.1104, time 3188.86ms, mfu 1.17%\niter 841: loss 6.9259, time 3191.20ms, mfu 1.17%\niter 842: loss 6.6068, time 3190.26ms, mfu 1.17%\niter 843: loss 6.7480, time 3187.69ms, mfu 1.17%\niter 844: loss 6.8258, time 3192.47ms, mfu 1.17%\niter 845: loss 6.9859, time 3187.79ms, mfu 1.17%\niter 846: loss 7.0377, time 3192.49ms, mfu 1.17%\niter 847: loss 7.0887, time 3189.95ms, mfu 1.17%\niter 848: loss 6.9487, time 3191.03ms, mfu 1.17%\niter 849: loss 7.0669, time 3191.82ms, mfu 1.17%\niter 850: loss 7.2030, time 3190.47ms, mfu 1.17%\niter 851: loss 7.4482, time 3188.74ms, mfu 1.17%\niter 852: loss 6.9639, time 3190.30ms, mfu 1.17%\niter 853: loss 6.6833, time 3191.72ms, mfu 1.17%\niter 854: loss 6.8964, time 3189.75ms, mfu 1.17%\niter 855: loss 6.7218, time 3192.34ms, mfu 1.17%\niter 856: loss 6.7440, time 3192.39ms, mfu 1.17%\niter 857: loss 7.1999, time 3192.25ms, mfu 1.17%\niter 858: loss 7.1495, time 3191.95ms, mfu 1.17%\niter 859: loss 6.8165, time 3190.81ms, mfu 1.17%\niter 860: loss 6.8723, time 3191.76ms, mfu 1.17%\niter 861: loss 7.0593, time 3190.36ms, mfu 1.17%\niter 862: loss 7.3415, time 3192.55ms, mfu 1.17%\niter 863: loss 6.7813, time 3191.90ms, mfu 1.17%\niter 864: loss 6.7194, time 3191.87ms, mfu 1.17%\niter 865: loss 7.2170, time 3190.95ms, mfu 1.17%\niter 866: loss 6.9257, time 3190.99ms, mfu 1.17%\niter 867: loss 6.7128, time 3191.97ms, mfu 1.17%\niter 868: loss 7.1399, time 3192.03ms, mfu 1.17%\niter 869: loss 7.0387, time 3190.39ms, mfu 1.17%\niter 870: loss 6.7481, time 3192.27ms, mfu 1.17%\niter 871: loss 6.8104, time 3192.30ms, mfu 1.17%\niter 872: loss 7.0487, time 3192.06ms, mfu 1.17%\niter 873: loss 6.8118, time 3190.73ms, mfu 1.17%\niter 874: loss 6.8651, time 3188.84ms, mfu 1.17%\niter 875: loss 6.8482, time 3192.10ms, mfu 1.17%\niter 876: loss 6.7309, time 3190.36ms, mfu 1.17%\niter 877: loss 6.7856, time 3192.46ms, mfu 1.17%\niter 878: loss 6.8258, time 3191.33ms, mfu 1.17%\niter 879: loss 6.5897, time 3193.32ms, mfu 1.17%\niter 880: loss 6.9146, time 3191.86ms, mfu 1.17%\niter 881: loss 6.6672, time 3190.79ms, mfu 1.17%\niter 882: loss 6.9078, time 3192.24ms, mfu 1.17%\niter 883: loss 7.1748, time 3191.26ms, mfu 1.17%\niter 884: loss 7.1550, time 3190.50ms, mfu 1.17%\niter 885: loss 6.9346, time 3190.66ms, mfu 1.17%\niter 886: loss 7.3568, time 3185.97ms, mfu 1.17%\niter 887: loss 6.7283, time 3192.34ms, mfu 1.17%\niter 888: loss 6.9501, time 3190.02ms, mfu 1.17%\niter 889: loss 6.9598, time 3189.84ms, mfu 1.17%\niter 890: loss 6.5830, time 3192.38ms, mfu 1.17%\niter 891: loss 6.7776, time 3192.35ms, mfu 1.17%\niter 892: loss 6.5577, time 3190.98ms, mfu 1.17%\niter 893: loss 6.7974, time 3191.49ms, mfu 1.17%\niter 894: loss 6.8987, time 3191.96ms, mfu 1.17%\niter 895: loss 6.7073, time 3190.79ms, mfu 1.17%\niter 896: loss 6.5833, time 3192.22ms, mfu 1.17%\niter 897: loss 6.5650, time 3192.35ms, mfu 1.17%\niter 898: loss 6.5674, time 3195.08ms, mfu 1.17%\niter 899: loss 6.6584, time 3190.00ms, mfu 1.17%\niter 900: loss 6.4630, time 3192.99ms, mfu 1.17%\niter 901: loss 6.8962, time 3192.13ms, mfu 1.17%\niter 902: loss 6.8777, time 3192.04ms, mfu 1.17%\niter 903: loss 7.0906, time 3192.78ms, mfu 1.17%\niter 904: loss 7.0374, time 3191.97ms, mfu 1.17%\niter 905: loss 6.7494, time 3192.08ms, mfu 1.17%\niter 906: loss 6.9483, time 3190.30ms, mfu 1.17%\niter 907: loss 6.5685, time 3190.55ms, mfu 1.17%\niter 908: loss 6.7031, time 3194.84ms, mfu 1.17%\niter 909: loss 6.7626, time 3188.77ms, mfu 1.17%\niter 910: loss 6.8257, time 3190.50ms, mfu 1.17%\niter 911: loss 7.1331, time 3193.17ms, mfu 1.17%\niter 912: loss 6.6681, time 3190.12ms, mfu 1.17%\niter 913: loss 6.8193, time 3192.16ms, mfu 1.17%\niter 914: loss 6.9997, time 3191.64ms, mfu 1.17%\niter 915: loss 6.5186, time 3191.92ms, mfu 1.17%\niter 916: loss 6.6648, time 3188.59ms, mfu 1.17%\niter 917: loss 6.7308, time 3188.93ms, mfu 1.17%\niter 918: loss 6.9100, time 3194.55ms, mfu 1.17%\niter 919: loss 6.8915, time 3190.53ms, mfu 1.17%\niter 920: loss 6.7652, time 3191.69ms, mfu 1.17%\niter 921: loss 6.9943, time 3192.22ms, mfu 1.17%\niter 922: loss 6.8628, time 3192.46ms, mfu 1.17%\niter 923: loss 6.7850, time 3192.22ms, mfu 1.17%\niter 924: loss 6.6770, time 3192.71ms, mfu 1.17%\niter 925: loss 6.6778, time 3192.41ms, mfu 1.17%\niter 926: loss 6.7292, time 3192.00ms, mfu 1.17%\niter 927: loss 6.3641, time 3190.85ms, mfu 1.17%\niter 928: loss 6.8957, time 3191.90ms, mfu 1.17%\niter 929: loss 6.7382, time 3192.65ms, mfu 1.17%\niter 930: loss 6.5821, time 3191.98ms, mfu 1.17%\niter 931: loss 6.9573, time 3192.33ms, mfu 1.17%\niter 932: loss 6.9340, time 3191.76ms, mfu 1.17%\niter 933: loss 6.9219, time 3191.83ms, mfu 1.17%\niter 934: loss 6.6403, time 3192.34ms, mfu 1.17%\niter 935: loss 6.7761, time 3189.98ms, mfu 1.17%\niter 936: loss 6.4932, time 3190.43ms, mfu 1.17%\niter 937: loss 7.3068, time 3191.80ms, mfu 1.17%\niter 938: loss 6.4558, time 3190.21ms, mfu 1.17%\niter 939: loss 6.4769, time 3191.43ms, mfu 1.17%\niter 940: loss 6.7207, time 3191.49ms, mfu 1.17%\niter 941: loss 6.6748, time 3192.02ms, mfu 1.17%\niter 942: loss 6.7490, time 3190.77ms, mfu 1.17%\niter 943: loss 6.8279, time 3192.05ms, mfu 1.17%\niter 944: loss 6.9976, time 3188.94ms, mfu 1.17%\niter 945: loss 6.5528, time 3190.80ms, mfu 1.17%\niter 946: loss 6.9989, time 3188.82ms, mfu 1.17%\niter 947: loss 6.7256, time 3190.61ms, mfu 1.17%\niter 948: loss 6.7559, time 3190.50ms, mfu 1.17%\niter 949: loss 6.5792, time 3191.98ms, mfu 1.17%\niter 950: loss 6.7182, time 3189.30ms, mfu 1.17%\niter 951: loss 6.7538, time 3189.63ms, mfu 1.17%\niter 952: loss 6.8369, time 3188.79ms, mfu 1.17%\niter 953: loss 6.7946, time 3192.79ms, mfu 1.17%\niter 954: loss 6.7218, time 3191.52ms, mfu 1.17%\niter 955: loss 6.7192, time 3192.29ms, mfu 1.17%\niter 956: loss 6.7446, time 3192.04ms, mfu 1.17%\niter 957: loss 6.7845, time 3191.76ms, mfu 1.17%\niter 958: loss 6.5930, time 3190.93ms, mfu 1.17%\niter 959: loss 6.6692, time 3190.17ms, mfu 1.17%\niter 960: loss 6.5630, time 3190.37ms, mfu 1.17%\niter 961: loss 6.5867, time 3193.05ms, mfu 1.17%\niter 962: loss 6.7114, time 3192.29ms, mfu 1.17%\niter 963: loss 6.4606, time 3190.88ms, mfu 1.17%\niter 964: loss 6.4816, time 3192.41ms, mfu 1.17%\niter 965: loss 6.8699, time 3188.71ms, mfu 1.17%\niter 966: loss 6.7408, time 3190.52ms, mfu 1.17%\niter 967: loss 6.7541, time 3193.43ms, mfu 1.17%\niter 968: loss 6.7499, time 3189.98ms, mfu 1.17%\niter 969: loss 6.5201, time 3192.01ms, mfu 1.17%\niter 970: loss 6.3976, time 3190.64ms, mfu 1.17%\niter 971: loss 6.8611, time 3190.62ms, mfu 1.17%\niter 972: loss 6.7606, time 3190.08ms, mfu 1.17%\niter 973: loss 7.4221, time 3192.20ms, mfu 1.17%\niter 974: loss 7.0631, time 3192.84ms, mfu 1.17%\niter 975: loss 6.5914, time 3190.71ms, mfu 1.17%\niter 976: loss 6.5939, time 3192.44ms, mfu 1.17%\niter 977: loss 6.5141, time 3193.87ms, mfu 1.17%\niter 978: loss 6.6987, time 3187.66ms, mfu 1.17%\niter 979: loss 6.5351, time 3190.64ms, mfu 1.17%\niter 980: loss 6.5337, time 3190.81ms, mfu 1.17%\niter 981: loss 6.7998, time 3192.02ms, mfu 1.17%\niter 982: loss 6.6163, time 3190.56ms, mfu 1.17%\niter 983: loss 6.4717, time 3191.71ms, mfu 1.17%\niter 984: loss 6.9124, time 3192.50ms, mfu 1.17%\niter 985: loss 6.5780, time 3190.59ms, mfu 1.17%\niter 986: loss 6.7460, time 3193.65ms, mfu 1.17%\niter 987: loss 6.3082, time 3194.39ms, mfu 1.17%\niter 988: loss 6.4040, time 3189.79ms, mfu 1.17%\niter 989: loss 6.4463, time 3190.54ms, mfu 1.17%\niter 990: loss 6.6542, time 3190.20ms, mfu 1.17%\niter 991: loss 6.5151, time 3192.07ms, mfu 1.17%\niter 992: loss 6.6085, time 3191.64ms, mfu 1.17%\niter 993: loss 6.5488, time 3192.14ms, mfu 1.17%\niter 994: loss 6.7670, time 3191.85ms, mfu 1.17%\niter 995: loss 6.5471, time 3191.96ms, mfu 1.17%\niter 996: loss 6.6738, time 3192.31ms, mfu 1.17%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"- обучение и сравнение качества обычного небольшого трансформера (например по 6 голов и 6 слоев) и reflex attention (в разных сетапах)\n- любые изменения/дополнения/улучшения, которые по-вашему могут работать\n- отчет об экспериментах, что получилось и что нет","metadata":{}}]}