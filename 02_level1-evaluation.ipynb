{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7105268,"sourceType":"datasetVersion","datasetId":4096203},{"sourceId":9984524,"sourceType":"datasetVersion","datasetId":6144291},{"sourceId":10026828,"sourceType":"datasetVersion","datasetId":6170020},{"sourceId":209694808,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/mingpt/')\nsys.path.append('/kaggle/input/reflextransformer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:40.659557Z","iopub.execute_input":"2024-11-27T18:53:40.660479Z","iopub.status.idle":"2024-11-27T18:53:40.672015Z","shell.execute_reply.started":"2024-11-27T18:53:40.660422Z","shell.execute_reply":"2024-11-27T18:53:40.670647Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nimport math\n\nfrom mingpt.utils import set_seed\nfrom mingpt.bpe import BPETokenizer\nset_seed(3407)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:40.673799Z","iopub.execute_input":"2024-11-27T18:53:40.674202Z","iopub.status.idle":"2024-11-27T18:53:42.286996Z","shell.execute_reply.started":"2024-11-27T18:53:40.674159Z","shell.execute_reply":"2024-11-27T18:53:42.286157Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import reflexTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:42.288900Z","iopub.execute_input":"2024-11-27T18:53:42.289683Z","iopub.status.idle":"2024-11-27T18:53:42.299049Z","shell.execute_reply.started":"2024-11-27T18:53:42.289639Z","shell.execute_reply":"2024-11-27T18:53:42.298322Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:42.300139Z","iopub.execute_input":"2024-11-27T18:53:42.300396Z","iopub.status.idle":"2024-11-27T18:53:42.306054Z","shell.execute_reply.started":"2024-11-27T18:53:42.300353Z","shell.execute_reply":"2024-11-27T18:53:42.305236Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# [ ] I try to get all possible params considered, but it is not that easy\n# [ ] nano has blocksize 1024\n# nano uses 0 dropout\nclass Config(): \n        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n        # C.model_type = 'gpt'\n        n_layer = 6\n        n_head = 6\n        n_embd = 64*6\n        # these options must be filled in externally\n        vocab_size = 50257\n        block_size = 1024\n        head_size = n_embd // n_head\n        # dropout hyperparameters\n        embd_pdrop = 0\n        resid_pdrop = 0\n        attn_pdrop = 0\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:42.308384Z","iopub.execute_input":"2024-11-27T18:53:42.309040Z","iopub.status.idle":"2024-11-27T18:53:42.317336Z","shell.execute_reply.started":"2024-11-27T18:53:42.309000Z","shell.execute_reply":"2024-11-27T18:53:42.316524Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = reflexTransformer.ReflexTransformer(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:42.318304Z","iopub.execute_input":"2024-11-27T18:53:42.318568Z","iopub.status.idle":"2024-11-27T18:53:43.363593Z","shell.execute_reply.started":"2024-11-27T18:53:42.318542Z","shell.execute_reply":"2024-11-27T18:53:43.362653Z"}},"outputs":[{"name":"stdout","text":"number of parameters: 30.33M\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/level1-6l6h-training/ckpt.pt', weights_only=False)['model'])\nmodel.to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:43.364863Z","iopub.execute_input":"2024-11-27T18:53:43.365251Z","iopub.status.idle":"2024-11-27T18:53:44.516505Z","shell.execute_reply.started":"2024-11-27T18:53:43.365208Z","shell.execute_reply":"2024-11-27T18:53:44.515659Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ReflexTransformer(\n  (transformer): ModuleDict(\n    (wte): Embedding(50257, 384)\n    (wpe): Embedding(1024, 384)\n    (drop): Dropout(p=0, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ra): MultiHeadReflexAttention(\n          (heads): ModuleList(\n            (0-5): 6 x ReflexAttention(\n              (key): Linear(in_features=384, out_features=64, bias=False)\n              (query): Linear(in_features=384, out_features=64, bias=False)\n              (value): Linear(in_features=384, out_features=64, bias=False)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (proj): Linear(in_features=384, out_features=384, bias=True)\n          (dropout): Dropout(p=0, inplace=False)\n        )\n        (mlp): ModuleDict(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n          (act): NewGELU()\n          (dropout): Dropout(p=0, inplace=False)\n        )\n        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=384, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:44.517756Z","iopub.execute_input":"2024-11-27T18:53:44.518154Z","iopub.status.idle":"2024-11-27T18:53:47.776147Z","shell.execute_reply.started":"2024-11-27T18:53:44.518114Z","shell.execute_reply":"2024-11-27T18:53:47.775235Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:47.777240Z","iopub.execute_input":"2024-11-27T18:53:47.778191Z","iopub.status.idle":"2024-11-27T18:53:48.516553Z","shell.execute_reply.started":"2024-11-27T18:53:47.778135Z","shell.execute_reply":"2024-11-27T18:53:48.515607Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def format_dataset(dataset):\n    \"\"\"\n    Formats a dataset stored in ds['train'] for GPT-2 input.\n\n    Parameters:\n    dataset (Dataset): The dataset containing multiple ARC examples.\n\n    Returns:\n    list: A list of formatted strings suitable for GPT-2 input.\n    \"\"\"\n    formatted_data = []\n    for example in dataset:\n        question = example[\"question\"]\n        choices = example[\"choices\"]\n        answer_key = example[\"answerKey\"]\n\n        # Create a prompt structure\n        formatted_choices = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(choices[\"label\"], choices[\"text\"])])\n        correct_answer = f\"The correct answer is: {answer_key}\"\n\n        # Combine everything into a single formatted string\n        formatted_input = f\"Question: {question}\\n\\nChoices:\\n{formatted_choices}\\n\\n{correct_answer}\"\n        formatted_data.append(formatted_input)\n    \n    return formatted_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:48.517661Z","iopub.execute_input":"2024-11-27T18:53:48.518102Z","iopub.status.idle":"2024-11-27T18:53:48.524300Z","shell.execute_reply.started":"2024-11-27T18:53:48.518072Z","shell.execute_reply":"2024-11-27T18:53:48.523347Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"res = format_dataset(ds['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:48.526948Z","iopub.execute_input":"2024-11-27T18:53:48.527322Z","iopub.status.idle":"2024-11-27T18:53:48.658375Z","shell.execute_reply.started":"2024-11-27T18:53:48.527294Z","shell.execute_reply":"2024-11-27T18:53:48.657173Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntrain = tokenizer(res, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)['input_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:48.659691Z","iopub.execute_input":"2024-11-27T18:53:48.660160Z","iopub.status.idle":"2024-11-27T18:53:49.094878Z","shell.execute_reply.started":"2024-11-27T18:53:48.660101Z","shell.execute_reply":"2024-11-27T18:53:49.093876Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import AdamW\n\n# Hyperparameters\nbatch_size = 32\nepochs = 1\nlearning_rate = 5e-5\n\n# Prepare data loader\ntrain_dataset = TensorDataset(train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Move model to device\nmodel.to(device)\n\n# Define optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    for batch in train_dataloader:\n        batch = batch[0].to(device)  # Move to device\n        optimizer.zero_grad()       # Zero gradients\n        \n        # Forward pass\n        outputs, loss = model(batch, targets=batch)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        print(f\"Loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:53:49.096223Z","iopub.execute_input":"2024-11-27T18:53:49.096521Z","iopub.status.idle":"2024-11-27T18:54:09.106970Z","shell.execute_reply.started":"2024-11-27T18:53:49.096493Z","shell.execute_reply":"2024-11-27T18:54:09.106033Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/1\nLoss: 6.781310081481934\nLoss: 5.056482315063477\nLoss: 4.056164264678955\nLoss: 3.628506660461426\nLoss: 3.3245227336883545\nLoss: 3.2118449211120605\nLoss: 3.092536211013794\nLoss: 3.080886125564575\nLoss: 2.83040714263916\nLoss: 2.643763542175293\nLoss: 2.77748966217041\nLoss: 2.903712272644043\nLoss: 2.5691919326782227\nLoss: 2.4700398445129395\nLoss: 2.4894444942474365\nLoss: 2.5609207153320312\nLoss: 2.4441065788269043\nLoss: 2.668217897415161\nLoss: 2.440274953842163\nLoss: 2.6520400047302246\nLoss: 2.510871410369873\nLoss: 2.6296803951263428\nLoss: 2.362942695617676\nLoss: 2.444359302520752\nLoss: 2.0768115520477295\nLoss: 2.2504847049713135\nLoss: 2.191472291946411\nLoss: 2.1208527088165283\nLoss: 2.451037883758545\nLoss: 2.1456515789031982\nLoss: 2.1040449142456055\nLoss: 2.2893497943878174\nLoss: 1.8656495809555054\nLoss: 2.1696741580963135\nLoss: 2.2204275131225586\nLoss: 2.155660629272461\nLoss: 2.076470375061035\nLoss: 1.9478766918182373\nLoss: 2.0486342906951904\nLoss: 2.0903825759887695\nLoss: 2.332881450653076\nLoss: 2.2322206497192383\nLoss: 1.931586503982544\nLoss: 2.0432090759277344\nLoss: 2.2891643047332764\nLoss: 1.8181244134902954\nLoss: 2.0080032348632812\nLoss: 1.8088257312774658\nLoss: 1.82644522190094\nLoss: 1.8253952264785767\nLoss: 1.9127823114395142\nLoss: 1.8186581134796143\nLoss: 1.7272495031356812\nLoss: 2.006291627883911\nLoss: 1.7140806913375854\nLoss: 1.7930256128311157\nLoss: 1.789873719215393\nLoss: 1.8139859437942505\nLoss: 1.9773025512695312\nLoss: 1.8972933292388916\nLoss: 1.833573341369629\nLoss: 1.8609683513641357\nLoss: 1.8759825229644775\nLoss: 1.8498862981796265\nLoss: 1.6707592010498047\nLoss: 1.8342701196670532\nLoss: 1.6741505861282349\nLoss: 1.526390790939331\nLoss: 1.6023298501968384\nLoss: 1.5952931642532349\nLoss: 1.781320571899414\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def preprocess_data(dataset, tokenizer, max_length=512):\n    \"\"\"\n    Tokenizes and processes the dataset for GPT-2 training.\n    \n    Args:\n    - dataset (list): List of strings (formatted examples).\n    - tokenizer: GPT-2 tokenizer.\n    - max_length (int): Maximum sequence length.\n\n    Returns:\n    - torch.Tensor: Input IDs tensor.\n    \"\"\"\n    inputs = tokenizer(dataset, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=True)\n    return inputs[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:02:41.861103Z","iopub.execute_input":"2024-11-27T19:02:41.861470Z","iopub.status.idle":"2024-11-27T19:02:41.866454Z","shell.execute_reply.started":"2024-11-27T19:02:41.861438Z","shell.execute_reply":"2024-11-27T19:02:41.865489Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def format_for_gpt2_no_answer(arc_example):\n    \"\"\"\n    Formats an ARC dataset example for GPT-2 input without including the answer.\n\n    Parameters:\n    arc_example (dict): A dictionary containing the ARC question, answer choices, and correct answer.\n\n    Returns:\n    str: A formatted string without the answer.\n    \"\"\"\n    question = arc_example[\"question\"]\n    choices = arc_example[\"choices\"]\n\n    # Create a prompt structure without the answer\n    formatted_choices = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(choices[\"label\"], choices[\"text\"])])\n    formatted_input = f\"Question: {question}\\n\\nChoices:\\n{formatted_choices}\\n\\n\"\n    \n    return formatted_input\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:02:43.780661Z","iopub.execute_input":"2024-11-27T19:02:43.781038Z","iopub.status.idle":"2024-11-27T19:02:43.786344Z","shell.execute_reply.started":"2024-11-27T19:02:43.781007Z","shell.execute_reply":"2024-11-27T19:02:43.785388Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"formatted_test_data_no_answer = [format_for_gpt2_no_answer(example) for example in ds['test']]\ntest_input_ids_no_answer = preprocess_data(formatted_test_data_no_answer, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:02:45.102107Z","iopub.execute_input":"2024-11-27T19:02:45.102462Z","iopub.status.idle":"2024-11-27T19:02:45.615068Z","shell.execute_reply.started":"2024-11-27T19:02:45.102431Z","shell.execute_reply":"2024-11-27T19:02:45.614275Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"correct = 0\ntotal = 0\n\nwith torch.no_grad():\n    for example in ds['test']:\n        question = example[\"question\"]\n        choices = example[\"choices\"][\"text\"]\n        labels = example[\"choices\"][\"label\"]\n        correct_answer = example[\"answerKey\"]\n\n        # Format input without the answer\n        formatted_input = format_for_gpt2_no_answer(example)\n        input_ids = tokenizer(formatted_input, return_tensors=\"pt\").input_ids.to(device)\n\n        # Generate model output\n        output_ids = model.generate(input_ids, max_new_tokens=5)\n        predicted_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n        # Determine which choice the model selected\n        for label, choice in zip(labels, choices):\n            if choice in predicted_text:\n                predicted_answer = label\n                break\n        else:\n            predicted_answer = None  # If no choice matches\n\n        # Check if the prediction is correct\n        if predicted_answer == correct_answer:\n            correct += 1\n        total += 1\n\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:03:38.459801Z","iopub.execute_input":"2024-11-27T19:03:38.460230Z","iopub.status.idle":"2024-11-27T19:06:14.873366Z","shell.execute_reply.started":"2024-11-27T19:03:38.460194Z","shell.execute_reply":"2024-11-27T19:06:14.872418Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 25.08%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:07:42.219072Z","iopub.execute_input":"2024-11-27T19:07:42.219455Z","iopub.status.idle":"2024-11-27T19:07:42.226180Z","shell.execute_reply.started":"2024-11-27T19:07:42.219421Z","shell.execute_reply":"2024-11-27T19:07:42.224898Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"2376"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"correct","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:07:47.185743Z","iopub.execute_input":"2024-11-27T19:07:47.186489Z","iopub.status.idle":"2024-11-27T19:07:47.193273Z","shell.execute_reply.started":"2024-11-27T19:07:47.186431Z","shell.execute_reply":"2024-11-27T19:07:47.191932Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"596"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom mingpt.utils import set_seed\nset_seed(3407)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nclass SortDataset(Dataset):\n    \"\"\" \n    Dataset for the Sort problem. E.g. for problem length 6:\n    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n    Which will feed into the transformer concatenated as:\n    input:  0 0 2 1 0 1 0 0 0 1 1\n    output: I I I I I 0 0 0 1 1 2\n    where I is \"ignore\", as the transformer is reading the input sequence\n    \"\"\"\n\n    def __init__(self, split, length=6, num_digits=3):\n        assert split in {'train', 'test'}\n        self.split = split\n        self.length = length\n        self.num_digits = num_digits\n    \n    def __len__(self):\n        return 10000 # ...\n    \n    def get_vocab_size(self):\n        return self.num_digits\n    \n    def get_block_size(self):\n        # the length of the sequence that will feed into transformer, \n        # containing concatenated input and the output, but -1 because\n        # the transformer starts making predictions at the last input element\n        return self.length * 2 - 1\n\n    def __getitem__(self, idx):\n        \n        # use rejection sampling to generate an input example from the desired split\n        while True:\n            # generate some random integers\n            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n            # half of the time let's try to boost the number of examples that \n            # have a large number of repeats, as this is what the model seems to struggle\n            # with later in training, and they are kind of rate\n            if torch.rand(1).item() < 0.5:\n                if inp.unique().nelement() > self.length // 2:\n                    # too many unqiue digits, re-sample\n                    continue\n            # figure out if this generated example is train or test based on its hash\n            h = hash(pickle.dumps(inp.tolist()))\n            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n            if inp_split == self.split:\n                break # ok\n        \n        # solve the task: i.e. sort\n        sol = torch.sort(inp)[0]\n\n        # concatenate the problem specification and the solution\n        cat = torch.cat((inp, sol), dim=0)\n\n        # the inputs to the transformer will be the offset sequence\n        x = cat[:-1].clone()\n        y = cat[1:].clone()\n        # we only want to predict at output locations, mask out the loss at the input locations\n        y[:self.length-1] = -1\n        return x, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print an example instance of the dataset\ntrain_dataset = SortDataset('train', length = 500, num_digits=10)\ntest_dataset = SortDataset('test')\nx, y = train_dataset[0]\n#for a, b in zip(x,y):\n#    print(int(a),int(b))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create a GPT instance\nfrom mingpt.model import GPT\n\n#model_config = GPT.get_default_config()\n#model_config.model_type = 'gpt-nano'\n#model_config.vocab_size = train_dataset.get_vocab_size()\n#model_config.block_size = train_dataset.get_block_size()\n#model = GPT(model_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create a Trainer object\nfrom mingpt.trainer import Trainer\n\ntrain_config = Trainer.get_default_config()\ntrain_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\ntrain_config.max_iters = 2000\ntrain_config.num_workers = 0\ntrain_config.batch_size = 8\ntrainer = Trainer(train_config, model, train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_end_callback(trainer):\n    if trainer.iter_num % 100 == 0:\n        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\ntrainer.set_callback('on_batch_end', batch_end_callback)\n\ntrainer.run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}