{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd85b53d",
   "metadata": {
    "papermill": {
     "duration": 0.010473,
     "end_time": "2024-11-26T09:18:27.455982",
     "exception": false,
     "start_time": "2024-11-26T09:18:27.445509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- в каждом блоке трансформера несколько голов, тогда будем в части голов считать атеншн по текущим хидденам (SA(QKV)), \n",
    "а в части голов KV считаем по хидденам после предыдущих блоков и эмбеддингам токенов, и Q по текущим хидденам (CA(KV, Q))\n",
    "- $Attn_i = Cat[SA(h_i), CA(h_{i-1}, h_i), CA(h_{i-2}, h_i)]$\n",
    "- в первом и втором слое все головы считаются по текущему контексту, начиная с 3 делаем reflex attention\n",
    "- в этой секции зафиксируем, что на SA и на каждый из CA по 2 головы (всего 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc61a4b",
   "metadata": {
    "papermill": {
     "duration": 0.007951,
     "end_time": "2024-11-26T09:18:27.472561",
     "exception": false,
     "start_time": "2024-11-26T09:18:27.464610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8a5bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:27.491365Z",
     "iopub.status.busy": "2024-11-26T09:18:27.490728Z",
     "iopub.status.idle": "2024-11-26T09:18:27.511909Z",
     "shell.execute_reply": "2024-11-26T09:18:27.510906Z"
    },
    "papermill": {
     "duration": 0.03277,
     "end_time": "2024-11-26T09:18:27.513522",
     "exception": false,
     "start_time": "2024-11-26T09:18:27.480752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mingpt/mingpt/trainer.py\n",
      "/kaggle/input/mingpt/mingpt/bpe.py\n",
      "/kaggle/input/mingpt/mingpt/model.py\n",
      "/kaggle/input/mingpt/mingpt/utils.py\n",
      "/kaggle/input/mingpt/mingpt/__init__.py\n",
      "/kaggle/input/openwebtext-data-prepared-for-nanogpt/train.bin\n",
      "/kaggle/input/openwebtext-data-prepared-for-nanogpt/val.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5368535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:27.531341Z",
     "iopub.status.busy": "2024-11-26T09:18:27.531064Z",
     "iopub.status.idle": "2024-11-26T09:18:27.534587Z",
     "shell.execute_reply": "2024-11-26T09:18:27.533936Z"
    },
    "papermill": {
     "duration": 0.014268,
     "end_time": "2024-11-26T09:18:27.536101",
     "exception": false,
     "start_time": "2024-11-26T09:18:27.521833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/mingpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc38b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:27.554215Z",
     "iopub.status.busy": "2024-11-26T09:18:27.553667Z",
     "iopub.status.idle": "2024-11-26T09:18:30.713270Z",
     "shell.execute_reply": "2024-11-26T09:18:30.712333Z"
    },
    "papermill": {
     "duration": 3.170996,
     "end_time": "2024-11-26T09:18:30.715301",
     "exception": false,
     "start_time": "2024-11-26T09:18:27.544305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mingpt.bpe\n",
    "import mingpt.utils\n",
    "import mingpt.model\n",
    "import mingpt.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5b1fb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.733513Z",
     "iopub.status.busy": "2024-11-26T09:18:30.733110Z",
     "iopub.status.idle": "2024-11-26T09:18:30.737030Z",
     "shell.execute_reply": "2024-11-26T09:18:30.736350Z"
    },
    "papermill": {
     "duration": 0.014675,
     "end_time": "2024-11-26T09:18:30.738613",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.723938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b93400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.756402Z",
     "iopub.status.busy": "2024-11-26T09:18:30.755863Z",
     "iopub.status.idle": "2024-11-26T09:18:30.759507Z",
     "shell.execute_reply": "2024-11-26T09:18:30.758712Z"
    },
    "papermill": {
     "duration": 0.014118,
     "end_time": "2024-11-26T09:18:30.761103",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.746985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dd7dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.778857Z",
     "iopub.status.busy": "2024-11-26T09:18:30.778243Z",
     "iopub.status.idle": "2024-11-26T09:18:30.786596Z",
     "shell.execute_reply": "2024-11-26T09:18:30.786034Z"
    },
    "papermill": {
     "duration": 0.018845,
     "end_time": "2024-11-26T09:18:30.788135",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.769290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mingpt.utils import set_seed\n",
    "from mingpt.bpe import BPETokenizer\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4804a49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.805932Z",
     "iopub.status.busy": "2024-11-26T09:18:30.805381Z",
     "iopub.status.idle": "2024-11-26T09:18:30.808739Z",
     "shell.execute_reply": "2024-11-26T09:18:30.808072Z"
    },
    "papermill": {
     "duration": 0.013886,
     "end_time": "2024-11-26T09:18:30.810274",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.796388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's run to see if layers can be accessed by names and added to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98385b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.827971Z",
     "iopub.status.busy": "2024-11-26T09:18:30.827288Z",
     "iopub.status.idle": "2024-11-26T09:18:30.833871Z",
     "shell.execute_reply": "2024-11-26T09:18:30.833053Z"
    },
    "papermill": {
     "duration": 0.017038,
     "end_time": "2024-11-26T09:18:30.835440",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.818402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Efficient implementation equivalent to the following:\\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\\n        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\\n    L, S = query.size(-2), key.size(-2)\\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\\n    attn_bias = torch.zeros(L, S, dtype=query.dtype)\\n    if is_causal:\\n        assert attn_mask is None\\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\\n        attn_bias.to(query.dtype)\\n\\n    if attn_mask is not None:\\n        if attn_mask.dtype == torch.bool:\\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\\n        else:\\n            attn_bias += attn_mask\\n\\n    if enable_gqa:\\n        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\\n        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\\n\\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\\n    attn_weight += attn_bias\\n    attn_weight = torch.softmax(attn_weight, dim=-1)\\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\\n    return attn_weight @ value'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Efficient implementation equivalent to the following:\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
    "        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b542db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.853108Z",
     "iopub.status.busy": "2024-11-26T09:18:30.852825Z",
     "iopub.status.idle": "2024-11-26T09:18:30.857210Z",
     "shell.execute_reply": "2024-11-26T09:18:30.856444Z"
    },
    "papermill": {
     "duration": 0.015067,
     "end_time": "2024-11-26T09:18:30.858792",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.843725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041b6518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.876718Z",
     "iopub.status.busy": "2024-11-26T09:18:30.876012Z",
     "iopub.status.idle": "2024-11-26T09:18:30.883690Z",
     "shell.execute_reply": "2024-11-26T09:18:30.882935Z"
    },
    "papermill": {
     "duration": 0.018337,
     "end_time": "2024-11-26T09:18:30.885370",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.867033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It is easier to add 3 separate W_q, W_k, W_v for now\n",
    "# models in the experiments will be relatively small anyway\n",
    "# [ ] do we need tril? I guess yes, because otherwise query would look into the future\n",
    "# [ ] tried to make make as close to scaled_dot_product_attention\n",
    "class ReflexAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.key = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
    "\n",
    "        #self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attn_pdrop)\n",
    "        #self.n_head = config.n_head [ ] do i need this\n",
    "        #self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x, x_prev = None):\n",
    "        B,T,C = x.shape\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        if x_prev is not None:\n",
    "            B_prev, T_prev, C_prev = x_prev.size()\n",
    "            # DEBUG print(B == B_prev, T == T_prev, C, C_prev)\n",
    "            k = self.key(x_prev)   # (B,T,hs)\n",
    "            v = self.value(x_prev) # (B,T,hs)\n",
    "        else:\n",
    "            k = self.key(x)   # (B,T,hs)\n",
    "            v = self.value(x) # (B,T,hs)\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b1932e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.904079Z",
     "iopub.status.busy": "2024-11-26T09:18:30.903424Z",
     "iopub.status.idle": "2024-11-26T09:18:30.910418Z",
     "shell.execute_reply": "2024-11-26T09:18:30.909762Z"
    },
    "papermill": {
     "duration": 0.018411,
     "end_time": "2024-11-26T09:18:30.911957",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.893546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [ ] there is a mess with drop out\n",
    "class MultiHeadReflexAttention(nn.Module):\n",
    "    # heads in parallel\n",
    "    # [ ] we just split and than concat, they are independent\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([ReflexAttention(config) for _ in range(config.n_head)])\n",
    "        self.proj = nn.Linear(config.head_size * config.n_head, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    \"\"\" self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\"\"\"\n",
    "    def forward(self, x, b_i):\n",
    "        out = []\n",
    "        for h_i, h in enumerate(self.heads):\n",
    "            # DEBUG print('forward', x.shape, b_i, h_i)\n",
    "            if b_i == 0 or b_i == 1:\n",
    "                t = h(x)\n",
    "                out.append(t)\n",
    "                continue\n",
    "            if h_i == 0 or h_i == 1:\n",
    "                out.append(h(x))\n",
    "            else:\n",
    "                # DEBUG print(hdn.hiddens[b_i-1].shape, embed.x_embed.shape)\n",
    "                if h_i == 2 or h_i == 3:\n",
    "                    out.append(h(x=x, x_prev=hdn.hiddens[b_i-1]+embed.x_embed))\n",
    "                elif h_i == 4 or h_i ==5:\n",
    "                    out.append(h(x=x, x_prev=hdn.hiddens[b_i-2]+embed.x_embed))\n",
    "        # DEBUG print('cat', [r.shape for r in out])       \n",
    "        # DEBUG print('proj', config.head_size * config.n_head, config.n_embd)\n",
    "        out = torch.cat(out, dim=-1) # [ ] check dim\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f69e9ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.929850Z",
     "iopub.status.busy": "2024-11-26T09:18:30.929199Z",
     "iopub.status.idle": "2024-11-26T09:18:30.935061Z",
     "shell.execute_reply": "2024-11-26T09:18:30.934301Z"
    },
    "papermill": {
     "duration": 0.016308,
     "end_time": "2024-11-26T09:18:30.936585",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.920277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check head size\n",
    "# add config\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        # head_size = n_embd // n_head\n",
    "        self.ra = MultiHeadReflexAttention(config)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x, b_i):\n",
    "        x = x + self.ra(self.ln1(x), b_i)\n",
    "        x = x + self.mlpf(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11dfac50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.954367Z",
     "iopub.status.busy": "2024-11-26T09:18:30.953725Z",
     "iopub.status.idle": "2024-11-26T09:18:30.957992Z",
     "shell.execute_reply": "2024-11-26T09:18:30.957308Z"
    },
    "papermill": {
     "duration": 0.014692,
     "end_time": "2024-11-26T09:18:30.959557",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.944865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [ ] I try to get all possible params considered, but it is not that easy\n",
    "# [ ] nano has blocksize 1024\n",
    "# nano uses 0 dropout\n",
    "class Config(): \n",
    "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "        # C.model_type = 'gpt'\n",
    "        n_layer = 6\n",
    "        n_head = 6\n",
    "        n_embd = 64*6\n",
    "        # these options must be filled in externally\n",
    "        vocab_size = 50257\n",
    "        block_size = 1024\n",
    "        head_size = n_embd // n_head\n",
    "        # dropout hyperparameters\n",
    "        embd_pdrop = 0\n",
    "        resid_pdrop = 0\n",
    "        attn_pdrop = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52cc0600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.977410Z",
     "iopub.status.busy": "2024-11-26T09:18:30.976847Z",
     "iopub.status.idle": "2024-11-26T09:18:30.980318Z",
     "shell.execute_reply": "2024-11-26T09:18:30.979599Z"
    },
    "papermill": {
     "duration": 0.014108,
     "end_time": "2024-11-26T09:18:30.981942",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.967834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e0897a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:30.999663Z",
     "iopub.status.busy": "2024-11-26T09:18:30.999203Z",
     "iopub.status.idle": "2024-11-26T09:18:31.002856Z",
     "shell.execute_reply": "2024-11-26T09:18:31.002102Z"
    },
    "papermill": {
     "duration": 0.014103,
     "end_time": "2024-11-26T09:18:31.004422",
     "exception": false,
     "start_time": "2024-11-26T09:18:30.990319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Hiddens():\n",
    "    hiddens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5374f70d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:31.022227Z",
     "iopub.status.busy": "2024-11-26T09:18:31.021974Z",
     "iopub.status.idle": "2024-11-26T09:18:31.025461Z",
     "shell.execute_reply": "2024-11-26T09:18:31.024833Z"
    },
    "papermill": {
     "duration": 0.013989,
     "end_time": "2024-11-26T09:18:31.027042",
     "exception": false,
     "start_time": "2024-11-26T09:18:31.013053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdn = Hiddens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e60fdd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:31.045016Z",
     "iopub.status.busy": "2024-11-26T09:18:31.044435Z",
     "iopub.status.idle": "2024-11-26T09:18:31.048102Z",
     "shell.execute_reply": "2024-11-26T09:18:31.047407Z"
    },
    "papermill": {
     "duration": 0.014275,
     "end_time": "2024-11-26T09:18:31.049604",
     "exception": false,
     "start_time": "2024-11-26T09:18:31.035329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embed():\n",
    "    x_embed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14346458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:31.067534Z",
     "iopub.status.busy": "2024-11-26T09:18:31.066955Z",
     "iopub.status.idle": "2024-11-26T09:18:31.070562Z",
     "shell.execute_reply": "2024-11-26T09:18:31.069909Z"
    },
    "papermill": {
     "duration": 0.014306,
     "end_time": "2024-11-26T09:18:31.072058",
     "exception": false,
     "start_time": "2024-11-26T09:18:31.057752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed = Embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c575cd45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:31.089822Z",
     "iopub.status.busy": "2024-11-26T09:18:31.089582Z",
     "iopub.status.idle": "2024-11-26T09:18:31.108773Z",
     "shell.execute_reply": "2024-11-26T09:18:31.108107Z"
    },
    "papermill": {
     "duration": 0.030089,
     "end_time": "2024-11-26T09:18:31.110430",
     "exception": false,
     "start_time": "2024-11-26T09:18:31.080341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReflexTransformer(nn.Module):\n",
    "    \"\"\" Transformer with reflex attention \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        #assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        embed.x_embed = self.transformer.drop(tok_emb + pos_emb)\n",
    "       \n",
    "        hdn.hiddens = []\n",
    "        # DEBUG print(x.shape, embed.x_embed.shape)\n",
    "        for b_i, block in enumerate(self.transformer.h):\n",
    "            x = block(x, b_i)\n",
    "            hdn.hiddens.append(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -config.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    \n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = config # [ ] was self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8bbf69f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:31.128703Z",
     "iopub.status.busy": "2024-11-26T09:18:31.128063Z",
     "iopub.status.idle": "2024-11-26T09:18:31.131591Z",
     "shell.execute_reply": "2024-11-26T09:18:31.130907Z"
    },
    "papermill": {
     "duration": 0.014492,
     "end_time": "2024-11-26T09:18:31.133364",
     "exception": false,
     "start_time": "2024-11-26T09:18:31.118872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e33687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:31.151246Z",
     "iopub.status.busy": "2024-11-26T09:18:31.150733Z",
     "iopub.status.idle": "2024-11-26T09:18:32.192214Z",
     "shell.execute_reply": "2024-11-26T09:18:32.191203Z"
    },
    "papermill": {
     "duration": 1.05209,
     "end_time": "2024-11-26T09:18:32.193917",
     "exception": false,
     "start_time": "2024-11-26T09:18:31.141827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 30.33M\n"
     ]
    }
   ],
   "source": [
    "model = ReflexTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f5485ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:32.212213Z",
     "iopub.status.busy": "2024-11-26T09:18:32.211612Z",
     "iopub.status.idle": "2024-11-26T09:18:32.536548Z",
     "shell.execute_reply": "2024-11-26T09:18:32.535701Z"
    },
    "papermill": {
     "duration": 0.337145,
     "end_time": "2024-11-26T09:18:32.539572",
     "exception": false,
     "start_time": "2024-11-26T09:18:32.202427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4873f049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:32.573942Z",
     "iopub.status.busy": "2024-11-26T09:18:32.573511Z",
     "iopub.status.idle": "2024-11-26T09:18:33.574089Z",
     "shell.execute_reply": "2024-11-26T09:18:33.573014Z"
    },
    "papermill": {
     "duration": 1.020263,
     "end_time": "2024-11-26T09:18:33.575882",
     "exception": false,
     "start_time": "2024-11-26T09:18:32.555619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json to /root/.cache/mingpt/encoder.json\n",
      "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe to /root/.cache/mingpt/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a7cc2c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:33.594806Z",
     "iopub.status.busy": "2024-11-26T09:18:33.594086Z",
     "iopub.status.idle": "2024-11-26T09:18:33.601193Z",
     "shell.execute_reply": "2024-11-26T09:18:33.600392Z"
    },
    "papermill": {
     "duration": 0.018099,
     "end_time": "2024-11-26T09:18:33.602768",
     "exception": false,
     "start_time": "2024-11-26T09:18:33.584669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tokenizer('test test 1 2 3').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b5925a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:33.621174Z",
     "iopub.status.busy": "2024-11-26T09:18:33.620486Z",
     "iopub.status.idle": "2024-11-26T09:18:34.208287Z",
     "shell.execute_reply": "2024-11-26T09:18:34.207314Z"
    },
    "papermill": {
     "duration": 0.59918,
     "end_time": "2024-11-26T09:18:34.210428",
     "exception": false,
     "start_time": "2024-11-26T09:18:33.611248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = model.generate(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c34834c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.228933Z",
     "iopub.status.busy": "2024-11-26T09:18:34.228606Z",
     "iopub.status.idle": "2024-11-26T09:18:34.234161Z",
     "shell.execute_reply": "2024-11-26T09:18:34.233170Z"
    },
    "papermill": {
     "duration": 0.016719,
     "end_time": "2024-11-26T09:18:34.236008",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.219289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "test test 1 2 3 E Unleashedgame sorcery antennas raplaughter trout Anchorage Scotland\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    out = tokenizer.decode(y[0].cpu().squeeze())\n",
    "    print('-'*10)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e604f2f",
   "metadata": {
    "papermill": {
     "duration": 0.008265,
     "end_time": "2024-11-26T09:18:34.252808",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.244543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training on openwebtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b8be720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.270885Z",
     "iopub.status.busy": "2024-11-26T09:18:34.270575Z",
     "iopub.status.idle": "2024-11-26T09:18:34.274772Z",
     "shell.execute_reply": "2024-11-26T09:18:34.273954Z"
    },
    "papermill": {
     "duration": 0.015143,
     "end_time": "2024-11-26T09:18:34.276500",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.261357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_iters=1000\n",
    "log_interval=1\n",
    "eval_interval=200\n",
    "eval_iters=20\n",
    "learning_rate=0.00008\n",
    "gradient_accumulation_steps=4\n",
    "batch_size=8\n",
    "compile=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73aefb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.294875Z",
     "iopub.status.busy": "2024-11-26T09:18:34.294589Z",
     "iopub.status.idle": "2024-11-26T09:18:34.298288Z",
     "shell.execute_reply": "2024-11-26T09:18:34.297583Z"
    },
    "papermill": {
     "duration": 0.014752,
     "end_time": "2024-11-26T09:18:34.299832",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.285080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "dtype = 'bfloat16'\n",
    "compile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74201404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.318163Z",
     "iopub.status.busy": "2024-11-26T09:18:34.317617Z",
     "iopub.status.idle": "2024-11-26T09:18:34.322452Z",
     "shell.execute_reply": "2024-11-26T09:18:34.321650Z"
    },
    "papermill": {
     "duration": 0.015665,
     "end_time": "2024-11-26T09:18:34.324010",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.308345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1e6e198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.342398Z",
     "iopub.status.busy": "2024-11-26T09:18:34.341846Z",
     "iopub.status.idle": "2024-11-26T09:18:34.348501Z",
     "shell.execute_reply": "2024-11-26T09:18:34.347671Z"
    },
    "papermill": {
     "duration": 0.017579,
     "end_time": "2024-11-26T09:18:34.350146",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.332567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join('/kaggle/input/openwebtext-data-prepared-for-nanogpt') # [ ] Removed ,dataset\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2358212a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.370289Z",
     "iopub.status.busy": "2024-11-26T09:18:34.369995Z",
     "iopub.status.idle": "2024-11-26T09:18:34.373644Z",
     "shell.execute_reply": "2024-11-26T09:18:34.372934Z"
    },
    "papermill": {
     "duration": 0.015197,
     "end_time": "2024-11-26T09:18:34.375220",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.360023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83ab3c14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.393618Z",
     "iopub.status.busy": "2024-11-26T09:18:34.393098Z",
     "iopub.status.idle": "2024-11-26T09:18:34.396612Z",
     "shell.execute_reply": "2024-11-26T09:18:34.395992Z"
    },
    "papermill": {
     "duration": 0.014461,
     "end_time": "2024-11-26T09:18:34.398208",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.383747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_type = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52d842c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.416425Z",
     "iopub.status.busy": "2024-11-26T09:18:34.416132Z",
     "iopub.status.idle": "2024-11-26T09:18:34.420138Z",
     "shell.execute_reply": "2024-11-26T09:18:34.419284Z"
    },
    "papermill": {
     "duration": 0.015033,
     "end_time": "2024-11-26T09:18:34.421822",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.406789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Train_config():\n",
    "    weight_decay = 1e-1\n",
    "    betas = [0.9, 0.95]\n",
    "    learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ed4a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:34.440214Z",
     "iopub.status.busy": "2024-11-26T09:18:34.439727Z",
     "iopub.status.idle": "2024-11-26T09:18:35.603133Z",
     "shell.execute_reply": "2024-11-26T09:18:35.602207Z"
    },
    "papermill": {
     "duration": 1.174666,
     "end_time": "2024-11-26T09:18:35.605177",
     "exception": false,
     "start_time": "2024-11-26T09:18:34.430511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = model.configure_optimizers(Train_config()) # [ ] Remvoed device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b358a50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.623947Z",
     "iopub.status.busy": "2024-11-26T09:18:35.623496Z",
     "iopub.status.idle": "2024-11-26T09:18:35.628133Z",
     "shell.execute_reply": "2024-11-26T09:18:35.627252Z"
    },
    "papermill": {
     "duration": 0.015979,
     "end_time": "2024-11-26T09:18:35.629961",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.613982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1972268714.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20d68ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.649324Z",
     "iopub.status.busy": "2024-11-26T09:18:35.648609Z",
     "iopub.status.idle": "2024-11-26T09:18:35.654435Z",
     "shell.execute_reply": "2024-11-26T09:18:35.653626Z"
    },
    "papermill": {
     "duration": 0.017142,
     "end_time": "2024-11-26T09:18:35.656017",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.638875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de64dacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.675569Z",
     "iopub.status.busy": "2024-11-26T09:18:35.674969Z",
     "iopub.status.idle": "2024-11-26T09:18:35.680073Z",
     "shell.execute_reply": "2024-11-26T09:18:35.679184Z"
    },
    "papermill": {
     "duration": 0.01656,
     "end_time": "2024-11-26T09:18:35.681804",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.665244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab038f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.700784Z",
     "iopub.status.busy": "2024-11-26T09:18:35.700483Z",
     "iopub.status.idle": "2024-11-26T09:18:35.704298Z",
     "shell.execute_reply": "2024-11-26T09:18:35.703455Z"
    },
    "papermill": {
     "duration": 0.015066,
     "end_time": "2024-11-26T09:18:35.705805",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.690739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bafa75a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.724775Z",
     "iopub.status.busy": "2024-11-26T09:18:35.724191Z",
     "iopub.status.idle": "2024-11-26T09:18:35.727746Z",
     "shell.execute_reply": "2024-11-26T09:18:35.726970Z"
    },
    "papermill": {
     "duration": 0.01503,
     "end_time": "2024-11-26T09:18:35.729618",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.714588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6475e15a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.749401Z",
     "iopub.status.busy": "2024-11-26T09:18:35.748778Z",
     "iopub.status.idle": "2024-11-26T09:18:35.752691Z",
     "shell.execute_reply": "2024-11-26T09:18:35.751855Z"
    },
    "papermill": {
     "duration": 0.015372,
     "end_time": "2024-11-26T09:18:35.754463",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.739091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_procesc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b154e619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.773295Z",
     "iopub.status.busy": "2024-11-26T09:18:35.772998Z",
     "iopub.status.idle": "2024-11-26T09:18:35.776738Z",
     "shell.execute_reply": "2024-11-26T09:18:35.775937Z"
    },
    "papermill": {
     "duration": 0.015246,
     "end_time": "2024-11-26T09:18:35.778512",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.763266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab9c8d0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.796801Z",
     "iopub.status.busy": "2024-11-26T09:18:35.796543Z",
     "iopub.status.idle": "2024-11-26T09:18:35.800057Z",
     "shell.execute_reply": "2024-11-26T09:18:35.799299Z"
    },
    "papermill": {
     "duration": 0.014471,
     "end_time": "2024-11-26T09:18:35.801671",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.787200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85aee1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.820056Z",
     "iopub.status.busy": "2024-11-26T09:18:35.819713Z",
     "iopub.status.idle": "2024-11-26T09:18:35.823271Z",
     "shell.execute_reply": "2024-11-26T09:18:35.822558Z"
    },
    "papermill": {
     "duration": 0.014721,
     "end_time": "2024-11-26T09:18:35.824831",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.810110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = config.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ef57f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.843375Z",
     "iopub.status.busy": "2024-11-26T09:18:35.843091Z",
     "iopub.status.idle": "2024-11-26T09:18:35.846765Z",
     "shell.execute_reply": "2024-11-26T09:18:35.846148Z"
    },
    "papermill": {
     "duration": 0.014692,
     "end_time": "2024-11-26T09:18:35.848335",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.833643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decay_lr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f759c6f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.866759Z",
     "iopub.status.busy": "2024-11-26T09:18:35.866172Z",
     "iopub.status.idle": "2024-11-26T09:18:35.870515Z",
     "shell.execute_reply": "2024-11-26T09:18:35.869934Z"
    },
    "papermill": {
     "duration": 0.015146,
     "end_time": "2024-11-26T09:18:35.872067",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.856921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warmup_iters = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "415e5a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.890205Z",
     "iopub.status.busy": "2024-11-26T09:18:35.889945Z",
     "iopub.status.idle": "2024-11-26T09:18:35.893387Z",
     "shell.execute_reply": "2024-11-26T09:18:35.892708Z"
    },
    "papermill": {
     "duration": 0.014325,
     "end_time": "2024-11-26T09:18:35.894959",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.880634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ab94c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.913515Z",
     "iopub.status.busy": "2024-11-26T09:18:35.912860Z",
     "iopub.status.idle": "2024-11-26T09:18:35.916591Z",
     "shell.execute_reply": "2024-11-26T09:18:35.915931Z"
    },
    "papermill": {
     "duration": 0.014654,
     "end_time": "2024-11-26T09:18:35.918272",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.903618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "469fd98d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.936337Z",
     "iopub.status.busy": "2024-11-26T09:18:35.936070Z",
     "iopub.status.idle": "2024-11-26T09:18:35.940435Z",
     "shell.execute_reply": "2024-11-26T09:18:35.939612Z"
    },
    "papermill": {
     "duration": 0.015373,
     "end_time": "2024-11-26T09:18:35.942097",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.926724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55bf7958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.960338Z",
     "iopub.status.busy": "2024-11-26T09:18:35.959748Z",
     "iopub.status.idle": "2024-11-26T09:18:35.963611Z",
     "shell.execute_reply": "2024-11-26T09:18:35.962764Z"
    },
    "papermill": {
     "duration": 0.014615,
     "end_time": "2024-11-26T09:18:35.965170",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.950555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c07815fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:35.984026Z",
     "iopub.status.busy": "2024-11-26T09:18:35.983309Z",
     "iopub.status.idle": "2024-11-26T09:18:35.987586Z",
     "shell.execute_reply": "2024-11-26T09:18:35.986773Z"
    },
    "papermill": {
     "duration": 0.015296,
     "end_time": "2024-11-26T09:18:35.989145",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.973849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
    "                  bias=False, vocab_size=None, dropout=0) # start with model_args from command line\n",
    "# [ ] dropout is set to 0\n",
    "# [ ] bias is set to False\n",
    "out_dir = '/kaggle/working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3d1efcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:18:36.007768Z",
     "iopub.status.busy": "2024-11-26T09:18:36.007489Z",
     "iopub.status.idle": "2024-11-26T09:57:22.045701Z",
     "shell.execute_reply": "2024-11-26T09:57:22.044659Z"
    },
    "papermill": {
     "duration": 2326.049657,
     "end_time": "2024-11-26T09:57:22.047422",
     "exception": false,
     "start_time": "2024-11-26T09:18:35.997765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9165, val loss 10.9206\n",
      "iter 0: loss 10.9336, time 10809.00ms, mfu -100.00%\n",
      "iter 1: loss 10.9224, time 2077.51ms, mfu -100.00%\n",
      "iter 2: loss 10.9180, time 2265.09ms, mfu -100.00%\n",
      "iter 3: loss 10.9296, time 2266.94ms, mfu -100.00%\n",
      "iter 4: loss 10.9179, time 2268.25ms, mfu -100.00%\n",
      "iter 5: loss 10.9165, time 2264.42ms, mfu 1.50%\n",
      "iter 6: loss 10.9161, time 2265.29ms, mfu 1.50%\n",
      "iter 7: loss 10.9019, time 2266.16ms, mfu 1.50%\n",
      "iter 8: loss 10.9092, time 2265.21ms, mfu 1.50%\n",
      "iter 9: loss 10.9201, time 2265.88ms, mfu 1.50%\n",
      "iter 10: loss 10.9156, time 2268.37ms, mfu 1.50%\n",
      "iter 11: loss 10.8911, time 2268.89ms, mfu 1.50%\n",
      "iter 12: loss 10.9125, time 2268.05ms, mfu 1.50%\n",
      "iter 13: loss 10.9153, time 2265.26ms, mfu 1.50%\n",
      "iter 14: loss 10.8818, time 2266.48ms, mfu 1.50%\n",
      "iter 15: loss 10.8835, time 2268.18ms, mfu 1.50%\n",
      "iter 16: loss 10.8849, time 2265.94ms, mfu 1.50%\n",
      "iter 17: loss 10.8742, time 2266.74ms, mfu 1.50%\n",
      "iter 18: loss 10.8646, time 2268.11ms, mfu 1.50%\n",
      "iter 19: loss 10.8619, time 2267.42ms, mfu 1.50%\n",
      "iter 20: loss 10.8681, time 2264.45ms, mfu 1.50%\n",
      "iter 21: loss 10.8510, time 2267.81ms, mfu 1.50%\n",
      "iter 22: loss 10.8500, time 2268.43ms, mfu 1.50%\n",
      "iter 23: loss 10.8393, time 2269.24ms, mfu 1.50%\n",
      "iter 24: loss 10.8185, time 2268.87ms, mfu 1.50%\n",
      "iter 25: loss 10.8126, time 2268.12ms, mfu 1.50%\n",
      "iter 26: loss 10.7992, time 2268.71ms, mfu 1.50%\n",
      "iter 27: loss 10.7980, time 2267.79ms, mfu 1.50%\n",
      "iter 28: loss 10.7887, time 2267.26ms, mfu 1.50%\n",
      "iter 29: loss 10.7918, time 2267.40ms, mfu 1.50%\n",
      "iter 30: loss 10.7810, time 2268.67ms, mfu 1.50%\n",
      "iter 31: loss 10.7710, time 2268.15ms, mfu 1.50%\n",
      "iter 32: loss 10.7676, time 2269.10ms, mfu 1.50%\n",
      "iter 33: loss 10.7550, time 2267.00ms, mfu 1.50%\n",
      "iter 34: loss 10.7512, time 2267.35ms, mfu 1.50%\n",
      "iter 35: loss 10.7470, time 2269.58ms, mfu 1.50%\n",
      "iter 36: loss 10.7288, time 2268.10ms, mfu 1.50%\n",
      "iter 37: loss 10.7148, time 2266.87ms, mfu 1.50%\n",
      "iter 38: loss 10.6953, time 2268.40ms, mfu 1.50%\n",
      "iter 39: loss 10.7065, time 2267.94ms, mfu 1.50%\n",
      "iter 40: loss 10.6921, time 2267.96ms, mfu 1.50%\n",
      "iter 41: loss 10.6887, time 2269.56ms, mfu 1.50%\n",
      "iter 42: loss 10.6810, time 2264.44ms, mfu 1.50%\n",
      "iter 43: loss 10.6818, time 2266.78ms, mfu 1.50%\n",
      "iter 44: loss 10.6605, time 2266.50ms, mfu 1.50%\n",
      "iter 45: loss 10.6376, time 2267.34ms, mfu 1.50%\n",
      "iter 46: loss 10.6213, time 2268.07ms, mfu 1.50%\n",
      "iter 47: loss 10.6248, time 2267.03ms, mfu 1.50%\n",
      "iter 48: loss 10.6066, time 2268.16ms, mfu 1.50%\n",
      "iter 49: loss 10.5927, time 2268.36ms, mfu 1.50%\n",
      "iter 50: loss 10.5875, time 2266.91ms, mfu 1.50%\n",
      "iter 51: loss 10.5743, time 2265.41ms, mfu 1.50%\n",
      "iter 52: loss 10.5818, time 2267.05ms, mfu 1.50%\n",
      "iter 53: loss 10.5505, time 2265.73ms, mfu 1.50%\n",
      "iter 54: loss 10.5502, time 2267.30ms, mfu 1.50%\n",
      "iter 55: loss 10.5620, time 2268.51ms, mfu 1.50%\n",
      "iter 56: loss 10.5538, time 2267.13ms, mfu 1.50%\n",
      "iter 57: loss 10.5106, time 2268.54ms, mfu 1.50%\n",
      "iter 58: loss 10.5054, time 2268.87ms, mfu 1.50%\n",
      "iter 59: loss 10.4955, time 2265.66ms, mfu 1.50%\n",
      "iter 60: loss 10.5073, time 2269.23ms, mfu 1.50%\n",
      "iter 61: loss 10.4686, time 2268.25ms, mfu 1.50%\n",
      "iter 62: loss 10.4643, time 2265.52ms, mfu 1.50%\n",
      "iter 63: loss 10.4600, time 2268.24ms, mfu 1.50%\n",
      "iter 64: loss 10.4831, time 2268.77ms, mfu 1.50%\n",
      "iter 65: loss 10.4482, time 2267.33ms, mfu 1.50%\n",
      "iter 66: loss 10.4391, time 2267.74ms, mfu 1.50%\n",
      "iter 67: loss 10.4338, time 2266.87ms, mfu 1.50%\n",
      "iter 68: loss 10.4062, time 2267.37ms, mfu 1.50%\n",
      "iter 69: loss 10.4305, time 2268.34ms, mfu 1.50%\n",
      "iter 70: loss 10.3733, time 2266.53ms, mfu 1.50%\n",
      "iter 71: loss 10.3944, time 2267.39ms, mfu 1.50%\n",
      "iter 72: loss 10.4064, time 2266.83ms, mfu 1.50%\n",
      "iter 73: loss 10.3967, time 2268.65ms, mfu 1.50%\n",
      "iter 74: loss 10.3193, time 2268.00ms, mfu 1.50%\n",
      "iter 75: loss 10.3592, time 2269.40ms, mfu 1.50%\n",
      "iter 76: loss 10.3830, time 2269.10ms, mfu 1.50%\n",
      "iter 77: loss 10.3185, time 2268.09ms, mfu 1.50%\n",
      "iter 78: loss 10.3481, time 2268.39ms, mfu 1.50%\n",
      "iter 79: loss 10.3444, time 2268.38ms, mfu 1.50%\n",
      "iter 80: loss 10.4052, time 2268.47ms, mfu 1.50%\n",
      "iter 81: loss 10.3385, time 2265.36ms, mfu 1.50%\n",
      "iter 82: loss 10.3470, time 2267.35ms, mfu 1.50%\n",
      "iter 83: loss 10.3379, time 2268.75ms, mfu 1.50%\n",
      "iter 84: loss 10.3715, time 2268.60ms, mfu 1.50%\n",
      "iter 85: loss 10.3216, time 2268.41ms, mfu 1.50%\n",
      "iter 86: loss 10.2596, time 2269.26ms, mfu 1.50%\n",
      "iter 87: loss 10.2237, time 2268.38ms, mfu 1.50%\n",
      "iter 88: loss 10.3574, time 2267.57ms, mfu 1.50%\n",
      "iter 89: loss 10.2799, time 2268.58ms, mfu 1.50%\n",
      "iter 90: loss 10.2927, time 2267.07ms, mfu 1.50%\n",
      "iter 91: loss 10.3185, time 2265.31ms, mfu 1.50%\n",
      "iter 92: loss 10.2796, time 2269.10ms, mfu 1.50%\n",
      "iter 93: loss 10.2583, time 2265.80ms, mfu 1.50%\n",
      "iter 94: loss 10.3391, time 2263.56ms, mfu 1.50%\n",
      "iter 95: loss 10.3035, time 2267.75ms, mfu 1.50%\n",
      "iter 96: loss 10.2227, time 2267.69ms, mfu 1.50%\n",
      "iter 97: loss 10.2170, time 2267.64ms, mfu 1.50%\n",
      "iter 98: loss 10.2448, time 2268.36ms, mfu 1.50%\n",
      "iter 99: loss 10.2274, time 2267.43ms, mfu 1.50%\n",
      "iter 100: loss 10.2136, time 2268.75ms, mfu 1.50%\n",
      "iter 101: loss 10.2863, time 2267.75ms, mfu 1.50%\n",
      "iter 102: loss 10.2118, time 2268.41ms, mfu 1.50%\n",
      "iter 103: loss 10.2301, time 2268.84ms, mfu 1.50%\n",
      "iter 104: loss 10.2425, time 2268.67ms, mfu 1.50%\n",
      "iter 105: loss 10.2020, time 2267.04ms, mfu 1.50%\n",
      "iter 106: loss 10.1294, time 2268.97ms, mfu 1.50%\n",
      "iter 107: loss 10.2286, time 2268.74ms, mfu 1.50%\n",
      "iter 108: loss 10.2028, time 2267.34ms, mfu 1.50%\n",
      "iter 109: loss 10.1861, time 2267.74ms, mfu 1.50%\n",
      "iter 110: loss 10.2240, time 2267.41ms, mfu 1.50%\n",
      "iter 111: loss 10.2293, time 2267.58ms, mfu 1.50%\n",
      "iter 112: loss 10.1717, time 2265.46ms, mfu 1.50%\n",
      "iter 113: loss 10.1525, time 2268.72ms, mfu 1.50%\n",
      "iter 114: loss 10.2378, time 2269.02ms, mfu 1.50%\n",
      "iter 115: loss 10.1905, time 2267.79ms, mfu 1.50%\n",
      "iter 116: loss 10.1127, time 2268.96ms, mfu 1.50%\n",
      "iter 117: loss 10.0776, time 2268.83ms, mfu 1.50%\n",
      "iter 118: loss 10.1251, time 2268.81ms, mfu 1.50%\n",
      "iter 119: loss 10.2618, time 2267.06ms, mfu 1.50%\n",
      "iter 120: loss 10.2738, time 2268.76ms, mfu 1.50%\n",
      "iter 121: loss 10.0882, time 2267.41ms, mfu 1.50%\n",
      "iter 122: loss 10.1649, time 2269.39ms, mfu 1.50%\n",
      "iter 123: loss 10.1782, time 2267.10ms, mfu 1.50%\n",
      "iter 124: loss 10.1884, time 2269.54ms, mfu 1.50%\n",
      "iter 125: loss 10.1657, time 2268.35ms, mfu 1.50%\n",
      "iter 126: loss 10.1588, time 2266.79ms, mfu 1.50%\n",
      "iter 127: loss 10.1210, time 2268.55ms, mfu 1.50%\n",
      "iter 128: loss 10.2743, time 2267.82ms, mfu 1.50%\n",
      "iter 129: loss 10.1058, time 2266.82ms, mfu 1.50%\n",
      "iter 130: loss 10.1007, time 2268.57ms, mfu 1.50%\n",
      "iter 131: loss 10.1145, time 2269.44ms, mfu 1.50%\n",
      "iter 132: loss 10.1136, time 2265.72ms, mfu 1.50%\n",
      "iter 133: loss 10.1333, time 2268.97ms, mfu 1.50%\n",
      "iter 134: loss 10.1270, time 2269.10ms, mfu 1.50%\n",
      "iter 135: loss 10.0916, time 2266.68ms, mfu 1.50%\n",
      "iter 136: loss 10.0830, time 2269.24ms, mfu 1.50%\n",
      "iter 137: loss 10.1325, time 2267.44ms, mfu 1.50%\n",
      "iter 138: loss 10.0972, time 2269.19ms, mfu 1.50%\n",
      "iter 139: loss 10.0844, time 2266.07ms, mfu 1.50%\n",
      "iter 140: loss 10.0993, time 2267.14ms, mfu 1.50%\n",
      "iter 141: loss 10.1625, time 2268.59ms, mfu 1.50%\n",
      "iter 142: loss 10.2041, time 2268.74ms, mfu 1.50%\n",
      "iter 143: loss 10.1725, time 2268.87ms, mfu 1.50%\n",
      "iter 144: loss 10.1430, time 2266.45ms, mfu 1.50%\n",
      "iter 145: loss 10.0783, time 2268.51ms, mfu 1.50%\n",
      "iter 146: loss 10.1223, time 2267.44ms, mfu 1.50%\n",
      "iter 147: loss 10.0635, time 2269.33ms, mfu 1.50%\n",
      "iter 148: loss 10.1352, time 2267.76ms, mfu 1.50%\n",
      "iter 149: loss 10.0976, time 2268.47ms, mfu 1.50%\n",
      "iter 150: loss 10.0727, time 2267.86ms, mfu 1.50%\n",
      "iter 151: loss 9.9762, time 2269.07ms, mfu 1.50%\n",
      "iter 152: loss 10.1825, time 2266.99ms, mfu 1.50%\n",
      "iter 153: loss 10.0446, time 2269.18ms, mfu 1.50%\n",
      "iter 154: loss 10.0153, time 2267.72ms, mfu 1.50%\n",
      "iter 155: loss 10.0217, time 2268.87ms, mfu 1.50%\n",
      "iter 156: loss 10.0797, time 2268.45ms, mfu 1.50%\n",
      "iter 157: loss 10.0796, time 2267.24ms, mfu 1.50%\n",
      "iter 158: loss 10.0815, time 2267.29ms, mfu 1.50%\n",
      "iter 159: loss 9.9998, time 2267.00ms, mfu 1.50%\n",
      "iter 160: loss 10.0311, time 2269.15ms, mfu 1.50%\n",
      "iter 161: loss 9.9753, time 2268.74ms, mfu 1.50%\n",
      "iter 162: loss 9.9675, time 2269.18ms, mfu 1.50%\n",
      "iter 163: loss 10.0514, time 2267.64ms, mfu 1.50%\n",
      "iter 164: loss 9.9626, time 2268.66ms, mfu 1.50%\n",
      "iter 165: loss 9.9216, time 2268.57ms, mfu 1.50%\n",
      "iter 166: loss 10.0469, time 2268.93ms, mfu 1.50%\n",
      "iter 167: loss 9.9621, time 2268.18ms, mfu 1.50%\n",
      "iter 168: loss 9.9789, time 2269.86ms, mfu 1.50%\n",
      "iter 169: loss 10.0060, time 2268.03ms, mfu 1.50%\n",
      "iter 170: loss 9.9679, time 2267.00ms, mfu 1.50%\n",
      "iter 171: loss 10.0202, time 2268.78ms, mfu 1.50%\n",
      "iter 172: loss 9.9295, time 2265.40ms, mfu 1.50%\n",
      "iter 173: loss 9.9356, time 2268.94ms, mfu 1.50%\n",
      "iter 174: loss 9.9303, time 2269.29ms, mfu 1.50%\n",
      "iter 175: loss 9.8715, time 2267.24ms, mfu 1.50%\n",
      "iter 176: loss 9.9048, time 2269.04ms, mfu 1.50%\n",
      "iter 177: loss 9.9431, time 2266.21ms, mfu 1.50%\n",
      "iter 178: loss 9.9625, time 2266.40ms, mfu 1.50%\n",
      "iter 179: loss 9.9425, time 2267.56ms, mfu 1.50%\n",
      "iter 180: loss 9.8919, time 2268.73ms, mfu 1.50%\n",
      "iter 181: loss 9.8761, time 2269.02ms, mfu 1.50%\n",
      "iter 182: loss 9.9033, time 2268.36ms, mfu 1.50%\n",
      "iter 183: loss 9.8828, time 2267.34ms, mfu 1.50%\n",
      "iter 184: loss 9.7779, time 2269.29ms, mfu 1.50%\n",
      "iter 185: loss 9.8853, time 2268.76ms, mfu 1.50%\n",
      "iter 186: loss 9.9008, time 2269.13ms, mfu 1.50%\n",
      "iter 187: loss 9.8346, time 2267.09ms, mfu 1.50%\n",
      "iter 188: loss 9.9071, time 2267.24ms, mfu 1.50%\n",
      "iter 189: loss 9.7796, time 2267.59ms, mfu 1.50%\n",
      "iter 190: loss 9.8319, time 2269.71ms, mfu 1.50%\n",
      "iter 191: loss 9.8099, time 2268.08ms, mfu 1.50%\n",
      "iter 192: loss 9.8065, time 2268.92ms, mfu 1.50%\n",
      "iter 193: loss 9.7995, time 2268.85ms, mfu 1.50%\n",
      "iter 194: loss 9.7703, time 2269.79ms, mfu 1.50%\n",
      "iter 195: loss 9.8374, time 2268.68ms, mfu 1.50%\n",
      "iter 196: loss 9.8076, time 2267.63ms, mfu 1.50%\n",
      "iter 197: loss 9.8065, time 2267.52ms, mfu 1.50%\n",
      "iter 198: loss 9.7796, time 2267.89ms, mfu 1.50%\n",
      "iter 199: loss 9.8211, time 2265.90ms, mfu 1.50%\n",
      "step 200: train loss 9.8033, val loss 9.8194\n",
      "saving checkpoint to /kaggle/working/\n",
      "iter 200: loss 9.7437, time 11010.56ms, mfu 1.38%\n",
      "iter 201: loss 9.7378, time 2273.19ms, mfu 1.39%\n",
      "iter 202: loss 9.7797, time 2262.66ms, mfu 1.40%\n",
      "iter 203: loss 9.7425, time 2269.15ms, mfu 1.41%\n",
      "iter 204: loss 9.7646, time 2268.81ms, mfu 1.42%\n",
      "iter 205: loss 9.7513, time 2267.78ms, mfu 1.43%\n",
      "iter 206: loss 9.8541, time 2266.96ms, mfu 1.44%\n",
      "iter 207: loss 9.7514, time 2265.00ms, mfu 1.44%\n",
      "iter 208: loss 9.7364, time 2268.90ms, mfu 1.45%\n",
      "iter 209: loss 9.8793, time 2267.61ms, mfu 1.45%\n",
      "iter 210: loss 9.7039, time 2268.04ms, mfu 1.46%\n",
      "iter 211: loss 9.7302, time 2268.27ms, mfu 1.46%\n",
      "iter 212: loss 9.7062, time 2266.27ms, mfu 1.47%\n",
      "iter 213: loss 9.7312, time 2266.96ms, mfu 1.47%\n",
      "iter 214: loss 9.8111, time 2269.95ms, mfu 1.47%\n",
      "iter 215: loss 9.7465, time 2267.42ms, mfu 1.47%\n",
      "iter 216: loss 9.7988, time 2269.31ms, mfu 1.48%\n",
      "iter 217: loss 9.8829, time 2267.84ms, mfu 1.48%\n",
      "iter 218: loss 9.7079, time 2268.69ms, mfu 1.48%\n",
      "iter 219: loss 9.7458, time 2269.04ms, mfu 1.48%\n",
      "iter 220: loss 9.6366, time 2269.51ms, mfu 1.48%\n",
      "iter 221: loss 9.8277, time 2268.55ms, mfu 1.49%\n",
      "iter 222: loss 9.6791, time 2268.52ms, mfu 1.49%\n",
      "iter 223: loss 9.8308, time 2269.29ms, mfu 1.49%\n",
      "iter 224: loss 9.6061, time 2264.90ms, mfu 1.49%\n",
      "iter 225: loss 9.7647, time 2268.64ms, mfu 1.49%\n",
      "iter 226: loss 9.6639, time 2266.75ms, mfu 1.49%\n",
      "iter 227: loss 9.6920, time 2269.07ms, mfu 1.49%\n",
      "iter 228: loss 9.6846, time 2267.40ms, mfu 1.49%\n",
      "iter 229: loss 9.6601, time 2267.33ms, mfu 1.49%\n",
      "iter 230: loss 9.7039, time 2268.87ms, mfu 1.49%\n",
      "iter 231: loss 9.5830, time 2267.74ms, mfu 1.49%\n",
      "iter 232: loss 9.6222, time 2266.36ms, mfu 1.50%\n",
      "iter 233: loss 9.6844, time 2268.75ms, mfu 1.50%\n",
      "iter 234: loss 9.5983, time 2266.05ms, mfu 1.50%\n",
      "iter 235: loss 9.6133, time 2269.55ms, mfu 1.50%\n",
      "iter 236: loss 9.6346, time 2269.61ms, mfu 1.50%\n",
      "iter 237: loss 9.6760, time 2270.38ms, mfu 1.50%\n",
      "iter 238: loss 9.5699, time 2265.38ms, mfu 1.50%\n",
      "iter 239: loss 9.5722, time 2269.49ms, mfu 1.50%\n",
      "iter 240: loss 9.6644, time 2269.95ms, mfu 1.50%\n",
      "iter 241: loss 9.6072, time 2269.25ms, mfu 1.50%\n",
      "iter 242: loss 9.7240, time 2267.54ms, mfu 1.50%\n",
      "iter 243: loss 9.5971, time 2267.74ms, mfu 1.50%\n",
      "iter 244: loss 9.4319, time 2267.97ms, mfu 1.50%\n",
      "iter 245: loss 9.6201, time 2266.80ms, mfu 1.50%\n",
      "iter 246: loss 9.6276, time 2266.98ms, mfu 1.50%\n",
      "iter 247: loss 9.6260, time 2267.77ms, mfu 1.50%\n",
      "iter 248: loss 9.4575, time 2268.14ms, mfu 1.50%\n",
      "iter 249: loss 9.5425, time 2265.19ms, mfu 1.50%\n",
      "iter 250: loss 9.5135, time 2266.66ms, mfu 1.50%\n",
      "iter 251: loss 9.4793, time 2268.59ms, mfu 1.50%\n",
      "iter 252: loss 9.6095, time 2268.77ms, mfu 1.50%\n",
      "iter 253: loss 9.5371, time 2269.50ms, mfu 1.50%\n",
      "iter 254: loss 9.5811, time 2266.48ms, mfu 1.50%\n",
      "iter 255: loss 9.4779, time 2269.20ms, mfu 1.50%\n",
      "iter 256: loss 9.5311, time 2269.73ms, mfu 1.50%\n",
      "iter 257: loss 9.5671, time 2267.57ms, mfu 1.50%\n",
      "iter 258: loss 9.3110, time 2267.47ms, mfu 1.50%\n",
      "iter 259: loss 9.4799, time 2269.63ms, mfu 1.50%\n",
      "iter 260: loss 9.4941, time 2267.07ms, mfu 1.50%\n",
      "iter 261: loss 9.4764, time 2270.38ms, mfu 1.50%\n",
      "iter 262: loss 9.3443, time 2267.94ms, mfu 1.50%\n",
      "iter 263: loss 9.4153, time 2269.72ms, mfu 1.50%\n",
      "iter 264: loss 9.3941, time 2267.71ms, mfu 1.50%\n",
      "iter 265: loss 9.5395, time 2265.92ms, mfu 1.50%\n",
      "iter 266: loss 9.4504, time 2267.74ms, mfu 1.50%\n",
      "iter 267: loss 9.3700, time 2269.64ms, mfu 1.50%\n",
      "iter 268: loss 9.4152, time 2268.53ms, mfu 1.50%\n",
      "iter 269: loss 9.3712, time 2269.77ms, mfu 1.50%\n",
      "iter 270: loss 9.5025, time 2268.00ms, mfu 1.50%\n",
      "iter 271: loss 9.4682, time 2266.02ms, mfu 1.50%\n",
      "iter 272: loss 9.4738, time 2266.48ms, mfu 1.50%\n",
      "iter 273: loss 9.4376, time 2269.72ms, mfu 1.50%\n",
      "iter 274: loss 9.3578, time 2269.41ms, mfu 1.50%\n",
      "iter 275: loss 9.4330, time 2270.20ms, mfu 1.50%\n",
      "iter 276: loss 9.3770, time 2266.43ms, mfu 1.50%\n",
      "iter 277: loss 9.4414, time 2267.99ms, mfu 1.50%\n",
      "iter 278: loss 9.3419, time 2267.53ms, mfu 1.50%\n",
      "iter 279: loss 9.5755, time 2269.69ms, mfu 1.50%\n",
      "iter 280: loss 9.3373, time 2267.33ms, mfu 1.50%\n",
      "iter 281: loss 9.3403, time 2266.99ms, mfu 1.50%\n",
      "iter 282: loss 9.4094, time 2269.79ms, mfu 1.50%\n",
      "iter 283: loss 9.2627, time 2267.93ms, mfu 1.50%\n",
      "iter 284: loss 9.3233, time 2266.32ms, mfu 1.50%\n",
      "iter 285: loss 9.4168, time 2269.21ms, mfu 1.50%\n",
      "iter 286: loss 9.3239, time 2270.25ms, mfu 1.50%\n",
      "iter 287: loss 9.3153, time 2267.96ms, mfu 1.50%\n",
      "iter 288: loss 9.4134, time 2268.35ms, mfu 1.50%\n",
      "iter 289: loss 9.2478, time 2269.20ms, mfu 1.50%\n",
      "iter 290: loss 9.2953, time 2268.82ms, mfu 1.50%\n",
      "iter 291: loss 9.3340, time 2268.93ms, mfu 1.50%\n",
      "iter 292: loss 9.3691, time 2269.42ms, mfu 1.50%\n",
      "iter 293: loss 9.2979, time 2268.19ms, mfu 1.50%\n",
      "iter 294: loss 9.2967, time 2269.42ms, mfu 1.50%\n",
      "iter 295: loss 9.4445, time 2270.03ms, mfu 1.50%\n",
      "iter 296: loss 9.3272, time 2269.57ms, mfu 1.50%\n",
      "iter 297: loss 9.2495, time 2265.96ms, mfu 1.50%\n",
      "iter 298: loss 9.3078, time 2268.41ms, mfu 1.50%\n",
      "iter 299: loss 9.2961, time 2268.08ms, mfu 1.50%\n",
      "iter 300: loss 9.3306, time 2270.09ms, mfu 1.50%\n",
      "iter 301: loss 9.3304, time 2266.34ms, mfu 1.50%\n",
      "iter 302: loss 9.1924, time 2269.46ms, mfu 1.50%\n",
      "iter 303: loss 9.1979, time 2269.68ms, mfu 1.50%\n",
      "iter 304: loss 9.2433, time 2266.32ms, mfu 1.50%\n",
      "iter 305: loss 9.3277, time 2267.84ms, mfu 1.50%\n",
      "iter 306: loss 9.1604, time 2268.43ms, mfu 1.50%\n",
      "iter 307: loss 9.1522, time 2268.48ms, mfu 1.50%\n",
      "iter 308: loss 9.0946, time 2265.67ms, mfu 1.50%\n",
      "iter 309: loss 9.2828, time 2269.47ms, mfu 1.50%\n",
      "iter 310: loss 9.2175, time 2269.91ms, mfu 1.50%\n",
      "iter 311: loss 9.1614, time 2266.35ms, mfu 1.50%\n",
      "iter 312: loss 9.1645, time 2269.19ms, mfu 1.50%\n",
      "iter 313: loss 9.1562, time 2269.85ms, mfu 1.50%\n",
      "iter 314: loss 9.2042, time 2266.49ms, mfu 1.50%\n",
      "iter 315: loss 9.1674, time 2267.20ms, mfu 1.50%\n",
      "iter 316: loss 9.1299, time 2269.55ms, mfu 1.50%\n",
      "iter 317: loss 9.1350, time 2266.07ms, mfu 1.50%\n",
      "iter 318: loss 9.2883, time 2269.93ms, mfu 1.50%\n",
      "iter 319: loss 9.2154, time 2268.13ms, mfu 1.50%\n",
      "iter 320: loss 9.1697, time 2269.43ms, mfu 1.50%\n",
      "iter 321: loss 9.1258, time 2268.04ms, mfu 1.50%\n",
      "iter 322: loss 9.0640, time 2268.00ms, mfu 1.50%\n",
      "iter 323: loss 9.1298, time 2268.81ms, mfu 1.50%\n",
      "iter 324: loss 9.2083, time 2268.20ms, mfu 1.50%\n",
      "iter 325: loss 9.2645, time 2268.58ms, mfu 1.50%\n",
      "iter 326: loss 9.1421, time 2265.70ms, mfu 1.50%\n",
      "iter 327: loss 9.2350, time 2267.91ms, mfu 1.50%\n",
      "iter 328: loss 9.2358, time 2268.28ms, mfu 1.50%\n",
      "iter 329: loss 9.0728, time 2269.47ms, mfu 1.50%\n",
      "iter 330: loss 9.1614, time 2270.33ms, mfu 1.50%\n",
      "iter 331: loss 9.2507, time 2269.00ms, mfu 1.50%\n",
      "iter 332: loss 9.1254, time 2269.31ms, mfu 1.50%\n",
      "iter 333: loss 9.0320, time 2269.28ms, mfu 1.50%\n",
      "iter 334: loss 9.0290, time 2269.54ms, mfu 1.50%\n",
      "iter 335: loss 9.1664, time 2269.68ms, mfu 1.50%\n",
      "iter 336: loss 9.0436, time 2267.68ms, mfu 1.50%\n",
      "iter 337: loss 9.1466, time 2268.08ms, mfu 1.50%\n",
      "iter 338: loss 9.0975, time 2269.71ms, mfu 1.50%\n",
      "iter 339: loss 9.0629, time 2270.26ms, mfu 1.50%\n",
      "iter 340: loss 9.0441, time 2269.50ms, mfu 1.50%\n",
      "iter 341: loss 9.0624, time 2268.05ms, mfu 1.50%\n",
      "iter 342: loss 9.1711, time 2269.13ms, mfu 1.50%\n",
      "iter 343: loss 8.9802, time 2268.42ms, mfu 1.50%\n",
      "iter 344: loss 9.0537, time 2269.46ms, mfu 1.50%\n",
      "iter 345: loss 9.0038, time 2269.45ms, mfu 1.50%\n",
      "iter 346: loss 9.0051, time 2266.83ms, mfu 1.50%\n",
      "iter 347: loss 8.9913, time 2270.15ms, mfu 1.50%\n",
      "iter 348: loss 9.0456, time 2269.80ms, mfu 1.50%\n",
      "iter 349: loss 9.0973, time 2268.06ms, mfu 1.50%\n",
      "iter 350: loss 8.8857, time 2268.98ms, mfu 1.50%\n",
      "iter 351: loss 9.0622, time 2269.70ms, mfu 1.50%\n",
      "iter 352: loss 9.1784, time 2270.12ms, mfu 1.50%\n",
      "iter 353: loss 9.0952, time 2269.07ms, mfu 1.50%\n",
      "iter 354: loss 9.0324, time 2266.99ms, mfu 1.50%\n",
      "iter 355: loss 9.0565, time 2267.95ms, mfu 1.50%\n",
      "iter 356: loss 8.9381, time 2268.59ms, mfu 1.50%\n",
      "iter 357: loss 8.9215, time 2269.37ms, mfu 1.50%\n",
      "iter 358: loss 8.9726, time 2268.53ms, mfu 1.50%\n",
      "iter 359: loss 8.9686, time 2269.45ms, mfu 1.50%\n",
      "iter 360: loss 8.9185, time 2266.62ms, mfu 1.50%\n",
      "iter 361: loss 9.0195, time 2269.16ms, mfu 1.50%\n",
      "iter 362: loss 8.8974, time 2266.20ms, mfu 1.50%\n",
      "iter 363: loss 8.8765, time 2268.65ms, mfu 1.50%\n",
      "iter 364: loss 9.0249, time 2266.60ms, mfu 1.50%\n",
      "iter 365: loss 8.9068, time 2268.21ms, mfu 1.50%\n",
      "iter 366: loss 8.9578, time 2269.21ms, mfu 1.50%\n",
      "iter 367: loss 8.9473, time 2266.66ms, mfu 1.50%\n",
      "iter 368: loss 8.8086, time 2269.91ms, mfu 1.50%\n",
      "iter 369: loss 8.8308, time 2269.87ms, mfu 1.50%\n",
      "iter 370: loss 9.1216, time 2268.62ms, mfu 1.50%\n",
      "iter 371: loss 8.9400, time 2267.86ms, mfu 1.50%\n",
      "iter 372: loss 8.9629, time 2268.29ms, mfu 1.50%\n",
      "iter 373: loss 8.9834, time 2267.50ms, mfu 1.50%\n",
      "iter 374: loss 8.8031, time 2267.99ms, mfu 1.50%\n",
      "iter 375: loss 8.7935, time 2265.92ms, mfu 1.50%\n",
      "iter 376: loss 8.8620, time 2268.18ms, mfu 1.50%\n",
      "iter 377: loss 8.8521, time 2269.77ms, mfu 1.50%\n",
      "iter 378: loss 8.8243, time 2266.99ms, mfu 1.50%\n",
      "iter 379: loss 8.8404, time 2269.52ms, mfu 1.50%\n",
      "iter 380: loss 8.8254, time 2268.42ms, mfu 1.50%\n",
      "iter 381: loss 8.8949, time 2269.63ms, mfu 1.50%\n",
      "iter 382: loss 8.7796, time 2269.05ms, mfu 1.50%\n",
      "iter 383: loss 8.9257, time 2398.18ms, mfu 1.49%\n",
      "iter 384: loss 8.7774, time 2269.13ms, mfu 1.49%\n",
      "iter 385: loss 8.7934, time 2269.95ms, mfu 1.49%\n",
      "iter 386: loss 8.8225, time 2265.26ms, mfu 1.49%\n",
      "iter 387: loss 8.6650, time 2269.47ms, mfu 1.49%\n",
      "iter 388: loss 8.7205, time 2266.38ms, mfu 1.49%\n",
      "iter 389: loss 8.6800, time 2267.98ms, mfu 1.49%\n",
      "iter 390: loss 8.7247, time 2269.39ms, mfu 1.50%\n",
      "iter 391: loss 8.7712, time 2269.52ms, mfu 1.50%\n",
      "iter 392: loss 8.8826, time 2268.99ms, mfu 1.50%\n",
      "iter 393: loss 8.7282, time 2268.76ms, mfu 1.50%\n",
      "iter 394: loss 8.8010, time 2269.35ms, mfu 1.50%\n",
      "iter 395: loss 8.6571, time 2269.85ms, mfu 1.50%\n",
      "iter 396: loss 8.8650, time 2267.72ms, mfu 1.50%\n",
      "iter 397: loss 8.8400, time 2268.36ms, mfu 1.50%\n",
      "iter 398: loss 8.6690, time 2266.73ms, mfu 1.50%\n",
      "iter 399: loss 8.6430, time 2270.29ms, mfu 1.50%\n",
      "step 400: train loss 8.6907, val loss 8.7218\n",
      "saving checkpoint to /kaggle/working/\n",
      "iter 400: loss 8.6568, time 11747.56ms, mfu 1.38%\n",
      "iter 401: loss 8.7072, time 2268.48ms, mfu 1.39%\n",
      "iter 402: loss 8.6551, time 2269.67ms, mfu 1.40%\n",
      "iter 403: loss 8.7659, time 2268.63ms, mfu 1.41%\n",
      "iter 404: loss 8.8386, time 2266.89ms, mfu 1.42%\n",
      "iter 405: loss 8.7065, time 2267.81ms, mfu 1.43%\n",
      "iter 406: loss 8.6972, time 2269.76ms, mfu 1.43%\n",
      "iter 407: loss 8.7376, time 2270.84ms, mfu 1.44%\n",
      "iter 408: loss 8.9341, time 2270.24ms, mfu 1.45%\n",
      "iter 409: loss 8.7060, time 2269.29ms, mfu 1.45%\n",
      "iter 410: loss 8.6251, time 2270.24ms, mfu 1.46%\n",
      "iter 411: loss 8.7256, time 2269.63ms, mfu 1.46%\n",
      "iter 412: loss 8.5960, time 2269.73ms, mfu 1.46%\n",
      "iter 413: loss 8.7952, time 2268.63ms, mfu 1.47%\n",
      "iter 414: loss 8.6618, time 2267.99ms, mfu 1.47%\n",
      "iter 415: loss 8.7786, time 2268.66ms, mfu 1.47%\n",
      "iter 416: loss 8.4768, time 2270.09ms, mfu 1.48%\n",
      "iter 417: loss 8.5386, time 2269.84ms, mfu 1.48%\n",
      "iter 418: loss 8.5564, time 2269.54ms, mfu 1.48%\n",
      "iter 419: loss 8.7578, time 2270.20ms, mfu 1.48%\n",
      "iter 420: loss 8.6405, time 2267.71ms, mfu 1.48%\n",
      "iter 421: loss 8.5930, time 2269.46ms, mfu 1.48%\n",
      "iter 422: loss 8.5958, time 2268.06ms, mfu 1.49%\n",
      "iter 423: loss 8.5146, time 2270.05ms, mfu 1.49%\n",
      "iter 424: loss 8.6227, time 2270.26ms, mfu 1.49%\n",
      "iter 425: loss 8.7093, time 2267.71ms, mfu 1.49%\n",
      "iter 426: loss 8.6566, time 2269.74ms, mfu 1.49%\n",
      "iter 427: loss 8.5870, time 2268.08ms, mfu 1.49%\n",
      "iter 428: loss 8.5100, time 2269.20ms, mfu 1.49%\n",
      "iter 429: loss 8.6589, time 2268.27ms, mfu 1.49%\n",
      "iter 430: loss 8.6384, time 2268.21ms, mfu 1.49%\n",
      "iter 431: loss 8.4404, time 2265.16ms, mfu 1.49%\n",
      "iter 432: loss 8.4798, time 2268.82ms, mfu 1.49%\n",
      "iter 433: loss 8.3926, time 2270.62ms, mfu 1.49%\n",
      "iter 434: loss 8.5543, time 2268.76ms, mfu 1.50%\n",
      "iter 435: loss 8.6165, time 2269.99ms, mfu 1.50%\n",
      "iter 436: loss 8.5717, time 2266.87ms, mfu 1.50%\n",
      "iter 437: loss 8.4407, time 2269.11ms, mfu 1.50%\n",
      "iter 438: loss 8.4229, time 2266.41ms, mfu 1.50%\n",
      "iter 439: loss 8.5513, time 2268.48ms, mfu 1.50%\n",
      "iter 440: loss 8.3468, time 2270.27ms, mfu 1.50%\n",
      "iter 441: loss 8.4329, time 2268.21ms, mfu 1.50%\n",
      "iter 442: loss 8.5587, time 2268.32ms, mfu 1.50%\n",
      "iter 443: loss 8.2706, time 2269.72ms, mfu 1.50%\n",
      "iter 444: loss 8.4377, time 2269.98ms, mfu 1.50%\n",
      "iter 445: loss 8.4700, time 2269.77ms, mfu 1.50%\n",
      "iter 446: loss 8.3752, time 2266.88ms, mfu 1.50%\n",
      "iter 447: loss 8.5534, time 2269.89ms, mfu 1.50%\n",
      "iter 448: loss 8.2377, time 2269.96ms, mfu 1.50%\n",
      "iter 449: loss 8.4015, time 2270.04ms, mfu 1.50%\n",
      "iter 450: loss 8.3824, time 2266.52ms, mfu 1.50%\n",
      "iter 451: loss 8.4676, time 2269.78ms, mfu 1.50%\n",
      "iter 452: loss 8.4239, time 2266.53ms, mfu 1.50%\n",
      "iter 453: loss 8.3948, time 2268.98ms, mfu 1.50%\n",
      "iter 454: loss 8.4245, time 2267.83ms, mfu 1.50%\n",
      "iter 455: loss 8.5625, time 2269.96ms, mfu 1.50%\n",
      "iter 456: loss 8.2559, time 2266.14ms, mfu 1.50%\n",
      "iter 457: loss 8.4761, time 2267.57ms, mfu 1.50%\n",
      "iter 458: loss 8.4409, time 2267.97ms, mfu 1.50%\n",
      "iter 459: loss 8.6005, time 2267.48ms, mfu 1.50%\n",
      "iter 460: loss 8.4909, time 2268.07ms, mfu 1.50%\n",
      "iter 461: loss 8.2786, time 2268.44ms, mfu 1.50%\n",
      "iter 462: loss 8.3434, time 2269.60ms, mfu 1.50%\n",
      "iter 463: loss 8.2709, time 2268.02ms, mfu 1.50%\n",
      "iter 464: loss 8.3246, time 2268.19ms, mfu 1.50%\n",
      "iter 465: loss 8.4507, time 2268.21ms, mfu 1.50%\n",
      "iter 466: loss 8.2942, time 2268.53ms, mfu 1.50%\n",
      "iter 467: loss 8.4605, time 2269.58ms, mfu 1.50%\n",
      "iter 468: loss 8.6517, time 2269.85ms, mfu 1.50%\n",
      "iter 469: loss 8.3103, time 2270.58ms, mfu 1.50%\n",
      "iter 470: loss 8.4137, time 2268.79ms, mfu 1.50%\n",
      "iter 471: loss 8.4104, time 2269.54ms, mfu 1.50%\n",
      "iter 472: loss 8.3077, time 2269.55ms, mfu 1.50%\n",
      "iter 473: loss 8.3418, time 2268.47ms, mfu 1.50%\n",
      "iter 474: loss 8.1481, time 2268.91ms, mfu 1.50%\n",
      "iter 475: loss 8.3496, time 2269.27ms, mfu 1.50%\n",
      "iter 476: loss 8.2134, time 2269.52ms, mfu 1.50%\n",
      "iter 477: loss 8.2380, time 2269.66ms, mfu 1.50%\n",
      "iter 478: loss 8.3517, time 2269.53ms, mfu 1.50%\n",
      "iter 479: loss 8.2390, time 2267.53ms, mfu 1.50%\n",
      "iter 480: loss 8.1193, time 2270.27ms, mfu 1.50%\n",
      "iter 481: loss 8.2652, time 2269.89ms, mfu 1.50%\n",
      "iter 482: loss 8.4628, time 2269.62ms, mfu 1.50%\n",
      "iter 483: loss 8.1400, time 2269.88ms, mfu 1.50%\n",
      "iter 484: loss 8.3392, time 2267.32ms, mfu 1.50%\n",
      "iter 485: loss 8.2131, time 2269.58ms, mfu 1.50%\n",
      "iter 486: loss 8.2424, time 2266.57ms, mfu 1.50%\n",
      "iter 487: loss 8.4088, time 2270.16ms, mfu 1.50%\n",
      "iter 488: loss 8.2940, time 2268.25ms, mfu 1.50%\n",
      "iter 489: loss 8.2387, time 2269.47ms, mfu 1.50%\n",
      "iter 490: loss 8.2847, time 2268.31ms, mfu 1.50%\n",
      "iter 491: loss 8.1145, time 2269.60ms, mfu 1.50%\n",
      "iter 492: loss 8.2499, time 2268.17ms, mfu 1.50%\n",
      "iter 493: loss 8.3756, time 2270.00ms, mfu 1.50%\n",
      "iter 494: loss 8.2483, time 2268.92ms, mfu 1.50%\n",
      "iter 495: loss 8.2137, time 2269.08ms, mfu 1.50%\n",
      "iter 496: loss 8.1206, time 2268.05ms, mfu 1.50%\n",
      "iter 497: loss 8.1304, time 2268.02ms, mfu 1.50%\n",
      "iter 498: loss 8.1176, time 2269.58ms, mfu 1.50%\n",
      "iter 499: loss 8.2755, time 2266.24ms, mfu 1.50%\n",
      "iter 500: loss 8.2804, time 2268.42ms, mfu 1.50%\n",
      "iter 501: loss 8.3191, time 2267.08ms, mfu 1.50%\n",
      "iter 502: loss 8.1909, time 2268.36ms, mfu 1.50%\n",
      "iter 503: loss 8.2475, time 2269.74ms, mfu 1.50%\n",
      "iter 504: loss 8.0564, time 2269.67ms, mfu 1.50%\n",
      "iter 505: loss 8.0551, time 2267.00ms, mfu 1.50%\n",
      "iter 506: loss 8.1752, time 2269.41ms, mfu 1.50%\n",
      "iter 507: loss 7.9974, time 2267.50ms, mfu 1.50%\n",
      "iter 508: loss 8.0154, time 2269.83ms, mfu 1.50%\n",
      "iter 509: loss 8.0498, time 2269.23ms, mfu 1.50%\n",
      "iter 510: loss 7.9674, time 2266.82ms, mfu 1.50%\n",
      "iter 511: loss 8.2951, time 2268.29ms, mfu 1.50%\n",
      "iter 512: loss 8.0260, time 2269.97ms, mfu 1.50%\n",
      "iter 513: loss 7.9361, time 2266.84ms, mfu 1.50%\n",
      "iter 514: loss 7.8672, time 2268.20ms, mfu 1.50%\n",
      "iter 515: loss 8.2413, time 2269.77ms, mfu 1.50%\n",
      "iter 516: loss 8.1603, time 2268.83ms, mfu 1.50%\n",
      "iter 517: loss 8.0732, time 2270.02ms, mfu 1.50%\n",
      "iter 518: loss 8.0977, time 2269.32ms, mfu 1.50%\n",
      "iter 519: loss 8.0364, time 2269.18ms, mfu 1.50%\n",
      "iter 520: loss 8.2608, time 2271.01ms, mfu 1.50%\n",
      "iter 521: loss 8.0826, time 2266.71ms, mfu 1.50%\n",
      "iter 522: loss 8.0340, time 2268.79ms, mfu 1.50%\n",
      "iter 523: loss 7.9779, time 2269.49ms, mfu 1.50%\n",
      "iter 524: loss 8.0479, time 2267.20ms, mfu 1.50%\n",
      "iter 525: loss 7.9079, time 2268.77ms, mfu 1.50%\n",
      "iter 526: loss 7.9509, time 2269.08ms, mfu 1.50%\n",
      "iter 527: loss 7.8982, time 2268.99ms, mfu 1.50%\n",
      "iter 528: loss 8.2143, time 2268.31ms, mfu 1.50%\n",
      "iter 529: loss 7.8276, time 2267.96ms, mfu 1.50%\n",
      "iter 530: loss 8.0389, time 2267.29ms, mfu 1.50%\n",
      "iter 531: loss 8.2932, time 2269.68ms, mfu 1.50%\n",
      "iter 532: loss 7.8292, time 2270.59ms, mfu 1.50%\n",
      "iter 533: loss 8.0587, time 2268.96ms, mfu 1.50%\n",
      "iter 534: loss 8.3826, time 2269.91ms, mfu 1.50%\n",
      "iter 535: loss 7.9510, time 2268.02ms, mfu 1.50%\n",
      "iter 536: loss 7.7569, time 2268.51ms, mfu 1.50%\n",
      "iter 537: loss 8.1605, time 2268.65ms, mfu 1.50%\n",
      "iter 538: loss 7.8691, time 2268.52ms, mfu 1.50%\n",
      "iter 539: loss 7.9022, time 2268.15ms, mfu 1.50%\n",
      "iter 540: loss 8.1104, time 2270.23ms, mfu 1.50%\n",
      "iter 541: loss 7.8437, time 2269.93ms, mfu 1.50%\n",
      "iter 542: loss 7.8581, time 2268.78ms, mfu 1.50%\n",
      "iter 543: loss 7.8726, time 2270.66ms, mfu 1.50%\n",
      "iter 544: loss 7.7284, time 2269.14ms, mfu 1.50%\n",
      "iter 545: loss 7.9155, time 2269.09ms, mfu 1.50%\n",
      "iter 546: loss 8.0034, time 2270.06ms, mfu 1.50%\n",
      "iter 547: loss 7.7726, time 2270.29ms, mfu 1.50%\n",
      "iter 548: loss 7.9247, time 2270.16ms, mfu 1.50%\n",
      "iter 549: loss 7.9156, time 2270.58ms, mfu 1.50%\n",
      "iter 550: loss 7.6588, time 2270.35ms, mfu 1.50%\n",
      "iter 551: loss 8.0621, time 2268.18ms, mfu 1.50%\n",
      "iter 552: loss 7.7185, time 2268.42ms, mfu 1.50%\n",
      "iter 553: loss 7.8176, time 2269.95ms, mfu 1.50%\n",
      "iter 554: loss 7.7449, time 2270.14ms, mfu 1.50%\n",
      "iter 555: loss 7.8445, time 2269.60ms, mfu 1.50%\n",
      "iter 556: loss 7.7827, time 2267.06ms, mfu 1.50%\n",
      "iter 557: loss 7.6824, time 2268.55ms, mfu 1.50%\n",
      "iter 558: loss 7.6717, time 2270.39ms, mfu 1.50%\n",
      "iter 559: loss 7.6921, time 2268.28ms, mfu 1.50%\n",
      "iter 560: loss 7.5538, time 2270.33ms, mfu 1.50%\n",
      "iter 561: loss 7.7348, time 2269.78ms, mfu 1.50%\n",
      "iter 562: loss 7.9622, time 2269.98ms, mfu 1.50%\n",
      "iter 563: loss 7.6485, time 2268.04ms, mfu 1.50%\n",
      "iter 564: loss 7.8522, time 2268.26ms, mfu 1.50%\n",
      "iter 565: loss 7.9322, time 2266.44ms, mfu 1.50%\n",
      "iter 566: loss 7.8680, time 2268.64ms, mfu 1.50%\n",
      "iter 567: loss 7.7332, time 2269.93ms, mfu 1.50%\n",
      "iter 568: loss 7.7371, time 2270.40ms, mfu 1.50%\n",
      "iter 569: loss 7.9542, time 2269.86ms, mfu 1.50%\n",
      "iter 570: loss 7.9940, time 2270.61ms, mfu 1.50%\n",
      "iter 571: loss 7.9095, time 2268.76ms, mfu 1.50%\n",
      "iter 572: loss 7.9311, time 2270.35ms, mfu 1.50%\n",
      "iter 573: loss 8.0876, time 2267.74ms, mfu 1.50%\n",
      "iter 574: loss 7.7910, time 2268.93ms, mfu 1.50%\n",
      "iter 575: loss 7.6708, time 2270.60ms, mfu 1.50%\n",
      "iter 576: loss 7.7313, time 2269.87ms, mfu 1.50%\n",
      "iter 577: loss 7.9478, time 2268.80ms, mfu 1.50%\n",
      "iter 578: loss 7.7645, time 2267.06ms, mfu 1.50%\n",
      "iter 579: loss 7.8878, time 2270.45ms, mfu 1.50%\n",
      "iter 580: loss 7.6275, time 2270.04ms, mfu 1.50%\n",
      "iter 581: loss 7.6466, time 2266.61ms, mfu 1.50%\n",
      "iter 582: loss 8.0104, time 2268.34ms, mfu 1.50%\n",
      "iter 583: loss 7.6161, time 2266.32ms, mfu 1.50%\n",
      "iter 584: loss 7.6657, time 2267.16ms, mfu 1.50%\n",
      "iter 585: loss 7.5749, time 2270.33ms, mfu 1.50%\n",
      "iter 586: loss 7.8287, time 2269.78ms, mfu 1.50%\n",
      "iter 587: loss 7.5821, time 2269.84ms, mfu 1.50%\n",
      "iter 588: loss 7.5377, time 2268.57ms, mfu 1.50%\n",
      "iter 589: loss 7.8666, time 2268.28ms, mfu 1.50%\n",
      "iter 590: loss 7.6592, time 2270.32ms, mfu 1.50%\n",
      "iter 591: loss 7.8452, time 2269.66ms, mfu 1.50%\n",
      "iter 592: loss 7.6438, time 2267.15ms, mfu 1.50%\n",
      "iter 593: loss 7.8331, time 2269.86ms, mfu 1.50%\n",
      "iter 594: loss 7.6913, time 2268.79ms, mfu 1.50%\n",
      "iter 595: loss 7.7150, time 2270.01ms, mfu 1.50%\n",
      "iter 596: loss 7.7580, time 2269.96ms, mfu 1.50%\n",
      "iter 597: loss 7.5525, time 2270.12ms, mfu 1.50%\n",
      "iter 598: loss 7.6011, time 2270.05ms, mfu 1.50%\n",
      "iter 599: loss 7.4799, time 2268.32ms, mfu 1.50%\n",
      "step 600: train loss 7.6386, val loss 7.5750\n",
      "saving checkpoint to /kaggle/working/\n",
      "iter 600: loss 7.7755, time 11641.50ms, mfu 1.38%\n",
      "iter 601: loss 7.8625, time 2269.38ms, mfu 1.39%\n",
      "iter 602: loss 7.7630, time 2267.62ms, mfu 1.40%\n",
      "iter 603: loss 7.5949, time 2268.25ms, mfu 1.41%\n",
      "iter 604: loss 7.6113, time 2268.26ms, mfu 1.42%\n",
      "iter 605: loss 7.6585, time 2268.35ms, mfu 1.43%\n",
      "iter 606: loss 7.4708, time 2267.85ms, mfu 1.43%\n",
      "iter 607: loss 7.4832, time 2268.27ms, mfu 1.44%\n",
      "iter 608: loss 7.6233, time 2268.32ms, mfu 1.45%\n",
      "iter 609: loss 7.6509, time 2268.06ms, mfu 1.45%\n",
      "iter 610: loss 7.8520, time 2268.36ms, mfu 1.46%\n",
      "iter 611: loss 7.5311, time 2270.04ms, mfu 1.46%\n",
      "iter 612: loss 7.6478, time 2270.07ms, mfu 1.46%\n",
      "iter 613: loss 7.6646, time 2270.02ms, mfu 1.47%\n",
      "iter 614: loss 7.8246, time 2269.02ms, mfu 1.47%\n",
      "iter 615: loss 7.6627, time 2269.83ms, mfu 1.47%\n",
      "iter 616: loss 7.4534, time 2268.53ms, mfu 1.48%\n",
      "iter 617: loss 7.6697, time 2268.52ms, mfu 1.48%\n",
      "iter 618: loss 7.5523, time 2267.02ms, mfu 1.48%\n",
      "iter 619: loss 7.6792, time 2269.45ms, mfu 1.48%\n",
      "iter 620: loss 7.6007, time 2268.26ms, mfu 1.48%\n",
      "iter 621: loss 7.5888, time 2268.73ms, mfu 1.49%\n",
      "iter 622: loss 7.5011, time 2270.33ms, mfu 1.49%\n",
      "iter 623: loss 7.4788, time 2267.94ms, mfu 1.49%\n",
      "iter 624: loss 7.6631, time 2268.46ms, mfu 1.49%\n",
      "iter 625: loss 7.5112, time 2268.72ms, mfu 1.49%\n",
      "iter 626: loss 7.6470, time 2268.78ms, mfu 1.49%\n",
      "iter 627: loss 7.1299, time 2270.26ms, mfu 1.49%\n",
      "iter 628: loss 7.3926, time 2265.30ms, mfu 1.49%\n",
      "iter 629: loss 7.6354, time 2270.01ms, mfu 1.49%\n",
      "iter 630: loss 7.3975, time 2270.43ms, mfu 1.49%\n",
      "iter 631: loss 7.4538, time 2268.77ms, mfu 1.49%\n",
      "iter 632: loss 7.7251, time 2268.40ms, mfu 1.49%\n",
      "iter 633: loss 7.3725, time 2268.15ms, mfu 1.49%\n",
      "iter 634: loss 7.4779, time 2270.60ms, mfu 1.50%\n",
      "iter 635: loss 7.2880, time 2269.52ms, mfu 1.50%\n",
      "iter 636: loss 7.4353, time 2267.79ms, mfu 1.50%\n",
      "iter 637: loss 7.4879, time 2267.71ms, mfu 1.50%\n",
      "iter 638: loss 7.4589, time 2268.28ms, mfu 1.50%\n",
      "iter 639: loss 7.3349, time 2269.34ms, mfu 1.50%\n",
      "iter 640: loss 7.6420, time 2268.37ms, mfu 1.50%\n",
      "iter 641: loss 7.2562, time 2268.46ms, mfu 1.50%\n",
      "iter 642: loss 7.3068, time 2268.35ms, mfu 1.50%\n",
      "iter 643: loss 7.2460, time 2270.20ms, mfu 1.50%\n",
      "iter 644: loss 7.4743, time 2266.58ms, mfu 1.50%\n",
      "iter 645: loss 7.4649, time 2267.59ms, mfu 1.50%\n",
      "iter 646: loss 7.3254, time 2266.35ms, mfu 1.50%\n",
      "iter 647: loss 7.1868, time 2268.33ms, mfu 1.50%\n",
      "iter 648: loss 7.3841, time 2270.25ms, mfu 1.50%\n",
      "iter 649: loss 7.3146, time 2268.29ms, mfu 1.50%\n",
      "iter 650: loss 7.4093, time 2270.12ms, mfu 1.50%\n",
      "iter 651: loss 7.5396, time 2270.08ms, mfu 1.50%\n",
      "iter 652: loss 7.4021, time 2269.15ms, mfu 1.50%\n",
      "iter 653: loss 7.2483, time 2270.61ms, mfu 1.50%\n",
      "iter 654: loss 7.3815, time 2268.46ms, mfu 1.50%\n",
      "iter 655: loss 7.4909, time 2270.45ms, mfu 1.50%\n",
      "iter 656: loss 7.4102, time 2267.77ms, mfu 1.50%\n",
      "iter 657: loss 7.5920, time 2267.82ms, mfu 1.50%\n",
      "iter 658: loss 7.2934, time 2270.27ms, mfu 1.50%\n",
      "iter 659: loss 7.3856, time 2268.08ms, mfu 1.50%\n",
      "iter 660: loss 7.3858, time 2269.69ms, mfu 1.50%\n",
      "iter 661: loss 7.1004, time 2268.43ms, mfu 1.50%\n",
      "iter 662: loss 7.2265, time 2270.55ms, mfu 1.50%\n",
      "iter 663: loss 7.2561, time 2269.15ms, mfu 1.50%\n",
      "iter 664: loss 7.5744, time 2270.21ms, mfu 1.50%\n",
      "iter 665: loss 7.3104, time 2270.06ms, mfu 1.50%\n",
      "iter 666: loss 7.2799, time 2268.10ms, mfu 1.50%\n",
      "iter 667: loss 7.2795, time 2270.29ms, mfu 1.50%\n",
      "iter 668: loss 7.4300, time 2268.31ms, mfu 1.50%\n",
      "iter 669: loss 7.1750, time 2268.30ms, mfu 1.50%\n",
      "iter 670: loss 7.2410, time 2268.36ms, mfu 1.50%\n",
      "iter 671: loss 7.2961, time 2269.35ms, mfu 1.50%\n",
      "iter 672: loss 7.8148, time 2269.74ms, mfu 1.50%\n",
      "iter 673: loss 7.4753, time 2269.20ms, mfu 1.50%\n",
      "iter 674: loss 7.1511, time 2270.57ms, mfu 1.50%\n",
      "iter 675: loss 7.0854, time 2270.12ms, mfu 1.50%\n",
      "iter 676: loss 7.1866, time 2268.94ms, mfu 1.50%\n",
      "iter 677: loss 6.9787, time 2266.32ms, mfu 1.50%\n",
      "iter 678: loss 7.3731, time 2269.67ms, mfu 1.50%\n",
      "iter 679: loss 7.1723, time 2269.71ms, mfu 1.50%\n",
      "iter 680: loss 7.4228, time 2271.21ms, mfu 1.50%\n",
      "iter 681: loss 7.2559, time 2268.70ms, mfu 1.50%\n",
      "iter 682: loss 7.2075, time 2268.48ms, mfu 1.50%\n",
      "iter 683: loss 7.1020, time 2270.15ms, mfu 1.50%\n",
      "iter 684: loss 7.1505, time 2267.14ms, mfu 1.50%\n",
      "iter 685: loss 7.1675, time 2270.34ms, mfu 1.50%\n",
      "iter 686: loss 7.3255, time 2269.69ms, mfu 1.50%\n",
      "iter 687: loss 7.2464, time 2268.39ms, mfu 1.50%\n",
      "iter 688: loss 7.4362, time 2270.44ms, mfu 1.50%\n",
      "iter 689: loss 7.1999, time 2268.21ms, mfu 1.50%\n",
      "iter 690: loss 7.3088, time 2270.77ms, mfu 1.50%\n",
      "iter 691: loss 7.2709, time 2270.64ms, mfu 1.50%\n",
      "iter 692: loss 7.0573, time 2267.32ms, mfu 1.50%\n",
      "iter 693: loss 7.5138, time 2268.62ms, mfu 1.50%\n",
      "iter 694: loss 7.1097, time 2267.69ms, mfu 1.50%\n",
      "iter 695: loss 7.4043, time 2268.79ms, mfu 1.50%\n",
      "iter 696: loss 6.9312, time 2268.47ms, mfu 1.50%\n",
      "iter 697: loss 7.1402, time 2268.62ms, mfu 1.50%\n",
      "iter 698: loss 7.3608, time 2268.89ms, mfu 1.50%\n",
      "iter 699: loss 7.1803, time 2268.15ms, mfu 1.50%\n",
      "iter 700: loss 7.6304, time 2267.96ms, mfu 1.50%\n",
      "iter 701: loss 7.3656, time 2268.35ms, mfu 1.50%\n",
      "iter 702: loss 7.2845, time 2268.72ms, mfu 1.50%\n",
      "iter 703: loss 7.1604, time 2270.34ms, mfu 1.50%\n",
      "iter 704: loss 7.1991, time 2268.88ms, mfu 1.50%\n",
      "iter 705: loss 7.1630, time 2270.34ms, mfu 1.50%\n",
      "iter 706: loss 7.0493, time 2269.88ms, mfu 1.50%\n",
      "iter 707: loss 6.9649, time 2270.73ms, mfu 1.50%\n",
      "iter 708: loss 7.2345, time 2268.34ms, mfu 1.50%\n",
      "iter 709: loss 7.1786, time 2270.67ms, mfu 1.50%\n",
      "iter 710: loss 6.9732, time 2265.02ms, mfu 1.50%\n",
      "iter 711: loss 7.5756, time 2268.27ms, mfu 1.50%\n",
      "iter 712: loss 7.1764, time 2270.49ms, mfu 1.50%\n",
      "iter 713: loss 7.0719, time 2268.79ms, mfu 1.50%\n",
      "iter 714: loss 7.0144, time 2270.09ms, mfu 1.50%\n",
      "iter 715: loss 6.8734, time 2270.18ms, mfu 1.50%\n",
      "iter 716: loss 6.9742, time 2270.61ms, mfu 1.50%\n",
      "iter 717: loss 7.1023, time 2269.34ms, mfu 1.50%\n",
      "iter 718: loss 7.4936, time 2269.22ms, mfu 1.50%\n",
      "iter 719: loss 7.4317, time 2269.21ms, mfu 1.50%\n",
      "iter 720: loss 7.2658, time 2270.48ms, mfu 1.50%\n",
      "iter 721: loss 7.0723, time 2270.86ms, mfu 1.50%\n",
      "iter 722: loss 7.1515, time 2268.33ms, mfu 1.50%\n",
      "iter 723: loss 7.1197, time 2269.88ms, mfu 1.50%\n",
      "iter 724: loss 7.3983, time 2269.90ms, mfu 1.50%\n",
      "iter 725: loss 7.1724, time 2269.27ms, mfu 1.50%\n",
      "iter 726: loss 7.0900, time 2268.16ms, mfu 1.50%\n",
      "iter 727: loss 7.0890, time 2269.33ms, mfu 1.50%\n",
      "iter 728: loss 7.1045, time 2268.92ms, mfu 1.50%\n",
      "iter 729: loss 7.0508, time 2268.22ms, mfu 1.50%\n",
      "iter 730: loss 7.0480, time 2268.85ms, mfu 1.50%\n",
      "iter 731: loss 7.0216, time 2270.47ms, mfu 1.50%\n",
      "iter 732: loss 7.1094, time 2269.85ms, mfu 1.50%\n",
      "iter 733: loss 6.9185, time 2268.70ms, mfu 1.50%\n",
      "iter 734: loss 7.0240, time 2270.18ms, mfu 1.50%\n",
      "iter 735: loss 7.0645, time 2268.24ms, mfu 1.50%\n",
      "iter 736: loss 7.1665, time 2267.36ms, mfu 1.50%\n",
      "iter 737: loss 6.8155, time 2268.66ms, mfu 1.50%\n",
      "iter 738: loss 7.0674, time 2268.97ms, mfu 1.50%\n",
      "iter 739: loss 7.1568, time 2269.92ms, mfu 1.50%\n",
      "iter 740: loss 7.1694, time 2268.91ms, mfu 1.50%\n",
      "iter 741: loss 7.3434, time 2270.26ms, mfu 1.50%\n",
      "iter 742: loss 7.2633, time 2270.21ms, mfu 1.50%\n",
      "iter 743: loss 7.1440, time 2268.86ms, mfu 1.50%\n",
      "iter 744: loss 7.0218, time 2270.47ms, mfu 1.50%\n",
      "iter 745: loss 7.1358, time 2270.33ms, mfu 1.50%\n",
      "iter 746: loss 6.9667, time 2268.47ms, mfu 1.50%\n",
      "iter 747: loss 7.0636, time 2270.47ms, mfu 1.50%\n",
      "iter 748: loss 7.4068, time 2270.29ms, mfu 1.50%\n",
      "iter 749: loss 7.0027, time 2270.30ms, mfu 1.50%\n",
      "iter 750: loss 7.0165, time 2267.26ms, mfu 1.50%\n",
      "iter 751: loss 7.1543, time 2269.46ms, mfu 1.50%\n",
      "iter 752: loss 6.8739, time 2266.73ms, mfu 1.50%\n",
      "iter 753: loss 7.1944, time 2268.37ms, mfu 1.50%\n",
      "iter 754: loss 7.0980, time 2270.59ms, mfu 1.50%\n",
      "iter 755: loss 6.9600, time 2270.72ms, mfu 1.50%\n",
      "iter 756: loss 7.2001, time 2269.64ms, mfu 1.50%\n",
      "iter 757: loss 7.1262, time 2268.08ms, mfu 1.50%\n",
      "iter 758: loss 7.0551, time 2270.44ms, mfu 1.50%\n",
      "iter 759: loss 7.2546, time 2269.49ms, mfu 1.50%\n",
      "iter 760: loss 7.1134, time 2270.35ms, mfu 1.50%\n",
      "iter 761: loss 7.1005, time 2270.91ms, mfu 1.50%\n",
      "iter 762: loss 7.1036, time 2269.76ms, mfu 1.50%\n",
      "iter 763: loss 7.2985, time 2270.71ms, mfu 1.50%\n",
      "iter 764: loss 6.9753, time 2270.24ms, mfu 1.50%\n",
      "iter 765: loss 7.3520, time 2269.47ms, mfu 1.50%\n",
      "iter 766: loss 7.0925, time 2270.50ms, mfu 1.50%\n",
      "iter 767: loss 7.1534, time 2271.14ms, mfu 1.50%\n",
      "iter 768: loss 7.1113, time 2265.81ms, mfu 1.50%\n",
      "iter 769: loss 7.1427, time 2268.93ms, mfu 1.50%\n",
      "iter 770: loss 7.0291, time 2267.33ms, mfu 1.50%\n",
      "iter 771: loss 7.2110, time 2270.27ms, mfu 1.50%\n",
      "iter 772: loss 6.7164, time 2270.86ms, mfu 1.50%\n",
      "iter 773: loss 7.1368, time 2270.52ms, mfu 1.50%\n",
      "iter 774: loss 7.0619, time 2271.05ms, mfu 1.50%\n",
      "iter 775: loss 7.1028, time 2268.32ms, mfu 1.50%\n",
      "iter 776: loss 7.0508, time 2269.39ms, mfu 1.50%\n",
      "iter 777: loss 6.9774, time 2270.25ms, mfu 1.50%\n",
      "iter 778: loss 7.0232, time 2268.91ms, mfu 1.50%\n",
      "iter 779: loss 7.0857, time 2267.48ms, mfu 1.50%\n",
      "iter 780: loss 7.3495, time 2270.31ms, mfu 1.50%\n",
      "iter 781: loss 7.1138, time 2271.04ms, mfu 1.50%\n",
      "iter 782: loss 7.1065, time 2270.16ms, mfu 1.50%\n",
      "iter 783: loss 6.8172, time 2268.06ms, mfu 1.50%\n",
      "iter 784: loss 6.9923, time 2269.09ms, mfu 1.50%\n",
      "iter 785: loss 7.1960, time 2267.61ms, mfu 1.50%\n",
      "iter 786: loss 6.8219, time 2269.56ms, mfu 1.50%\n",
      "iter 787: loss 6.9631, time 2268.52ms, mfu 1.50%\n",
      "iter 788: loss 6.9361, time 2267.62ms, mfu 1.50%\n",
      "iter 789: loss 6.8519, time 2270.61ms, mfu 1.50%\n",
      "iter 790: loss 6.9680, time 2269.79ms, mfu 1.50%\n",
      "iter 791: loss 7.2430, time 2270.38ms, mfu 1.50%\n",
      "iter 792: loss 7.3853, time 2270.65ms, mfu 1.50%\n",
      "iter 793: loss 7.1116, time 2268.92ms, mfu 1.50%\n",
      "iter 794: loss 6.8875, time 2270.19ms, mfu 1.50%\n",
      "iter 795: loss 6.9218, time 2269.63ms, mfu 1.50%\n",
      "iter 796: loss 6.8272, time 2268.67ms, mfu 1.50%\n",
      "iter 797: loss 6.7888, time 2269.24ms, mfu 1.50%\n",
      "iter 798: loss 6.7195, time 2267.54ms, mfu 1.50%\n",
      "iter 799: loss 6.9472, time 2268.89ms, mfu 1.50%\n",
      "step 800: train loss 6.9898, val loss 6.9780\n",
      "saving checkpoint to /kaggle/working/\n",
      "iter 800: loss 6.9724, time 11573.75ms, mfu 1.38%\n",
      "iter 801: loss 6.9673, time 2269.61ms, mfu 1.39%\n",
      "iter 802: loss 7.1001, time 2270.22ms, mfu 1.40%\n",
      "iter 803: loss 7.1803, time 2269.35ms, mfu 1.41%\n",
      "iter 804: loss 6.9288, time 2270.56ms, mfu 1.42%\n",
      "iter 805: loss 6.8156, time 2269.43ms, mfu 1.43%\n",
      "iter 806: loss 6.8841, time 2270.34ms, mfu 1.43%\n",
      "iter 807: loss 6.7692, time 2270.83ms, mfu 1.44%\n",
      "iter 808: loss 6.8243, time 2268.97ms, mfu 1.45%\n",
      "iter 809: loss 6.8686, time 2270.41ms, mfu 1.45%\n",
      "iter 810: loss 7.0309, time 2268.84ms, mfu 1.46%\n",
      "iter 811: loss 6.9511, time 2268.77ms, mfu 1.46%\n",
      "iter 812: loss 6.9491, time 2267.09ms, mfu 1.46%\n",
      "iter 813: loss 7.0761, time 2270.08ms, mfu 1.47%\n",
      "iter 814: loss 7.1885, time 2270.87ms, mfu 1.47%\n",
      "iter 815: loss 7.0157, time 2268.83ms, mfu 1.47%\n",
      "iter 816: loss 6.9539, time 2269.09ms, mfu 1.48%\n",
      "iter 817: loss 6.8655, time 2270.23ms, mfu 1.48%\n",
      "iter 818: loss 7.0330, time 2268.56ms, mfu 1.48%\n",
      "iter 819: loss 6.9193, time 2270.48ms, mfu 1.48%\n",
      "iter 820: loss 6.9085, time 2269.13ms, mfu 1.48%\n",
      "iter 821: loss 6.8612, time 2270.59ms, mfu 1.48%\n",
      "iter 822: loss 7.0670, time 2270.59ms, mfu 1.49%\n",
      "iter 823: loss 6.7236, time 2269.23ms, mfu 1.49%\n",
      "iter 824: loss 6.8281, time 2270.99ms, mfu 1.49%\n",
      "iter 825: loss 7.0093, time 2267.76ms, mfu 1.49%\n",
      "iter 826: loss 6.8280, time 2270.53ms, mfu 1.49%\n",
      "iter 827: loss 6.6071, time 2269.74ms, mfu 1.49%\n",
      "iter 828: loss 6.9753, time 2270.37ms, mfu 1.49%\n",
      "iter 829: loss 6.7849, time 2272.12ms, mfu 1.49%\n",
      "iter 830: loss 7.0806, time 2268.78ms, mfu 1.49%\n",
      "iter 831: loss 6.9534, time 2268.94ms, mfu 1.49%\n",
      "iter 832: loss 7.0301, time 2268.99ms, mfu 1.49%\n",
      "iter 833: loss 6.8127, time 2270.84ms, mfu 1.49%\n",
      "iter 834: loss 6.9579, time 2270.11ms, mfu 1.49%\n",
      "iter 835: loss 6.8367, time 2268.92ms, mfu 1.49%\n",
      "iter 836: loss 6.7294, time 2270.06ms, mfu 1.50%\n",
      "iter 837: loss 7.0982, time 2270.60ms, mfu 1.50%\n",
      "iter 838: loss 6.8888, time 2269.18ms, mfu 1.50%\n",
      "iter 839: loss 7.0503, time 2270.57ms, mfu 1.50%\n",
      "iter 840: loss 7.1113, time 2268.52ms, mfu 1.50%\n",
      "iter 841: loss 6.9340, time 2270.22ms, mfu 1.50%\n",
      "iter 842: loss 6.6090, time 2270.44ms, mfu 1.50%\n",
      "iter 843: loss 6.7583, time 2270.81ms, mfu 1.50%\n",
      "iter 844: loss 6.8236, time 2267.32ms, mfu 1.50%\n",
      "iter 845: loss 7.0017, time 2270.44ms, mfu 1.50%\n",
      "iter 846: loss 7.0392, time 2270.09ms, mfu 1.50%\n",
      "iter 847: loss 7.0936, time 2267.68ms, mfu 1.50%\n",
      "iter 848: loss 6.9460, time 2267.58ms, mfu 1.50%\n",
      "iter 849: loss 7.0567, time 2270.42ms, mfu 1.50%\n",
      "iter 850: loss 7.2127, time 2270.49ms, mfu 1.50%\n",
      "iter 851: loss 7.4565, time 2270.72ms, mfu 1.50%\n",
      "iter 852: loss 6.9645, time 2267.03ms, mfu 1.50%\n",
      "iter 853: loss 6.6834, time 2269.45ms, mfu 1.50%\n",
      "iter 854: loss 6.8968, time 2269.70ms, mfu 1.50%\n",
      "iter 855: loss 6.7292, time 2269.02ms, mfu 1.50%\n",
      "iter 856: loss 6.7439, time 2269.06ms, mfu 1.50%\n",
      "iter 857: loss 7.1982, time 2267.51ms, mfu 1.50%\n",
      "iter 858: loss 7.1606, time 2270.69ms, mfu 1.50%\n",
      "iter 859: loss 6.8240, time 2270.58ms, mfu 1.50%\n",
      "iter 860: loss 6.8779, time 2269.34ms, mfu 1.50%\n",
      "iter 861: loss 7.0638, time 2268.83ms, mfu 1.50%\n",
      "iter 862: loss 7.3546, time 2270.75ms, mfu 1.50%\n",
      "iter 863: loss 6.7870, time 2269.29ms, mfu 1.50%\n",
      "iter 864: loss 6.7176, time 2269.34ms, mfu 1.50%\n",
      "iter 865: loss 7.2241, time 2267.18ms, mfu 1.50%\n",
      "iter 866: loss 6.9340, time 2268.91ms, mfu 1.50%\n",
      "iter 867: loss 6.7152, time 2270.22ms, mfu 1.50%\n",
      "iter 868: loss 7.1401, time 2271.22ms, mfu 1.50%\n",
      "iter 869: loss 7.0275, time 2268.96ms, mfu 1.50%\n",
      "iter 870: loss 6.7517, time 2267.17ms, mfu 1.50%\n",
      "iter 871: loss 6.8127, time 2272.12ms, mfu 1.50%\n",
      "iter 872: loss 7.0542, time 2269.11ms, mfu 1.50%\n",
      "iter 873: loss 6.8147, time 2267.47ms, mfu 1.50%\n",
      "iter 874: loss 6.8700, time 2268.42ms, mfu 1.50%\n",
      "iter 875: loss 6.8653, time 2270.71ms, mfu 1.50%\n",
      "iter 876: loss 6.7362, time 2271.09ms, mfu 1.50%\n",
      "iter 877: loss 6.7926, time 2269.68ms, mfu 1.50%\n",
      "iter 878: loss 6.8413, time 2271.03ms, mfu 1.50%\n",
      "iter 879: loss 6.5938, time 2269.46ms, mfu 1.50%\n",
      "iter 880: loss 6.9192, time 2271.53ms, mfu 1.50%\n",
      "iter 881: loss 6.6725, time 2270.28ms, mfu 1.50%\n",
      "iter 882: loss 6.9142, time 2270.42ms, mfu 1.50%\n",
      "iter 883: loss 7.1810, time 2268.40ms, mfu 1.50%\n",
      "iter 884: loss 7.1634, time 2271.00ms, mfu 1.50%\n",
      "iter 885: loss 6.9444, time 2270.75ms, mfu 1.50%\n",
      "iter 886: loss 7.3577, time 2270.73ms, mfu 1.50%\n",
      "iter 887: loss 6.7346, time 2270.74ms, mfu 1.50%\n",
      "iter 888: loss 6.9525, time 2268.84ms, mfu 1.50%\n",
      "iter 889: loss 6.9668, time 2270.61ms, mfu 1.50%\n",
      "iter 890: loss 6.5791, time 2269.00ms, mfu 1.50%\n",
      "iter 891: loss 6.7898, time 2269.74ms, mfu 1.50%\n",
      "iter 892: loss 6.5629, time 2269.15ms, mfu 1.50%\n",
      "iter 893: loss 6.7995, time 2268.94ms, mfu 1.50%\n",
      "iter 894: loss 6.8988, time 2269.02ms, mfu 1.50%\n",
      "iter 895: loss 6.7129, time 2269.09ms, mfu 1.50%\n",
      "iter 896: loss 6.5988, time 2268.93ms, mfu 1.50%\n",
      "iter 897: loss 6.5666, time 2270.98ms, mfu 1.50%\n",
      "iter 898: loss 6.5670, time 2271.31ms, mfu 1.50%\n",
      "iter 899: loss 6.6620, time 2267.79ms, mfu 1.50%\n",
      "iter 900: loss 6.4572, time 2269.45ms, mfu 1.50%\n",
      "iter 901: loss 6.8948, time 2270.69ms, mfu 1.50%\n",
      "iter 902: loss 6.8768, time 2267.39ms, mfu 1.50%\n",
      "iter 903: loss 7.0923, time 2270.91ms, mfu 1.50%\n",
      "iter 904: loss 7.0408, time 2271.58ms, mfu 1.50%\n",
      "iter 905: loss 6.7581, time 2268.87ms, mfu 1.50%\n",
      "iter 906: loss 6.9686, time 2268.85ms, mfu 1.50%\n",
      "iter 907: loss 6.5748, time 2269.36ms, mfu 1.50%\n",
      "iter 908: loss 6.7136, time 2269.60ms, mfu 1.50%\n",
      "iter 909: loss 6.7716, time 2272.01ms, mfu 1.50%\n",
      "iter 910: loss 6.8199, time 2271.42ms, mfu 1.50%\n",
      "iter 911: loss 7.1384, time 2269.17ms, mfu 1.50%\n",
      "iter 912: loss 6.6772, time 2271.04ms, mfu 1.50%\n",
      "iter 913: loss 6.8104, time 2268.98ms, mfu 1.50%\n",
      "iter 914: loss 7.0087, time 2271.30ms, mfu 1.50%\n",
      "iter 915: loss 6.5220, time 2270.60ms, mfu 1.50%\n",
      "iter 916: loss 6.6684, time 2268.60ms, mfu 1.50%\n",
      "iter 917: loss 6.7342, time 2267.78ms, mfu 1.50%\n",
      "iter 918: loss 6.9062, time 2268.00ms, mfu 1.50%\n",
      "iter 919: loss 6.8963, time 2270.79ms, mfu 1.50%\n",
      "iter 920: loss 6.7724, time 2270.42ms, mfu 1.50%\n",
      "iter 921: loss 7.0012, time 2271.28ms, mfu 1.50%\n",
      "iter 922: loss 6.8623, time 2269.41ms, mfu 1.50%\n",
      "iter 923: loss 6.7995, time 2270.69ms, mfu 1.50%\n",
      "iter 924: loss 6.6814, time 2270.62ms, mfu 1.50%\n",
      "iter 925: loss 6.6836, time 2267.89ms, mfu 1.50%\n",
      "iter 926: loss 6.7350, time 2269.08ms, mfu 1.50%\n",
      "iter 927: loss 6.3707, time 2269.96ms, mfu 1.50%\n",
      "iter 928: loss 6.9087, time 2271.07ms, mfu 1.50%\n",
      "iter 929: loss 6.7458, time 2270.92ms, mfu 1.50%\n",
      "iter 930: loss 6.5931, time 2270.65ms, mfu 1.50%\n",
      "iter 931: loss 6.9628, time 2270.21ms, mfu 1.50%\n",
      "iter 932: loss 6.9311, time 2270.28ms, mfu 1.50%\n",
      "iter 933: loss 6.9265, time 2269.63ms, mfu 1.50%\n",
      "iter 934: loss 6.6474, time 2270.67ms, mfu 1.50%\n",
      "iter 935: loss 6.7772, time 2269.35ms, mfu 1.50%\n",
      "iter 936: loss 6.4957, time 2269.96ms, mfu 1.50%\n",
      "iter 937: loss 7.3200, time 2267.67ms, mfu 1.50%\n",
      "iter 938: loss 6.4609, time 2269.03ms, mfu 1.50%\n",
      "iter 939: loss 6.4801, time 2270.55ms, mfu 1.50%\n",
      "iter 940: loss 6.7269, time 2271.17ms, mfu 1.50%\n",
      "iter 941: loss 6.6753, time 2269.30ms, mfu 1.50%\n",
      "iter 942: loss 6.7591, time 2269.39ms, mfu 1.50%\n",
      "iter 943: loss 6.8351, time 2271.20ms, mfu 1.50%\n",
      "iter 944: loss 6.9963, time 2268.00ms, mfu 1.50%\n",
      "iter 945: loss 6.5631, time 2270.89ms, mfu 1.50%\n",
      "iter 946: loss 7.0044, time 2271.11ms, mfu 1.50%\n",
      "iter 947: loss 6.7287, time 2270.70ms, mfu 1.50%\n",
      "iter 948: loss 6.7600, time 2270.91ms, mfu 1.50%\n",
      "iter 949: loss 6.5918, time 2270.22ms, mfu 1.50%\n",
      "iter 950: loss 6.7287, time 2269.71ms, mfu 1.50%\n",
      "iter 951: loss 6.7537, time 2269.91ms, mfu 1.50%\n",
      "iter 952: loss 6.8468, time 2270.34ms, mfu 1.50%\n",
      "iter 953: loss 6.8108, time 2272.03ms, mfu 1.50%\n",
      "iter 954: loss 6.7223, time 2270.93ms, mfu 1.50%\n",
      "iter 955: loss 6.7259, time 2267.94ms, mfu 1.50%\n",
      "iter 956: loss 6.7441, time 2271.17ms, mfu 1.50%\n",
      "iter 957: loss 6.7938, time 2267.67ms, mfu 1.50%\n",
      "iter 958: loss 6.6008, time 2270.51ms, mfu 1.50%\n",
      "iter 959: loss 6.6674, time 2270.60ms, mfu 1.50%\n",
      "iter 960: loss 6.5653, time 2270.68ms, mfu 1.50%\n",
      "iter 961: loss 6.5858, time 2269.69ms, mfu 1.50%\n",
      "iter 962: loss 6.7278, time 2270.53ms, mfu 1.50%\n",
      "iter 963: loss 6.4666, time 2271.12ms, mfu 1.50%\n",
      "iter 964: loss 6.4967, time 2269.25ms, mfu 1.50%\n",
      "iter 965: loss 6.8789, time 2269.33ms, mfu 1.50%\n",
      "iter 966: loss 6.7444, time 2270.60ms, mfu 1.50%\n",
      "iter 967: loss 6.7636, time 2269.35ms, mfu 1.50%\n",
      "iter 968: loss 6.7439, time 2270.84ms, mfu 1.50%\n",
      "iter 969: loss 6.5315, time 2270.77ms, mfu 1.50%\n",
      "iter 970: loss 6.4081, time 2271.07ms, mfu 1.50%\n",
      "iter 971: loss 6.8692, time 2269.42ms, mfu 1.50%\n",
      "iter 972: loss 6.7636, time 2271.48ms, mfu 1.50%\n",
      "iter 973: loss 7.4315, time 2269.31ms, mfu 1.50%\n",
      "iter 974: loss 7.0893, time 2270.96ms, mfu 1.50%\n",
      "iter 975: loss 6.5999, time 2269.58ms, mfu 1.50%\n",
      "iter 976: loss 6.6079, time 2269.35ms, mfu 1.50%\n",
      "iter 977: loss 6.5160, time 2270.60ms, mfu 1.50%\n",
      "iter 978: loss 6.7101, time 2268.11ms, mfu 1.50%\n",
      "iter 979: loss 6.5456, time 2269.13ms, mfu 1.50%\n",
      "iter 980: loss 6.5391, time 2270.54ms, mfu 1.50%\n",
      "iter 981: loss 6.7994, time 2271.49ms, mfu 1.50%\n",
      "iter 982: loss 6.6211, time 2269.47ms, mfu 1.50%\n",
      "iter 983: loss 6.4829, time 2267.72ms, mfu 1.50%\n",
      "iter 984: loss 6.9117, time 2270.71ms, mfu 1.50%\n",
      "iter 985: loss 6.5888, time 2268.00ms, mfu 1.50%\n",
      "iter 986: loss 6.7575, time 2270.69ms, mfu 1.50%\n",
      "iter 987: loss 6.3195, time 2269.18ms, mfu 1.50%\n",
      "iter 988: loss 6.4212, time 2270.80ms, mfu 1.50%\n",
      "iter 989: loss 6.4603, time 2271.04ms, mfu 1.50%\n",
      "iter 990: loss 6.6671, time 2269.21ms, mfu 1.50%\n",
      "iter 991: loss 6.5153, time 2270.80ms, mfu 1.50%\n",
      "iter 992: loss 6.6135, time 2271.43ms, mfu 1.50%\n",
      "iter 993: loss 6.5594, time 2270.87ms, mfu 1.50%\n",
      "iter 994: loss 6.7785, time 2270.49ms, mfu 1.50%\n",
      "iter 995: loss 6.5371, time 2268.09ms, mfu 1.50%\n",
      "iter 996: loss 6.6823, time 2271.18ms, mfu 1.50%\n",
      "iter 997: loss 6.6974, time 2269.24ms, mfu 1.50%\n",
      "iter 998: loss 6.5732, time 2269.19ms, mfu 1.50%\n",
      "iter 999: loss 6.4324, time 2269.17ms, mfu 1.50%\n",
      "step 1000: train loss 6.6714, val loss 6.5733\n",
      "saving checkpoint to /kaggle/working/\n",
      "iter 1000: loss 6.5401, time 11573.62ms, mfu 1.38%\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568dd26e",
   "metadata": {
    "papermill": {
     "duration": 0.049429,
     "end_time": "2024-11-26T09:57:22.147038",
     "exception": false,
     "start_time": "2024-11-26T09:57:22.097609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- обучение и сравнение качества обычного небольшого трансформера (например по 6 голов и 6 слоев) и reflex attention (в разных сетапах)\n",
    "- любые изменения/дополнения/улучшения, которые по-вашему могут работать\n",
    "- отчет об экспериментах, что получилось и что нет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72f430",
   "metadata": {
    "papermill": {
     "duration": 0.049621,
     "end_time": "2024-11-26T09:57:22.245855",
     "exception": false,
     "start_time": "2024-11-26T09:57:22.196234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experminets\n",
    "- Add a bit of dropout\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480de83",
   "metadata": {
    "papermill": {
     "duration": 0.068723,
     "end_time": "2024-11-26T09:57:22.368398",
     "exception": false,
     "start_time": "2024-11-26T09:57:22.299675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a143a6d",
   "metadata": {
    "papermill": {
     "duration": 0.052261,
     "end_time": "2024-11-26T09:57:22.471058",
     "exception": false,
     "start_time": "2024-11-26T09:57:22.418797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metrics ???\n",
    "- MinGPT's Dataset for the Sort problem. E.g. for problem length 6 Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
    "- Long context find max element\n",
    "- \n",
    "- Language stuff\n",
    "    - Perplexity\n",
    "    - BLEU\n",
    "- GLUE\n",
    "- SuperGLUE (reasoning and understanding in complex contexts)\n",
    "- Long Range Arena (LRA)\n",
    "- MNLI: For natural language inference tasks ???\n",
    "- CoNLL: For named entity recognition tasks"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4297632,
     "sourceId": 7395155,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6144291,
     "sourceId": 9984524,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2338.868185,
   "end_time": "2024-11-26T09:57:23.866803",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-26T09:18:24.998618",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
